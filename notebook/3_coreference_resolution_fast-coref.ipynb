{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import py file to jupyter\n",
    "import os, sys\n",
    "PROJECT_HOME = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref\"\n",
    "PROJECT_SRC = PROJECT_HOME + \"/src\"\n",
    "sys.path.append(os.path.abspath(f\"{PROJECT_SRC}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlStr_inference = \"\"\"\n",
    "metrics:\n",
    "- MUC\n",
    "- Bcub\n",
    "- CEAFE\n",
    "keep_singletons: true\n",
    "seed: 45\n",
    "train: true\n",
    "eval_all: false\n",
    "use_wandb: true\n",
    "paths:\n",
    "  resource_dir: ${infra.work_dir}/../../coref_resources\n",
    "  base_data_dir: ${paths.resource_dir}/data\n",
    "  conll_scorer: ${paths.resource_dir}/reference-coreference-scorers/scorer.pl\n",
    "  base_model_dir: ${infra.work_dir}/../models\n",
    "  model_dir: .//../models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0\n",
    "  best_model_dir: .//../models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/best\n",
    "  model_filename: model.pth\n",
    "  model_name: None\n",
    "  model_name_prefix: coref_\n",
    "  model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/model.pth\n",
    "  best_model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/best/model.pth\n",
    "  doc_encoder_dirname: doc_encoder\n",
    "datasets:\n",
    "  ontonotes:\n",
    "    name: OntoNotes\n",
    "    cluster_threshold: 2\n",
    "    canonical_cluster_threshold: 2\n",
    "    targeted_eval: false\n",
    "    num_train_docs: 2802\n",
    "    num_dev_docs: 343\n",
    "    num_test_docs: 348\n",
    "    has_conll: true\n",
    "    singleton_file: ontonotes/ment_singletons_longformer_speaker/60.jsonlines\n",
    "model:\n",
    "  doc_encoder:\n",
    "    transformer:\n",
    "      name: longformer\n",
    "      model_size: large\n",
    "      model_str: allenai/longformer-large-4096\n",
    "      max_encoder_segment_len: 4096\n",
    "      max_segment_len: 4096\n",
    "    chunking: independent\n",
    "    finetune: true\n",
    "    add_speaker_tokens: true\n",
    "    speaker_start: '[SPEAKER_START]'\n",
    "    speaker_end: '[SPEAKER_END]'\n",
    "  memory:\n",
    "    mem_type:\n",
    "      name: unbounded\n",
    "      max_ents: None\n",
    "      eval_max_ents: None\n",
    "    emb_size: 20\n",
    "    mlp_size: 3000\n",
    "    mlp_depth: 1\n",
    "    sim_func: hadamard\n",
    "    entity_rep: wt_avg\n",
    "    num_feats: 2\n",
    "  mention_params:\n",
    "    max_span_width: 20\n",
    "    ment_emb: attn\n",
    "    use_gold_ments: false\n",
    "    use_topk: false\n",
    "    top_span_ratio: 0.4\n",
    "    emb_size: 20\n",
    "    mlp_size: 3000\n",
    "    mlp_depth: 1\n",
    "    ment_emb_to_size_factor:\n",
    "      attn: 3\n",
    "      endpoint: 2\n",
    "      max: 1\n",
    "  metadata_params:\n",
    "    use_genre_feature: false\n",
    "    default_genre: nw\n",
    "    genres:\n",
    "    - bc\n",
    "    - bn\n",
    "    - mz\n",
    "    - nw\n",
    "    - pt\n",
    "    - tc\n",
    "    - wb\n",
    "optimizer:\n",
    "  init_lr: 0.0003\n",
    "  fine_tune_lr: 1.0e-05\n",
    "  max_gradient_norm: 1.0\n",
    "  lr_decay: linear\n",
    "trainer:\n",
    "  dropout_rate: 0.3\n",
    "  label_smoothing_wt: 0.1\n",
    "  ment_loss: all\n",
    "  normalize_loss: false\n",
    "  max_evals: 20\n",
    "  to_save_model: true\n",
    "  log_frequency: 250\n",
    "  patience: 10\n",
    "  eval_per_k_steps: 5000\n",
    "  num_training_steps: 100000\n",
    "infra:\n",
    "  is_local: false\n",
    "  job_time: 14280\n",
    "  job_id: 72519194\n",
    "  work_dir: ./\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os import path\n",
    "# from model.utils import action_sequences_to_clusters\n",
    "from model.entity_ranking_model import EntityRankingModel\n",
    "# from inference.tokenize_doc import tokenize_and_segment_doc, basic_tokenize_doc\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "inference_config = OmegaConf.create(yamlStr_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_processing.utils import split_into_segments, get_sentence_map\n",
    "\n",
    "\n",
    "class DocumentState:\n",
    "    def __init__(self):\n",
    "        self.sentence_end = []\n",
    "        self.token_end = []\n",
    "        self.orig_tokens = []\n",
    "        self.tokens = []\n",
    "        self.subtokens = []\n",
    "        self.segments = []\n",
    "        self.subtoken_map = []\n",
    "        self.segment_subtoken_map = []\n",
    "        self.sentence_map = []\n",
    "        self.tensorized_sent = []\n",
    "        self.sent_len_list = []\n",
    "\n",
    "    def finalize(self):\n",
    "        subtoken_map = flatten(self.segment_subtoken_map)\n",
    "        num_words = len(flatten(self.segments))\n",
    "        assert num_words == len(subtoken_map), (num_words, len(subtoken_map))\n",
    "\n",
    "        return {\n",
    "            \"orig_tokens\": self.orig_tokens,\n",
    "            \"sentences\": self.segments,\n",
    "            \"sent_len_list\": self.sent_len_list,\n",
    "            \"tensorized_sent\": self.tensorized_sent,\n",
    "            \"sentence_map\": torch.tensor(\n",
    "                get_sentence_map(self.segments, self.sentence_end)\n",
    "            ),\n",
    "            \"subtoken_map\": subtoken_map,\n",
    "        }\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def get_tokenized_doc(doc, subword_tokenizer):\n",
    "    document_state = DocumentState()\n",
    "\n",
    "    word_idx = -1\n",
    "    for sentence in doc:\n",
    "        for word in sentence:\n",
    "            document_state.orig_tokens.append(word)\n",
    "            subtokens = subword_tokenizer.convert_tokens_to_ids(\n",
    "                subword_tokenizer.tokenize(\" \" + word)\n",
    "            )\n",
    "            document_state.tokens.append(word)\n",
    "            document_state.token_end += ([False] * (len(subtokens) - 1)) + [True]\n",
    "            word_idx += 1\n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                document_state.subtokens.append(subtoken)\n",
    "                document_state.sentence_end.append(False)\n",
    "                document_state.subtoken_map.append(word_idx)\n",
    "\n",
    "        document_state.sentence_end[-1] = True\n",
    "\n",
    "    # print(document_state.subtokens)\n",
    "    return document_state\n",
    "\n",
    "\n",
    "def basic_tokenize_doc(doc_str, basic_tokenizer):\n",
    "    doc = []\n",
    "    for sent in basic_tokenizer(doc_str).sents:\n",
    "        wordlist = [str(word) for word in sent]\n",
    "        doc.append(wordlist)\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tokenize_and_segment_doc(\n",
    "    basic_tokenized_doc, subword_tokenizer, max_segment_len=4096\n",
    "):\n",
    "    document_state: DocumentState = get_tokenized_doc(\n",
    "        basic_tokenized_doc, subword_tokenizer\n",
    "    )\n",
    "    document = post_tokenization_processing(\n",
    "        document_state, subword_tokenizer, max_segment_len=max_segment_len\n",
    "    )\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def post_tokenization_processing(\n",
    "    document_state: DocumentState, subword_tokenizer, max_segment_len=4096\n",
    "):\n",
    "    split_into_segments(\n",
    "        document_state,\n",
    "        max_segment_len,\n",
    "        document_state.sentence_end,\n",
    "        document_state.token_end,\n",
    "    )\n",
    "\n",
    "    # sentences = [lm_tokenizer.convert_tokens_to_ids(sent) for sent in document_state.segments]\n",
    "    sent_len_list = [len(sent) for sent in document_state.segments]\n",
    "    document_state.sent_len_list = sent_len_list\n",
    "    document_state.segments_indices = document_state.segments\n",
    "\n",
    "    # # Tensorize sentence - Streaming coreference is done one window at a time, so no padding is required\n",
    "    tensorized_sent = [\n",
    "        torch.unsqueeze(\n",
    "            torch.tensor(\n",
    "                [subword_tokenizer.cls_token_id]\n",
    "                + sent\n",
    "                + [subword_tokenizer.sep_token_id]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        for sent in document_state.segments\n",
    "    ]\n",
    "    document_state.tensorized_sent = tensorized_sent\n",
    "    return document_state.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_sequences_to_clusters(actions, mentions):\n",
    "    clusters = []\n",
    "    cell_to_clusters = {}\n",
    "\n",
    "    for mention, (cell_idx, action_type) in zip(mentions, actions):\n",
    "        # print(f\"mention:{mention}, cell_idx:{cell_idx}, action_type:{action_type}\")\n",
    "        # mention:[17, 17], cell_idx:2, action_type:o\n",
    "        # cell_to_clusters:{0: [[0, 14]], 1: [[5, 5]], 2: [[17, 17]]}\n",
    "        # mention:[19, 19], cell_idx:1, action_type:c\n",
    "        # cell_to_clusters:{0: [[0, 14]], 1: [[5, 5], [19, 19]], 2: [[17, 17]]}\n",
    "        \n",
    "        if action_type == \"c\":\n",
    "            # Insert one to the existing cluster\n",
    "            cell_to_clusters[cell_idx].append(mention)\n",
    "        elif action_type == \"o\":\n",
    "            # Overwrite\n",
    "            if cell_idx in cell_to_clusters:\n",
    "                # Remove (Save) the old cluster and initialize the new\n",
    "                clusters.append(cell_to_clusters[cell_idx])\n",
    "            # Create a cluster \n",
    "            cell_to_clusters[cell_idx] = [mention]\n",
    "        elif action_type == \"n\":\n",
    "            # Directly save the cluster with only one element\n",
    "            clusters.append([mention])\n",
    "        # print(f\"cell_to_clusters:{cell_to_clusters}\")\n",
    "        # print(f\"clusters:{clusters}\")\n",
    "\n",
    "    for cell_idx, cluster in cell_to_clusters.items():\n",
    "        clusters.append(cluster)\n",
    "    # print(f\"final clusters:{clusters}\")\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, model_path, encoder_name=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model\n",
    "        checkpoint = torch.load(\n",
    "            path.join(model_path, \"model.pth\"), map_location=self.device\n",
    "        )\n",
    "        self.config = OmegaConf.create(checkpoint[\"config\"])\n",
    "        if encoder_name is not None:\n",
    "            self.config.model.doc_encoder.transformer.model_str = encoder_name\n",
    "        self.model = EntityRankingModel(self.config.model, self.config.trainer)\n",
    "        self._load_model(checkpoint, model_path, encoder_name=encoder_name)\n",
    "\n",
    "        self.max_segment_len = self.config.model.doc_encoder.transformer.max_segment_len\n",
    "        self.tokenizer = self.model.mention_proposer.doc_encoder.tokenizer\n",
    "\n",
    "    def _load_model(self, checkpoint, model_path, encoder_name=None):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "        if self.config.model.doc_encoder.finetune:\n",
    "            # Load the document encoder params if encoder is finetuned\n",
    "            if encoder_name is None:\n",
    "                doc_encoder_dir = path.join(\n",
    "                    model_path, self.config.paths.doc_encoder_dirname\n",
    "                )\n",
    "            else:\n",
    "                doc_encoder_dir = encoder_name\n",
    "            # Load the encoder\n",
    "            print(f\"Loading [{doc_encoder_dir}] as mention_proposer.doc_encoder encoder and tokenizer\")\n",
    "            self.model.mention_proposer.doc_encoder.lm_encoder = (\n",
    "                AutoModel.from_pretrained(\n",
    "                    pretrained_model_name_or_path=doc_encoder_dir\n",
    "                )\n",
    "            )\n",
    "            self.model.mention_proposer.doc_encoder.tokenizer = (\n",
    "                AutoTokenizer.from_pretrained(\n",
    "                    pretrained_model_name_or_path=doc_encoder_dir\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.cuda()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_coreference(self, document):\n",
    "        if isinstance(document, list):\n",
    "            # Document is already tokenized\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                document, self.tokenizer, max_segment_len=self.max_segment_len\n",
    "            )\n",
    "        elif isinstance(document, str):\n",
    "            # Raw document string. First perform basic tokenization before further tokenization.\n",
    "            import spacy\n",
    "\n",
    "            basic_tokenizer = spacy.load(\"en_core_web_md\")\n",
    "            basic_tokenized_doc = basic_tokenize_doc(document, basic_tokenizer)\n",
    "            print(f\"subword_tokenizer:{self.tokenizer}\")\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                basic_tokenized_doc,\n",
    "                self.tokenizer,\n",
    "                max_segment_len=self.max_segment_len,\n",
    "            )\n",
    "        elif isinstance(document, dict):\n",
    "            tokenized_doc = document\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        pred_mentions, mention_scores, gt_actions, pred_actions = self.model(tokenized_doc)\n",
    "        # print(f\"pred_mentions:{pred_mentions}\")\n",
    "        # The first and last indices of the mentions. The indices refer to the indices of tokenized_doc[\"subtoken_map\"]\n",
    "        # Then we need to use the values in tokenized_doc[\"subtoken_map\"] to map the actual tokens which are stored in tokenized_doc[\"orig_tokens\"]\n",
    "        # pred_mentions:[[0, 14], [5, 5], [17, 17], [19, 19], [25, 25], [32, 32], [35, 35], [35, 37], [43, 44], [47, 47], [48, 53], [48, 54], [60, 60], [65, 65]]\n",
    "\n",
    "        # print(f\"pred_actions:{pred_actions}\")\n",
    "        # The first value is the id of cluster to which the mention belong. \n",
    "        # The second character means action: \n",
    "        # [c] will insert one mention to the existing cluster.\n",
    "        # [o] will create a new cluster with the current mention and waiting for other mentions to add in, \n",
    "        # or save the old cluster and initialize a new cluster if this id already create a cluster before.\n",
    "        # [n] will directly save the cluster with only one mention\n",
    "        # pred_actions:[(0, 'o'), (1, 'o'), (2, 'o'), (1, 'c'), (2, 'c'), (3, 'o'), (1, 'c'), (4, 'o'), (4, 'c'), (5, 'o'), (1, 'c'), (4, 'c'), (3, 'c'), (5, 'c')]\n",
    "\n",
    "        idx_clusters = action_sequences_to_clusters(pred_actions, pred_mentions)\n",
    "        # print(f\"idx_clusters:{idx_clusters}\")\n",
    "        # idx_clusters:[[[0, 14]], [[5, 5], [19, 19], [35, 35], [48, 53]], [[17, 17], [25, 25]], [[32, 32], [60, 60]], [[35, 37], [43, 44], [48, 54]], [[47, 47], [65, 65]]]\n",
    "\n",
    "        # The values in idx_clusters/pred_mentions refers to the indices of tokenized_doc[\"subtoken_map\"], \n",
    "        # while the values in tokenized_doc[\"subtoken_map\"] refers to the indces of tokenized_doc[\"orig_tokens\"].\n",
    "        subtoken_map = tokenized_doc[\"subtoken_map\"]\n",
    "        orig_tokens = tokenized_doc[\"orig_tokens\"]\n",
    "        clusters = []\n",
    "        coref_group_list = []\n",
    "        for idx_cluster in idx_clusters:\n",
    "            cur_cluster = []\n",
    "            coref_group = []\n",
    "            for (ment_start, ment_end) in idx_cluster:\n",
    "                coref_group.append(list(range(subtoken_map[ment_start], subtoken_map[ment_end] + 1)))\n",
    "                cur_cluster.append(\n",
    "                    (\n",
    "                        (ment_start, ment_end),\n",
    "                        \" \".join(\n",
    "                            orig_tokens[\n",
    "                                subtoken_map[ment_start] : subtoken_map[ment_end] + 1\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            coref_group_list.append(coref_group)\n",
    "            clusters.append(cur_cluster)\n",
    "\n",
    "        return {\n",
    "            \"tokenized_doc\": tokenized_doc,\n",
    "            \"clusters\": clusters,\n",
    "            \"subtoken_idx_clusters\": idx_clusters,\n",
    "            \"actions\": pred_actions,\n",
    "            \"mentions\": pred_mentions,\n",
    "            \"coref_group_list\": coref_group_list,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 22:18:56,463 - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-08-26 22:18:56,463 - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pid        sid  \\\n",
      "0       p10000032  s50414267   \n",
      "1       p10000032  s53189527   \n",
      "2       p10000032  s53911762   \n",
      "3       p10000032  s56699142   \n",
      "4       p10000764  s57375967   \n",
      "...           ...        ...   \n",
      "227830  p19999442  s58708861   \n",
      "227831  p19999733  s57132437   \n",
      "227832  p19999987  s55368167   \n",
      "227833  p19999987  s58621812   \n",
      "227834  p19999987  s58971208   \n",
      "\n",
      "                                                 findings  \\\n",
      "0       There is no focal consolidation, pleural effus...   \n",
      "1       The cardiac, mediastinal and hilar contours ar...   \n",
      "2       Single frontal view of the chest provided. \\n ...   \n",
      "3       The lungs are clear of focal consolidation, pl...   \n",
      "4       PA and lateral views of the chest provided.   ...   \n",
      "...                                                   ...   \n",
      "227830  ET tube ends 4.7 cm above the carina.  NG tube...   \n",
      "227831  The lungs are clear, and the cardiomediastinal...   \n",
      "227832  There has been interval extubation and improve...   \n",
      "227833  Portable supine AP view of the chest provided ...   \n",
      "227834  The ET tube terminates approximately 2.9 cm fr...   \n",
      "\n",
      "                                               impression  \\\n",
      "0                       No acute cardiopulmonary process.   \n",
      "1                   No acute cardiopulmonary abnormality.   \n",
      "2                         No acute intrathoracic process.   \n",
      "3                       No acute cardiopulmonary process.   \n",
      "4       Focal consolidation at the left lung base, pos...   \n",
      "...                                                   ...   \n",
      "227830  1.  Lines and tubes are in adequate position. ...   \n",
      "227831                   No acute cardiothoracic process.   \n",
      "227832                                                      \n",
      "227833  Appropriately positioned ET and NG tubes.  Bib...   \n",
      "227834  Slight interval worsening of right lower lung ...   \n",
      "\n",
      "       provisional_findings_impression findings_and_impression  \n",
      "0                                                               \n",
      "1                                                               \n",
      "2                                                               \n",
      "3                                                               \n",
      "4                                                               \n",
      "...                                ...                     ...  \n",
      "227830                                                          \n",
      "227831                                                          \n",
      "227832                                                          \n",
      "227833                                                          \n",
      "227834                                                          \n",
      "\n",
      "[227835 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "REPORT_PATH = \"/home/yuxiangliao/PhD/data/mimic_cxr_reports_core.json\"\n",
    "df = pd.read_json(REPORT_PATH, orient=\"records\", lines=True)\n",
    "print(df)\n",
    "\n",
    "pid_list = df.loc[:, \"pid\"].to_list()\n",
    "sid_list = df.loc[:, \"sid\"].to_list()\n",
    "findings_list = df.loc[:, \"findings\"].to_list()\n",
    "impression_list = df.loc[:, \"impression\"].to_list()\n",
    "pfi_list = df.loc[:, \"provisional_findings_impression\"].to_list()\n",
    "fai_list = df.loc[:, \"findings_and_impression\"].to_list()\n",
    "\n",
    "DATA_SIZE = len(sid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading [/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint] as mention_proposer.doc_encoder encoder and tokenizer\n",
      "subword_tokenizer:PreTrainedTokenizerFast(name_or_path='/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False), 'additional_special_tokens': ['[SPEAKER_START]', '[SPEAKER_END]']})\n",
      "\n",
      "output_dict:{'tokenized_doc': {'orig_tokens': ['1', '.', ' ', 'Increased', 'diffuse', 'interstitial', 'abnormality', ',', 'likely', 'reflecting', 'worsening', 'mild', 'interstitial', 'pulmonary', 'edema', '.', '\\n ', '2', '.', ' ', 'Decreased', 'bibasilar', 'minimal', 'atelectasis', '.', '\\n ', '3', '.', ' ', 'No', 'evidence', 'of', 'pneumothorax', ',', 'status', 'post', 'placement', 'of', 'new', 'right', 'IJ', 'central', 'venous', 'catheter', '.'], 'sentences': [[112, 479, 1437, 1437, 29746, 41118, 3222, 47228, 36536, 6948, 2156, 533, 10811, 21319, 10439, 3222, 47228, 34049, 4803, 8557, 479, 1437, 50118, 1437, 132, 479, 1437, 1437, 43985, 11835, 741, 1452, 281, 16813, 9865, 23, 6930, 17048, 479, 1437, 50118, 1437, 155, 479, 1437, 1437, 440, 1283, 9, 45001, 6157, 368, 3631, 2156, 2194, 618, 13133, 9, 92, 235, 38, 863, 1353, 21011, 1827, 4758, 31995, 479]], 'sent_len_list': [68], 'tensorized_sent': [tensor([[    0,   112,   479,  1437,  1437, 29746, 41118,  3222, 47228, 36536,\n",
      "          6948,  2156,   533, 10811, 21319, 10439,  3222, 47228, 34049,  4803,\n",
      "          8557,   479,  1437, 50118,  1437,   132,   479,  1437,  1437, 43985,\n",
      "         11835,   741,  1452,   281, 16813,  9865,    23,  6930, 17048,   479,\n",
      "          1437, 50118,  1437,   155,   479,  1437,  1437,   440,  1283,     9,\n",
      "         45001,  6157,   368,  3631,  2156,  2194,   618, 13133,     9,    92,\n",
      "           235,    38,   863,  1353, 21011,  1827,  4758, 31995,   479,     2]])], 'sentence_map': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]), 'subtoken_map': [0, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 14, 15, 16, 16, 16, 17, 18, 19, 19, 20, 20, 21, 21, 21, 21, 22, 23, 23, 23, 24, 25, 25, 25, 26, 27, 28, 28, 29, 30, 31, 32, 32, 32, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, 42, 42, 43, 43, 44]}, 'clusters': [[((13, 19), 'worsening mild interstitial pulmonary edema')], [((30, 37), 'bibasilar minimal atelectasis')], [((46, 52), 'No evidence of pneumothorax')], [((49, 52), 'pneumothorax')], [((58, 66), 'new right IJ central venous catheter')]], 'subtoken_idx_clusters': [[[13, 19]], [[30, 37]], [[46, 52]], [[49, 52]], [[58, 66]]], 'actions': [(0, 'o'), (1, 'o'), (2, 'o'), (3, 'o'), (4, 'o')], 'mentions': [[13, 19], [30, 37], [46, 52], [49, 52], [58, 66]], 'coref_group_list': [[[10, 11, 12, 13, 14]], [[21, 22, 23]], [[29, 30, 31, 32]], [[32]], [[38, 39, 40, 41, 42, 43]]]}\n",
      "\n",
      "clusters:[[((13, 19), 'worsening mild interstitial pulmonary edema')], [((30, 37), 'bibasilar minimal atelectasis')], [((46, 52), 'No evidence of pneumothorax')], [((49, 52), 'pneumothorax')], [((58, 66), 'new right IJ central venous catheter')]]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best\"\n",
    "doc_path = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint\"\n",
    "model = Inference(model_path, doc_path)\n",
    "\n",
    "# doc = \" \".join(open(\"/home/shtoshni/Research/coref_resources/data/ccarol/doc.txt\").readlines())\n",
    "doc = impression_list[sid_list.index(\"s57195248\")]\n",
    "output_dict = model.perform_coreference(doc)\n",
    "print(f\"\\noutput_dict:{output_dict}\")\n",
    "print(f\"\\nclusters:{output_dict['clusters']}\")\n",
    "# [[((0, 14), 'The practice of referring to Voldemort as \" He Who Must Not Be Named \"')], \n",
    "# [((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')], \n",
    "# [((17, 17), 'begun'), ((25, 25), 'This')], \n",
    "# [((32, 32), 'Dumbledore'), ((60, 60), 'he')], \n",
    "# [((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')], \n",
    "# [((47, 47), 'saying'), ((65, 65), 'it')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in output_dict[\"clusters\"]:\n",
    "  if len(cluster) > 1:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((13, 19), 'worsening mild interstitial pulmonary edema')]\n",
      "[((30, 37), 'bibasilar minimal atelectasis')]\n",
      "[((46, 52), 'No evidence of pneumothorax')]\n",
      "[((49, 52), 'pneumothorax')]\n",
      "[((58, 66), 'new right IJ central venous catheter')]\n"
     ]
    }
   ],
   "source": [
    "for cluster in output_dict[\"clusters\"]:\n",
    "  print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 11, 12, 13, 14]]\n",
      "[[21, 22, 23]]\n",
      "[[29, 30, 31, 32]]\n",
      "[[32]]\n",
      "[[38, 39, 40, 41, 42, 43]]\n"
     ]
    }
   ],
   "source": [
    "for cluster in output_dict[\"coref_group_list\"]:\n",
    "  print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic command on mylinux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test paths.model_name=i2b2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model_name, and contine training on that model (when we update the trainer config and want to continue from that checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test paths.model_name=joint_mylinux_quizbowl continue_training=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on arcca\n",
    "\n",
    "12h = 42500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\\n",
    "python3 /scratch/c.c21051562/workspace/fast-coref/src/main.py infra=arcca experiment=arcca_test infra.job_time=50000 infra.job_id=1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval on mylinux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test train=False paths.model_dir=/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval scorer.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/coref_resources/reference-coreference-scorers/scorer.pl muc /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/coref_resources/data/i2b2/conll/0/dev.conll /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best/i2b2/dev.conll none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('corenlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
