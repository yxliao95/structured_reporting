{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import py file to jupyter\n",
    "import os, sys\n",
    "PROJECT_HOME = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref\"\n",
    "PROJECT_SRC = PROJECT_HOME + \"/src\"\n",
    "sys.path.append(os.path.abspath(f\"{PROJECT_SRC}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlStr_inference = \"\"\"\n",
    "metrics:\n",
    "- MUC\n",
    "- Bcub\n",
    "- CEAFE\n",
    "keep_singletons: true\n",
    "seed: 45\n",
    "train: true\n",
    "eval_all: false\n",
    "use_wandb: true\n",
    "paths:\n",
    "  resource_dir: ${infra.work_dir}/../../coref_resources\n",
    "  base_data_dir: ${paths.resource_dir}/data\n",
    "  conll_scorer: ${paths.resource_dir}/reference-coreference-scorers/scorer.pl\n",
    "  base_model_dir: ${infra.work_dir}/../models\n",
    "  model_dir: .//../models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0\n",
    "  best_model_dir: .//../models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/best\n",
    "  model_filename: model.pth\n",
    "  model_name: None\n",
    "  model_name_prefix: coref_\n",
    "  model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/model.pth\n",
    "  best_model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_ontonotes_0eee8b4bcf686c0970cfa299fe9790f0/best/model.pth\n",
    "  doc_encoder_dirname: doc_encoder\n",
    "datasets:\n",
    "  ontonotes:\n",
    "    name: OntoNotes\n",
    "    cluster_threshold: 2\n",
    "    canonical_cluster_threshold: 2\n",
    "    targeted_eval: false\n",
    "    num_train_docs: 2802\n",
    "    num_dev_docs: 343\n",
    "    num_test_docs: 348\n",
    "    has_conll: true\n",
    "    singleton_file: ontonotes/ment_singletons_longformer_speaker/60.jsonlines\n",
    "model:\n",
    "  doc_encoder:\n",
    "    transformer:\n",
    "      name: longformer\n",
    "      model_size: large\n",
    "      model_str: allenai/longformer-large-4096\n",
    "      max_encoder_segment_len: 4096\n",
    "      max_segment_len: 4096\n",
    "    chunking: independent\n",
    "    finetune: true\n",
    "    add_speaker_tokens: true\n",
    "    speaker_start: '[SPEAKER_START]'\n",
    "    speaker_end: '[SPEAKER_END]'\n",
    "  memory:\n",
    "    mem_type:\n",
    "      name: unbounded\n",
    "      max_ents: None\n",
    "      eval_max_ents: None\n",
    "    emb_size: 20\n",
    "    mlp_size: 3000\n",
    "    mlp_depth: 1\n",
    "    sim_func: hadamard\n",
    "    entity_rep: wt_avg\n",
    "    num_feats: 2\n",
    "  mention_params:\n",
    "    max_span_width: 20\n",
    "    ment_emb: attn\n",
    "    use_gold_ments: false\n",
    "    use_topk: false\n",
    "    top_span_ratio: 0.4\n",
    "    emb_size: 20\n",
    "    mlp_size: 3000\n",
    "    mlp_depth: 1\n",
    "    ment_emb_to_size_factor:\n",
    "      attn: 3\n",
    "      endpoint: 2\n",
    "      max: 1\n",
    "  metadata_params:\n",
    "    use_genre_feature: false\n",
    "    default_genre: nw\n",
    "    genres:\n",
    "    - bc\n",
    "    - bn\n",
    "    - mz\n",
    "    - nw\n",
    "    - pt\n",
    "    - tc\n",
    "    - wb\n",
    "optimizer:\n",
    "  init_lr: 0.0003\n",
    "  fine_tune_lr: 1.0e-05\n",
    "  max_gradient_norm: 1.0\n",
    "  lr_decay: linear\n",
    "trainer:\n",
    "  dropout_rate: 0.3\n",
    "  label_smoothing_wt: 0.1\n",
    "  ment_loss: all\n",
    "  normalize_loss: false\n",
    "  max_evals: 20\n",
    "  to_save_model: true\n",
    "  log_frequency: 250\n",
    "  patience: 10\n",
    "  eval_per_k_steps: 5000\n",
    "  num_training_steps: 100000\n",
    "infra:\n",
    "  is_local: false\n",
    "  job_time: 14280\n",
    "  job_id: 72519194\n",
    "  work_dir: ./\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os import path\n",
    "# from model.utils import action_sequences_to_clusters\n",
    "from model.entity_ranking_model import EntityRankingModel\n",
    "# from inference.tokenize_doc import tokenize_and_segment_doc, basic_tokenize_doc\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "inference_config = OmegaConf.create(yamlStr_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_processing.utils import split_into_segments, get_sentence_map\n",
    "\n",
    "\n",
    "class DocumentState:\n",
    "    def __init__(self):\n",
    "        self.sentence_end = []\n",
    "        self.token_end = []\n",
    "        self.orig_tokens = []\n",
    "        self.tokens = []\n",
    "        self.subtokens = []\n",
    "        self.segments = []\n",
    "        self.subtoken_map = []\n",
    "        self.segment_subtoken_map = []\n",
    "        self.sentence_map = []\n",
    "        self.tensorized_sent = []\n",
    "        self.sent_len_list = []\n",
    "\n",
    "    def finalize(self):\n",
    "        subtoken_map = flatten(self.segment_subtoken_map)\n",
    "        num_words = len(flatten(self.segments))\n",
    "        assert num_words == len(subtoken_map), (num_words, len(subtoken_map))\n",
    "\n",
    "        return {\n",
    "            \"orig_tokens\": self.orig_tokens,\n",
    "            \"sentences\": self.segments,\n",
    "            \"sent_len_list\": self.sent_len_list,\n",
    "            \"tensorized_sent\": self.tensorized_sent,\n",
    "            \"sentence_map\": torch.tensor(\n",
    "                get_sentence_map(self.segments, self.sentence_end)\n",
    "            ),\n",
    "            \"subtoken_map\": subtoken_map,\n",
    "        }\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def get_tokenized_doc(doc, subword_tokenizer):\n",
    "    document_state = DocumentState()\n",
    "\n",
    "    word_idx = -1\n",
    "    for sentence in doc:\n",
    "        for word in sentence:\n",
    "            document_state.orig_tokens.append(word)\n",
    "            subtokens = subword_tokenizer.convert_tokens_to_ids(\n",
    "                subword_tokenizer.tokenize(\" \" + word)\n",
    "            )\n",
    "            document_state.tokens.append(word)\n",
    "            document_state.token_end += ([False] * (len(subtokens) - 1)) + [True]\n",
    "            word_idx += 1\n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                document_state.subtokens.append(subtoken)\n",
    "                document_state.sentence_end.append(False)\n",
    "                document_state.subtoken_map.append(word_idx)\n",
    "\n",
    "        document_state.sentence_end[-1] = True\n",
    "\n",
    "    # print(document_state.subtokens)\n",
    "    return document_state\n",
    "\n",
    "\n",
    "def basic_tokenize_doc(doc_str, basic_tokenizer):\n",
    "    doc = []\n",
    "    for sent in basic_tokenizer(doc_str).sents:\n",
    "        wordlist = [str(word) for word in sent]\n",
    "        doc.append(wordlist)\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tokenize_and_segment_doc(\n",
    "    basic_tokenized_doc, subword_tokenizer, max_segment_len=4096\n",
    "):\n",
    "    document_state: DocumentState = get_tokenized_doc(\n",
    "        basic_tokenized_doc, subword_tokenizer\n",
    "    )\n",
    "    document = post_tokenization_processing(\n",
    "        document_state, subword_tokenizer, max_segment_len=max_segment_len\n",
    "    )\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def post_tokenization_processing(\n",
    "    document_state: DocumentState, subword_tokenizer, max_segment_len=4096\n",
    "):\n",
    "    split_into_segments(\n",
    "        document_state,\n",
    "        max_segment_len,\n",
    "        document_state.sentence_end,\n",
    "        document_state.token_end,\n",
    "    )\n",
    "\n",
    "    # sentences = [lm_tokenizer.convert_tokens_to_ids(sent) for sent in document_state.segments]\n",
    "    sent_len_list = [len(sent) for sent in document_state.segments]\n",
    "    document_state.sent_len_list = sent_len_list\n",
    "    document_state.segments_indices = document_state.segments\n",
    "\n",
    "    # # Tensorize sentence - Streaming coreference is done one window at a time, so no padding is required\n",
    "    tensorized_sent = [\n",
    "        torch.unsqueeze(\n",
    "            torch.tensor(\n",
    "                [subword_tokenizer.cls_token_id]\n",
    "                + sent\n",
    "                + [subword_tokenizer.sep_token_id]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        for sent in document_state.segments\n",
    "    ]\n",
    "    document_state.tensorized_sent = tensorized_sent\n",
    "    return document_state.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_sequences_to_clusters(actions, mentions):\n",
    "    clusters = []\n",
    "    cell_to_clusters = {}\n",
    "\n",
    "    for mention, (cell_idx, action_type) in zip(mentions, actions):\n",
    "        # print(f\"mention:{mention}, cell_idx:{cell_idx}, action_type:{action_type}\")\n",
    "        # mention:[17, 17], cell_idx:2, action_type:o\n",
    "        # cell_to_clusters:{0: [[0, 14]], 1: [[5, 5]], 2: [[17, 17]]}\n",
    "        # mention:[19, 19], cell_idx:1, action_type:c\n",
    "        # cell_to_clusters:{0: [[0, 14]], 1: [[5, 5], [19, 19]], 2: [[17, 17]]}\n",
    "        \n",
    "        if action_type == \"c\":\n",
    "            # Insert one to the existing cluster\n",
    "            cell_to_clusters[cell_idx].append(mention)\n",
    "        elif action_type == \"o\":\n",
    "            # Overwrite\n",
    "            if cell_idx in cell_to_clusters:\n",
    "                # Remove (Save) the old cluster and initialize the new\n",
    "                clusters.append(cell_to_clusters[cell_idx])\n",
    "            # Create a cluster \n",
    "            cell_to_clusters[cell_idx] = [mention]\n",
    "        elif action_type == \"n\":\n",
    "            # Directly save the cluster with only one element\n",
    "            clusters.append([mention])\n",
    "        # print(f\"cell_to_clusters:{cell_to_clusters}\")\n",
    "        # print(f\"clusters:{clusters}\")\n",
    "\n",
    "    for cell_idx, cluster in cell_to_clusters.items():\n",
    "        clusters.append(cluster)\n",
    "    # print(f\"final clusters:{clusters}\")\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, model_path, encoder_name=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model\n",
    "        checkpoint = torch.load(\n",
    "            path.join(model_path, \"model.pth\"), map_location=self.device\n",
    "        )\n",
    "        self.config = OmegaConf.create(checkpoint[\"config\"])\n",
    "        if encoder_name is not None:\n",
    "            self.config.model.doc_encoder.transformer.model_str = encoder_name\n",
    "        self.model = EntityRankingModel(self.config.model, self.config.trainer)\n",
    "        self._load_model(checkpoint, model_path, encoder_name=encoder_name)\n",
    "\n",
    "        self.max_segment_len = self.config.model.doc_encoder.transformer.max_segment_len\n",
    "        self.tokenizer = self.model.mention_proposer.doc_encoder.tokenizer\n",
    "\n",
    "    def _load_model(self, checkpoint, model_path, encoder_name=None):\n",
    "        self.model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "        if self.config.model.doc_encoder.finetune:\n",
    "            # Load the document encoder params if encoder is finetuned\n",
    "            if encoder_name is None:\n",
    "                doc_encoder_dir = path.join(\n",
    "                    model_path, self.config.paths.doc_encoder_dirname\n",
    "                )\n",
    "            else:\n",
    "                doc_encoder_dir = encoder_name\n",
    "            # Load the encoder\n",
    "            print(f\"Loading [{doc_encoder_dir}] as mention_proposer.doc_encoder encoder and tokenizer\")\n",
    "            self.model.mention_proposer.doc_encoder.lm_encoder = (\n",
    "                AutoModel.from_pretrained(\n",
    "                    pretrained_model_name_or_path=doc_encoder_dir\n",
    "                )\n",
    "            )\n",
    "            self.model.mention_proposer.doc_encoder.tokenizer = (\n",
    "                AutoTokenizer.from_pretrained(\n",
    "                    pretrained_model_name_or_path=doc_encoder_dir\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.cuda()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_coreference(self, document):\n",
    "        if isinstance(document, list):\n",
    "            # Document is already tokenized\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                document, self.tokenizer, max_segment_len=self.max_segment_len\n",
    "            )\n",
    "        elif isinstance(document, str):\n",
    "            # Raw document string. First perform basic tokenization before further tokenization.\n",
    "            import spacy\n",
    "\n",
    "            basic_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "            basic_tokenized_doc = basic_tokenize_doc(document, basic_tokenizer)\n",
    "            print(f\"subword_tokenizer:{self.tokenizer}\")\n",
    "            tokenized_doc = tokenize_and_segment_doc(\n",
    "                basic_tokenized_doc,\n",
    "                self.tokenizer,\n",
    "                max_segment_len=self.max_segment_len,\n",
    "            )\n",
    "        elif isinstance(document, dict):\n",
    "            tokenized_doc = document\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        pred_mentions, mention_scores, gt_actions, pred_actions = self.model(tokenized_doc)\n",
    "        # print(f\"pred_mentions:{pred_mentions}\")\n",
    "        # The first and last indices of the mentions. The indices refer to the indices of tokenized_doc[\"subtoken_map\"]\n",
    "        # Then we need to use the values in tokenized_doc[\"subtoken_map\"] to map the actual tokens which are stored in tokenized_doc[\"orig_tokens\"]\n",
    "        # pred_mentions:[[0, 14], [5, 5], [17, 17], [19, 19], [25, 25], [32, 32], [35, 35], [35, 37], [43, 44], [47, 47], [48, 53], [48, 54], [60, 60], [65, 65]]\n",
    "\n",
    "        # print(f\"pred_actions:{pred_actions}\")\n",
    "        # The first value is the id of cluster to which the mention belong. \n",
    "        # The second character means action: \n",
    "        # [c] will insert one mention to the existing cluster.\n",
    "        # [o] will create a new cluster with the current mention and waiting for other mentions to add in, \n",
    "        # or save the old cluster and initialize a new cluster if this id already create a cluster before.\n",
    "        # [n] will directly save the cluster with only one mention\n",
    "        # pred_actions:[(0, 'o'), (1, 'o'), (2, 'o'), (1, 'c'), (2, 'c'), (3, 'o'), (1, 'c'), (4, 'o'), (4, 'c'), (5, 'o'), (1, 'c'), (4, 'c'), (3, 'c'), (5, 'c')]\n",
    "\n",
    "        idx_clusters = action_sequences_to_clusters(pred_actions, pred_mentions)\n",
    "        # print(f\"idx_clusters:{idx_clusters}\")\n",
    "        # idx_clusters:[[[0, 14]], [[5, 5], [19, 19], [35, 35], [48, 53]], [[17, 17], [25, 25]], [[32, 32], [60, 60]], [[35, 37], [43, 44], [48, 54]], [[47, 47], [65, 65]]]\n",
    "\n",
    "        # The values in idx_clusters/pred_mentions refers to the indices of tokenized_doc[\"subtoken_map\"], \n",
    "        # while the values in tokenized_doc[\"subtoken_map\"] refers to the indces of tokenized_doc[\"orig_tokens\"].\n",
    "        subtoken_map = tokenized_doc[\"subtoken_map\"]\n",
    "        orig_tokens = tokenized_doc[\"orig_tokens\"]\n",
    "        clusters = []\n",
    "        for idx_cluster in idx_clusters:\n",
    "            cur_cluster = []\n",
    "            for (ment_start, ment_end) in idx_cluster:\n",
    "                cur_cluster.append(\n",
    "                    (\n",
    "                        (ment_start, ment_end),\n",
    "                        \" \".join(\n",
    "                            orig_tokens[\n",
    "                                subtoken_map[ment_start] : subtoken_map[ment_end] + 1\n",
    "                            ]\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            clusters.append(cur_cluster)\n",
    "\n",
    "        return {\n",
    "            \"tokenized_doc\": tokenized_doc,\n",
    "            \"clusters\": clusters,\n",
    "            \"subtoken_idx_clusters\": idx_clusters,\n",
    "            \"actions\": pred_actions,\n",
    "            \"mentions\": pred_mentions,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading [/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint] as mention_proposer.doc_encoder encoder and tokenizer\n",
      "subword_tokenizer:PreTrainedTokenizerFast(name_or_path='/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint', vocab_size=50265, model_max_len=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False), 'additional_special_tokens': ['[SPEAKER_START]', '[SPEAKER_END]']})\n",
      "\n",
      "output_dict:{'tokenized_doc': {'orig_tokens': ['The', 'practice', 'of', 'referring', 'to', 'Voldemort', 'as', '\"', 'He', 'Who', 'Must', 'Not', 'Be', 'Named', '\"', 'might', 'have', 'begun', 'when', 'he', 'used', 'a', 'Taboo', '.', 'This', 'is', ',', 'however', ',', 'unlikely', 'because', 'Dumbledore', 'encouraged', 'using', 'his', 'proper', 'name', 'so', 'as', 'to', 'not', 'fear', 'the', 'name', '.', 'If', 'saying', 'the', 'Dark', 'Lord', '’s', 'name', 'would', 'have', 'endangered', 'people', ',', 'he', 'would', 'not', 'have', 'encouraged', 'it', '.'], 'sentences': [[20, 1524, 9, 5056, 7, 47005, 25, 22, 91, 3394, 8495, 1491, 1456, 30436, 22, 429, 33, 5812, 77, 37, 341, 10, 12765, 3036, 479, 152, 16, 2156, 959, 2156, 3752, 142, 44868, 4446, 634, 39, 4692, 766, 98, 25, 7, 45, 2490, 5, 766, 479, 318, 584, 5, 10524, 5736, 44, 27, 29, 766, 74, 33, 14739, 82, 2156, 37, 74, 45, 33, 4446, 24, 479]], 'sent_len_list': [67], 'tensorized_sent': [tensor([[    0,    20,  1524,     9,  5056,     7, 47005,    25,    22,    91,\n",
      "          3394,  8495,  1491,  1456, 30436,    22,   429,    33,  5812,    77,\n",
      "            37,   341,    10, 12765,  3036,   479,   152,    16,  2156,   959,\n",
      "          2156,  3752,   142, 44868,  4446,   634,    39,  4692,   766,    98,\n",
      "            25,     7,    45,  2490,     5,   766,   479,   318,   584,     5,\n",
      "         10524,  5736,    44,    27,    29,   766,    74,    33, 14739,    82,\n",
      "          2156,    37,    74,    45,    33,  4446,    24,   479,     2]])], 'sentence_map': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'subtoken_map': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 50, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]}, 'clusters': [[((0, 14), 'The practice of referring to Voldemort as \" He Who Must Not Be Named \"')], [((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')], [((21, 23), 'a Taboo')], [((25, 25), 'This')], [((32, 32), 'Dumbledore'), ((60, 60), 'he')], [((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')], [((47, 47), 'saying'), ((65, 65), 'it')], [((58, 58), 'people')]], 'subtoken_idx_clusters': [[[0, 14]], [[5, 5], [19, 19], [35, 35], [48, 53]], [[21, 23]], [[25, 25]], [[32, 32], [60, 60]], [[35, 37], [43, 44], [48, 54]], [[47, 47], [65, 65]], [[58, 58]]], 'actions': [(0, 'o'), (1, 'o'), (1, 'c'), (2, 'o'), (3, 'o'), (4, 'o'), (1, 'c'), (5, 'o'), (5, 'c'), (6, 'o'), (1, 'c'), (5, 'c'), (7, 'o'), (4, 'c'), (6, 'c')], 'mentions': [[0, 14], [5, 5], [19, 19], [21, 23], [25, 25], [32, 32], [35, 35], [35, 37], [43, 44], [47, 47], [48, 53], [48, 54], [58, 58], [60, 60], [65, 65]]}\n",
      "\n",
      "clusters:[[((0, 14), 'The practice of referring to Voldemort as \" He Who Must Not Be Named \"')], [((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')], [((21, 23), 'a Taboo')], [((25, 25), 'This')], [((32, 32), 'Dumbledore'), ((60, 60), 'he')], [((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')], [((47, 47), 'saying'), ((65, 65), 'it')], [((58, 58), 'people')]]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best\"\n",
    "doc_path = \"/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint\"\n",
    "model = Inference(model_path, doc_path)\n",
    "\n",
    "# doc = \" \".join(open(\"/home/shtoshni/Research/coref_resources/data/ccarol/doc.txt\").readlines())\n",
    "doc = (\n",
    "    'The practice of referring to Voldemort as \"He Who Must Not Be Named\" might have begun when he used a '\n",
    "    \"Taboo. This is, however, unlikely because Dumbledore encouraged using his proper name so as to not fear \"\n",
    "    \"the name. If saying the Dark Lord’s name would have endangered people, he would not have encouraged it.\"\n",
    ")\n",
    "output_dict = model.perform_coreference(doc)\n",
    "print(f\"\\noutput_dict:{output_dict}\")\n",
    "print(f\"\\nclusters:{output_dict['clusters']}\")\n",
    "# [[((0, 14), 'The practice of referring to Voldemort as \" He Who Must Not Be Named \"')], \n",
    "# [((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')], \n",
    "# [((17, 17), 'begun'), ((25, 25), 'This')], \n",
    "# [((32, 32), 'Dumbledore'), ((60, 60), 'he')], \n",
    "# [((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')], \n",
    "# [((47, 47), 'saying'), ((65, 65), 'it')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')]\n",
      "[((32, 32), 'Dumbledore'), ((60, 60), 'he')]\n",
      "[((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')]\n",
      "[((47, 47), 'saying'), ((65, 65), 'it')]\n"
     ]
    }
   ],
   "source": [
    "for cluster in output_dict[\"clusters\"]:\n",
    "  if len(cluster) > 1:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 14), 'The practice of referring to Voldemort as \" He Who Must Not Be Named \"')]\n",
      "[((5, 5), 'Voldemort'), ((19, 19), 'he'), ((35, 35), 'his'), ((48, 53), 'the Dark Lord ’s')]\n",
      "[((21, 23), 'a Taboo')]\n",
      "[((25, 25), 'This')]\n",
      "[((32, 32), 'Dumbledore'), ((60, 60), 'he')]\n",
      "[((35, 37), 'his proper name'), ((43, 44), 'the name'), ((48, 54), 'the Dark Lord ’s name')]\n",
      "[((47, 47), 'saying'), ((65, 65), 'it')]\n",
      "[((58, 58), 'people')]\n"
     ]
    }
   ],
   "source": [
    "for cluster in output_dict[\"clusters\"]:\n",
    "  print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic command on mylinux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test paths.model_name=i2b2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model_name, and contine training on that model (when we update the trainer config and want to continue from that checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test paths.model_name=joint_mylinux_quizbowl continue_training=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on arcca\n",
    "\n",
    "12h = 42500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\\n",
    "python3 /scratch/c.c21051562/workspace/fast-coref/src/main.py infra=arcca experiment=arcca_test infra.job_time=50000 infra.job_id=1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval on mylinux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py infra=mylinux experiment=mylinux_test train=False paths.model_dir=/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval scorer.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/coref_resources/reference-coreference-scorers/scorer.pl muc /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/coref_resources/data/i2b2/conll/0/dev.conll /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/joint_best/i2b2/dev.conll none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coref_hf]",
   "language": "python",
   "name": "conda-env-coref_hf-py"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
