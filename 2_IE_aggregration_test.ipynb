{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.path.abspath(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from loguru import logger\n",
    "\n",
    "LOG_ROOT = os.path.abspath(\"./\")\n",
    "LOG_FILE = LOG_ROOT + \"/logs/sr-3.log\"\n",
    "\n",
    "# Remove all handlers and reset stderr\n",
    "logger.remove(handler_id=None)\n",
    "logger.add(\n",
    "    LOG_FILE,\n",
    "    level=\"TRACE\",\n",
    "    mode=\"w\",\n",
    "    backtrace=False,\n",
    "    diagnose=True,\n",
    "    colorize=False,\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    ")\n",
    "logger.info(\"\\r\\n\" + \">\" * 29 + \"\\r\\n\" + \">>> New execution started >>>\" + \"\\r\\n\" + \">\" * 29)\n",
    "# To filter log level: TRACE=5, DEBUG=10, INFO=20, SUCCESS=25, WARNING=30, ERROR=40, CRITICAL=50\n",
    "logger.add(sys.stdout, level=\"INFO\", filter=lambda record: record[\"level\"].no < 40, colorize=True)\n",
    "logger.add(sys.stderr, level=\"ERROR\", backtrace=False, diagnose=True, colorize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Metamap\n",
    "\n",
    "Follow the following instructions:\n",
    "- Install Metamap2020: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/Installation.html \n",
    "- Install additional datasets (2022 Specialist Lexicon, 2022AA UMLS NLM Datasets): https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/additional-tools/DataSetDownload.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the servers started\n",
    "- taggerServer\n",
    "- DisambiguatorServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = \"ps -ef | grep java\"\n",
    "out = os.popen(cmd)\n",
    "print(out.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check metamap human readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess, shlex\n",
    "# text =  \"There is no focal consolidation, pleural effusion or pneumothorax.  Cardiomediastinal silhouette and hilar contours are otherwise unremarkable.\"\n",
    "# input_command = f\"echo -e {text}\"\n",
    "# input_process = subprocess.Popen(shlex.split(input_command), stdout=subprocess.PIPE)\n",
    "# meta_command = \"metamap -V NLM -Z 2022AA -A --silent -I\"\n",
    "# metamap_process = subprocess.Popen(shlex.split(meta_command), stdout=subprocess.PIPE, stdin=input_process.stdout)\n",
    "# output, error = metamap_process.communicate()\n",
    "# print(output.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "REPORT_PATH = \"/home/yuxiangliao/PhD/data/mimic_cxr_reports_core.json\"\n",
    "df = pd.read_json(REPORT_PATH, orient=\"records\", lines=True)\n",
    "print(df)\n",
    "\n",
    "id_list = df.loc[:, \"sid\"].to_list()\n",
    "findings_list = df.loc[:, \"findings\"].to_list()\n",
    "impression_list = df.loc[:, \"impression\"].to_list()\n",
    "pfi_list = df.loc[:, \"provisional_findings_impression\"].to_list()\n",
    "fai_list = df.loc[:, \"findings_and_impression\"].to_list()\n",
    "\n",
    "DATA_SIZE = len(id_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiprocessing in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct metama command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "\n",
    "# Documentation: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "def get_metamap_command():\n",
    "    command = format_command_arg(\"metamap\")\n",
    "    command += format_command_arg(\"-V NLM\")  # Data Version: -V (--mm data version) [Base, USAbase, NLM]\n",
    "    command += format_command_arg(\"-Z 2022AA\")  # Knowledge Source: -Z (--mm data year)\n",
    "    command += format_command_arg(\"-A\")  # Data Model: [-A (--strict model), -C (--relaxed model)]\n",
    "    command += format_command_arg(\"--silent\")  # Hide Header Output: --silent\n",
    "    command += format_command_arg(\n",
    "        \"--JSONn\"\n",
    "    )  # Output format: [-q (--machine output), --XMLf, --XMLn, --XMLf1, --XMLn1, --JSONf, --JSONn, -N (--fielded mmi output), -F (--formal tagger output)]\n",
    "    # command += \" --conj\"                                   # Turn on Conjunction Processing\n",
    "    # command += \" -y\"                                       # Word-Sense Disambiguation: -y (--word sense disambiguation)\n",
    "    # UDA_path = \"/home/yuxiangliao/PhD/UMLS/custom-resources/custom-word-replacement\"\n",
    "    # command += format_command_arg(f\"--UDA {UDA_path}\")     # User-Defined Acronyms/Abbreviations (word replacement): --UDA <file>\n",
    "    # semantic_types = \"virs,cgab,acab,ffas,bpoc,medd,tmco,qlco,qnco,bsoj,blor,fndg,sosy,topp,ortf,patf,dsyn,inpo\"\n",
    "    # commend += f\"-J {semantic_types}\"                      # Retain only Concepts with Specified Semantic Types: -J (--restrict to sts) <list>\n",
    "    # command += format_command_arg(\"-I\")                    # For human readable output\n",
    "    return command\n",
    "\n",
    "\n",
    "def format_command_arg(arg):\n",
    "    return \" \" + arg\n",
    "\n",
    "\n",
    "def run_metamap(startIndex, batch_size):\n",
    "    endIndex = startIndex + batch_size if startIndex + batch_size < DATA_SIZE else DATA_SIZE\n",
    "    input_list = [(record if record else \"None\") for record in findings_list[startIndex:endIndex]]\n",
    "    input = repr(\"\\n\\n\".join(input_list))\n",
    "    input_command = f\"echo -e {input}\"\n",
    "    input_process = subprocess.Popen(shlex.split(input_command), stdout=subprocess.PIPE)\n",
    "\n",
    "    meta_command = get_metamap_command()\n",
    "    metamap_process = subprocess.Popen(shlex.split(meta_command), stdout=subprocess.PIPE, stdin=input_process.stdout)\n",
    "\n",
    "    output_bytes, error_bytes = metamap_process.communicate()\n",
    "    if error_bytes:\n",
    "        logger.error(error_bytes.decode())\n",
    "    return output_bytes.decode(), [startIndex, endIndex], input_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Object for JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concept(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sourceTokens: list,\n",
    "        startPosList: list,\n",
    "        lengthList: list,\n",
    "        umlsCUI: str,\n",
    "        preferedName: str,\n",
    "        hitTerm: str,\n",
    "        categories: list,\n",
    "        isHead: int,\n",
    "        isNegated: int,\n",
    "    ):\n",
    "        self.sourceTokens = sourceTokens\n",
    "        self.startPosList = startPosList\n",
    "        self.lengthList = lengthList\n",
    "        self.umlsCUI = umlsCUI\n",
    "        self.preferedName = preferedName\n",
    "        self.hitTerm = hitTerm\n",
    "        self.categories = categories\n",
    "        self.isHead = isHead\n",
    "        self.isNegated = isNegated\n",
    "\n",
    "\n",
    "class ConceptGroup(object):\n",
    "    def __init__(self):\n",
    "        self.concepts = []\n",
    "\n",
    "    def addConcept(self, concept: Concept):\n",
    "        self.concepts.append(concept)\n",
    "\n",
    "\n",
    "class SyntaxChunk(object):\n",
    "    def __init__(self, text: str, lexicalMatch: str, syntaxType: str, partOfSpeech: str, tokens: list):\n",
    "        self.text = text  # The original form of the text (case sensitive)\n",
    "        self.lexicalMatch = lexicalMatch\n",
    "        self.syntaxType = syntaxType\n",
    "        self.partOfSpeech = partOfSpeech\n",
    "        self.tokens = tokens\n",
    "\n",
    "\n",
    "class Phrase(object):\n",
    "    def __init__(self, text: str, startPos: int, length: int):\n",
    "        self.text = text\n",
    "        self.startPos = startPos\n",
    "        self.length = length\n",
    "        self.syntaxChunks = []\n",
    "        self.mappings = []\n",
    "\n",
    "    def addSyntaxChunk(self, syntaxChunk: SyntaxChunk):\n",
    "        self.syntaxChunks.append(syntaxChunk)\n",
    "\n",
    "    def addConceptGroup(self, conceptGroup: ConceptGroup):\n",
    "        self.mappings.append(conceptGroup)\n",
    "\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, text: str, startPos: int, length: int):\n",
    "        self.text = text\n",
    "        self.startPos = startPos\n",
    "        self.length = length\n",
    "        self.phrases = []\n",
    "\n",
    "    def addPhrase(self, phrase: Phrase):\n",
    "        self.phrases.append(phrase)\n",
    "\n",
    "\n",
    "class Negation(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text: str,\n",
    "        triStartPosList: list,\n",
    "        triLengthList: list,\n",
    "        conceptsCUIs: list,\n",
    "        tarStartPosList: list,\n",
    "        tarLengthList: list,\n",
    "    ):\n",
    "        self.trgger = {\n",
    "            \"text\": text,\n",
    "            \"startPosList\": triStartPosList,\n",
    "            \"lengthList\": triLengthList,\n",
    "        }\n",
    "        self.tarrget = {\n",
    "            \"conceptsCUIs\": conceptsCUIs,\n",
    "            \"startPosList\": tarStartPosList,\n",
    "            \"lengthList\": tarLengthList,\n",
    "        }\n",
    "\n",
    "\n",
    "class Section(object):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.text = \"\"  # context\n",
    "        self.sentences = []\n",
    "        self.negations = []\n",
    "\n",
    "    def addSentence(self, sentence: Sentence):\n",
    "        self.sentences.append(sentence)\n",
    "        self.text += sentence.text\n",
    "\n",
    "    def addNegation(self, negation: Negation):\n",
    "        self.negations.append(negation)\n",
    "\n",
    "\n",
    "class Record(object):\n",
    "    def __init__(self, sid: str):\n",
    "        self.sid = sid\n",
    "        self.sections = []\n",
    "\n",
    "    def addSection(self, section: Section):\n",
    "        self.sections.append(section)\n",
    "\n",
    "    def getFindingSection(self) -> Section:\n",
    "        assert self.sections[0].name == \"findings\"\n",
    "        return self.sections[0]\n",
    "\n",
    "\n",
    "class Records(object):\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "\n",
    "    def addRecord(self, record: Record):\n",
    "        self.records.append(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to resolve specific JSON subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveSyntaxUnit(syntaxUnit):\n",
    "    text = syntaxUnit[\"InputMatch\"]\n",
    "    syntaxType = syntaxUnit[\"SyntaxType\"]\n",
    "    tokens = syntaxUnit[\"Tokens\"]\n",
    "    # Add punc to token list\n",
    "    if not tokens:\n",
    "        logger.trace(f\"Empty token detected: SyntaxType:{syntaxType}, InputMatch:{text}\")\n",
    "        tokens = [text]\n",
    "    try:\n",
    "        lexicalMatch = syntaxUnit[\"LexMatch\"]\n",
    "        partOfSpeech = syntaxUnit[\"LexCat\"]\n",
    "    except KeyError:\n",
    "        lexicalMatch = \"\"\n",
    "        partOfSpeech = \"\"\n",
    "    if text.lower() != lexicalMatch and text.isalnum():\n",
    "        logger.trace(f\"text:[{text}], lexicalMatch:[{lexicalMatch}]\")\n",
    "    return SyntaxChunk(text, lexicalMatch, syntaxType, partOfSpeech, tokens)\n",
    "\n",
    "\n",
    "def resolveConcept(mappingCandidate):\n",
    "    sourceTokens = mappingCandidate[\"MatchedWords\"]\n",
    "    startPosList = [int(i[\"StartPos\"]) for i in mappingCandidate[\"ConceptPIs\"]]\n",
    "    lengthList = [int(i[\"Length\"]) for i in mappingCandidate[\"ConceptPIs\"]]\n",
    "    umlsCUI = mappingCandidate[\"CandidateCUI\"]\n",
    "    preferedName = mappingCandidate[\"CandidatePreferred\"]\n",
    "    hitTerm = mappingCandidate[\"CandidateMatched\"]\n",
    "    categories = mappingCandidate[\"SemTypes\"]\n",
    "    isHead = 1 if mappingCandidate[\"IsHead\"] == \"yes\" else 0\n",
    "    isNegated = 1 if mappingCandidate[\"Negated\"] == \"1\" else 0\n",
    "    return Concept(\n",
    "        sourceTokens, startPosList, lengthList, umlsCUI, preferedName, hitTerm, categories, isHead, isNegated\n",
    "    )\n",
    "\n",
    "\n",
    "def resolveNegation(negation):\n",
    "    trigger = negation[\"NegTrigger\"]\n",
    "    triggerStartPosList = [int(i[\"StartPos\"]) for i in negation[\"NegTriggerPIs\"]]\n",
    "    triggerLengthList = [int(i[\"Length\"]) for i in negation[\"NegTriggerPIs\"]]\n",
    "    conceptCUIs = [i[\"NegConcCUI\"] for i in negation[\"NegConcepts\"]]\n",
    "    targetStartPosList = [int(i[\"StartPos\"]) for i in negation[\"NegConcPIs\"]]\n",
    "    targetLengthList = [int(i[\"Length\"]) for i in negation[\"NegConcPIs\"]]\n",
    "    return Negation(trigger, triggerStartPosList, triggerLengthList, conceptCUIs, targetStartPosList, targetLengthList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to resolve JSON format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMetamapJSON(json_obj, id_subList) -> Records:\n",
    "    records = Records()\n",
    "    for _idx, _document in enumerate(json_obj[\"AllDocuments\"]):\n",
    "        # print(_document.keys())\n",
    "        # print(record['Document']['Negations'])\n",
    "        record = Record(id_subList[_idx])\n",
    "        section = Section(\"findings\")\n",
    "        for _utterance in _document[\"Document\"][\"Utterances\"]:\n",
    "            # print(_utterance.keys())\n",
    "            sentence = Sentence(\n",
    "                text=_utterance[\"UttText\"], startPos=int(_utterance[\"UttStartPos\"]), length=int(_utterance[\"UttLength\"])\n",
    "            )\n",
    "            for _phrase in _utterance[\"Phrases\"]:\n",
    "                # print(_phrase.keys())\n",
    "                phrase = Phrase(\n",
    "                    text=_phrase[\"PhraseText\"],\n",
    "                    startPos=int(_phrase[\"PhraseStartPos\"]),\n",
    "                    length=int(_phrase[\"PhraseLength\"]),\n",
    "                )\n",
    "                for _syntaxUnit in _phrase[\"SyntaxUnits\"]:\n",
    "                    # print(_syntaxUnit.keys())\n",
    "                    syntaxChunk = resolveSyntaxUnit(_syntaxUnit)\n",
    "                    phrase.addSyntaxChunk(syntaxChunk)\n",
    "                for _mapping in _phrase[\"Mappings\"]:\n",
    "                    # print(_mapping.keys())\n",
    "                    conceptGroup = ConceptGroup()\n",
    "                    for _mappingCandidate in _mapping[\"MappingCandidates\"]:\n",
    "                        # print(_mappingCandidate.keys())\n",
    "                        concept = resolveConcept(_mappingCandidate)\n",
    "                        conceptGroup.addConcept(concept)\n",
    "                    phrase.addConceptGroup(conceptGroup)\n",
    "                sentence.addPhrase(phrase)\n",
    "            section.addSentence(sentence)\n",
    "        for _negation in _document[\"Document\"][\"Negations\"]:\n",
    "            negation = resolveNegation(_negation)\n",
    "            section.addNegation(negation)\n",
    "        record.addSection(section)\n",
    "        records.addRecord(record)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to align the metamap output to the spacy output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def align(baseList: Doc, inputTokenGroups):\n",
    "    alignment = [-1] * len(baseList)\n",
    "    for id, tokenGroup in enumerate(inputTokenGroups):\n",
    "        alignment[tokenGroup.start : tokenGroup.end] = [id] * (tokenGroup.end - tokenGroup.start)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def align_byIndex(baseList: Doc, inputIndexGroups):\n",
    "    alignment = [-1] * len(baseList)\n",
    "    for id, indexGroup in enumerate(inputIndexGroups):\n",
    "        alignment[indexGroup[0] : indexGroup[-1] + 1] = [id] * len(indexGroup)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def align_byIndex_individually_withData_noOverlap(baseList: Doc, inputIndexGroups_withData):\n",
    "    alignment = [-1] * len(baseList)\n",
    "    for id, indexGroup_withData in enumerate(inputIndexGroups_withData):\n",
    "        indexGroup = indexGroup_withData[\"indices\"]\n",
    "        extra_str = indexGroup_withData[\"extra_str\"]\n",
    "        for index in indexGroup:\n",
    "            alignment[index] = f\"{id}|{extra_str}\"\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def align_byIndex_individually_withData(baseList: Doc, inputIndexGroups_withData):\n",
    "    alignment = [-1] * len(baseList)\n",
    "    for id, indexGroup_withData in enumerate(inputIndexGroups_withData):\n",
    "        indexGroup = indexGroup_withData[\"indices\"]\n",
    "        extra_str = indexGroup_withData[\"extra_str\"]\n",
    "        for index in indexGroup:\n",
    "            if alignment[index] == -1:\n",
    "                alignment[index] = [extra_str]\n",
    "            else:\n",
    "                alignment[index].append(extra_str)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def getTokenOffset(baseText: str, inputTokens):\n",
    "    startPos = 0\n",
    "    offset = []\n",
    "    for token in inputTokens:\n",
    "        offsetPos = baseText.find(token.text, startPos, len(baseText))\n",
    "        offset.append(offsetPos)\n",
    "        startPos = offsetPos + len(token.text)\n",
    "    return offset\n",
    "\n",
    "\n",
    "def resolveTokenIndices_byPosition(tokenOffset, startPos, length) -> list:\n",
    "    indicesList = []\n",
    "    doInsert = False\n",
    "    posPointer = startPos\n",
    "    for i, currPos in enumerate(tokenOffset):\n",
    "        nextPos = tokenOffset[i + 1] if i + 1 < len(tokenOffset) else tokenOffset[i] + 99\n",
    "        if not doInsert and posPointer >= currPos and posPointer < nextPos:\n",
    "            doInsert = True\n",
    "            posPointer = startPos + length - 1\n",
    "        elif doInsert and posPointer < currPos:\n",
    "            break  # break the loop in advance, othewise will stop when finish the loop.\n",
    "        if doInsert:\n",
    "            indicesList.append(i)\n",
    "    return indicesList\n",
    "\n",
    "\n",
    "def resolveTokenIndices_byPosition_multiToken(tokenOffset, startPosList, lengthList) -> list:\n",
    "    idxList_3d = [\n",
    "        resolveTokenIndices_byPosition(tokenOffset, startPos, length)\n",
    "        for startPos, length in zip(startPosList, lengthList)\n",
    "    ]\n",
    "    idxList_flatten = [idx for idxList in idxList_3d for idx in idxList]\n",
    "    return idxList_flatten\n",
    "\n",
    "\n",
    "def trimIndices(_indices, keepNum):\n",
    "    interval = []\n",
    "    for id, current in enumerate(_indices):\n",
    "        if id == len(_indices) - 1:\n",
    "            break\n",
    "        nextid = id + 1\n",
    "        next = _indices[nextid]\n",
    "        interval.append(next - current)\n",
    "    interval_withIdx = list(enumerate(interval))\n",
    "    trimed_list = sorted(interval_withIdx, key=itemgetter(1))[0 : keepNum - 1]\n",
    "    idx_remained = set()\n",
    "    for i in trimed_list:\n",
    "        idx_remained.add(i[0])\n",
    "        idx_remained.add(i[0] + 1)\n",
    "    return [_indices[i] for i in idx_remained]\n",
    "\n",
    "\n",
    "def replPunc(matchObj):\n",
    "    if matchObj.string == matchObj.group(0):\n",
    "        return matchObj.string\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def findSubString(sourceText, subStr, subStr_tokens, begin):\n",
    "    sourceText = sourceText.lower()\n",
    "    startPos = sourceText.find(subStr.lower(), begin)\n",
    "    if startPos != -1:\n",
    "        return startPos, len(subStr)\n",
    "    else:\n",
    "        # Sometimes metamap will rewrite the text, making the subStr differ to the source text.\n",
    "        # In this case, we use token.\n",
    "        if subStr_tokens:\n",
    "            subStr_tokens = [i.lower() for i in subStr_tokens]\n",
    "            startPos = sourceText.find(subStr_tokens[0], begin)\n",
    "            assert startPos != -1\n",
    "            nextStartPos = startPos + len(subStr_tokens[0])\n",
    "            for token in subStr_tokens[1:]:\n",
    "                nextStartPos = sourceText.find(token, nextStartPos)\n",
    "                nextStartPos += len(token)\n",
    "            assert nextStartPos - startPos > 0\n",
    "            return startPos, nextStartPos - startPos\n",
    "        else:\n",
    "            return begin, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "\n",
    "\n",
    "def classToJSON(obj) -> str:\n",
    "\n",
    "    return jsonpickle.encode(obj, unpicklable=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format metamap outputs so that it can be aligned to spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatMetamapRecord(metamapRecord):\n",
    "    reportText = metamapRecord.getFindingSection().text\n",
    "    phrases = [phrase for sentence in metamapRecord.getFindingSection().sentences for phrase in sentence.phrases]\n",
    "    tokenOffset = spacyOutput.loc[:, SPACY_COLUMN_NAME[\"token_offset\"]].tolist()\n",
    "    phraseIdxGroups = []\n",
    "    syntaxChunkIdxGroups_withData = []\n",
    "    conceptIdxGroup_withData = []\n",
    "    negTriggerGroups_withData = []\n",
    "    negTargetGroups_withData = []\n",
    "    conceptGroupId = 0\n",
    "    negationGroupId = 0\n",
    "    offsetBegin = 0\n",
    "    for phrase in phrases:\n",
    "        phraseIdxList = resolveTokenIndices_byPosition(tokenOffset, phrase.startPos, phrase.length)\n",
    "        phraseIdxGroups.append(phraseIdxList)\n",
    "        for syntaxChunk in phrase.syntaxChunks:\n",
    "            startPos, length = findSubString(reportText, syntaxChunk.text, syntaxChunk.tokens, offsetBegin)\n",
    "            offsetBegin = startPos + length\n",
    "            syntaxChunkIdxGroups_withData.append(\n",
    "                {\n",
    "                    \"indices\": resolveTokenIndices_byPosition(tokenOffset, startPos, length),\n",
    "                    \"extra_str\": f\"{syntaxChunk.syntaxType}|{syntaxChunk.partOfSpeech}|{syntaxChunk.tokens}\",\n",
    "                }\n",
    "            )\n",
    "        for conceptGroup in phrase.mappings:\n",
    "            for concept in conceptGroup.concepts:\n",
    "                conceptIdxList_flatten = resolveTokenIndices_byPosition_multiToken(\n",
    "                    tokenOffset, concept.startPosList, concept.lengthList\n",
    "                )\n",
    "                conceptIdxGroup_withData.append(\n",
    "                    {\n",
    "                        \"indices\": conceptIdxList_flatten,\n",
    "                        \"extra_str\": f\"{conceptGroupId}|{concept.umlsCUI}|{concept.preferedName}({concept.hitTerm})|{','.join(concept.categories)}|{concept.isHead}|{concept.isNegated}\",\n",
    "                    }\n",
    "                )\n",
    "            conceptGroupId += 1\n",
    "    for negation in metamapRecord.getFindingSection().negations:\n",
    "        negTriggerIdxList_flatten = resolveTokenIndices_byPosition_multiToken(\n",
    "            tokenOffset, negation.trgger[\"startPosList\"], negation.trgger[\"lengthList\"]\n",
    "        )\n",
    "        negTargetIdxList_flatten = resolveTokenIndices_byPosition_multiToken(\n",
    "            tokenOffset, negation.tarrget[\"startPosList\"], negation.tarrget[\"lengthList\"]\n",
    "        )\n",
    "        negTriggerGroups_withData.append(\n",
    "            {\n",
    "                \"indices\": negTriggerIdxList_flatten,\n",
    "                \"extra_str\": f\"{negationGroupId}|{','.join([str(i) for i in negTargetIdxList_flatten])}|{','.join(negation.tarrget['conceptsCUIs'])}\",\n",
    "            }\n",
    "        )\n",
    "        negTargetGroups_withData.append(\n",
    "            {\n",
    "                \"indices\": negTargetIdxList_flatten,\n",
    "                \"extra_str\": f\"{negationGroupId}|{','.join([str(i) for i in negTriggerIdxList_flatten])}|{','.join(negation.tarrget['conceptsCUIs'])}\",\n",
    "            }\n",
    "        )\n",
    "        negationGroupId += 1\n",
    "    return (\n",
    "        phraseIdxGroups,\n",
    "        syntaxChunkIdxGroups_withData,\n",
    "        conceptIdxGroup_withData,\n",
    "        negTriggerGroups_withData,\n",
    "        negTargetGroups_withData,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_PREFIXX = \"[sp]\"\n",
    "SPACY_COLUMN_NAME = {\n",
    "    \"token\": SPACY_PREFIXX + \"token\",\n",
    "    \"token_offset\": SPACY_PREFIXX + \"token_offset\",\n",
    "    \"sentence_group\": SPACY_PREFIXX + \"sentence_group\",\n",
    "    \"noun_chunk\": SPACY_PREFIXX + \"noun_chunk\",\n",
    "    \"lemma\": SPACY_PREFIXX + \"lemma\",\n",
    "    \"pos_core\": SPACY_PREFIXX + \"pos_core\",\n",
    "    \"pos_feature\": SPACY_PREFIXX + \"pos_feature\",\n",
    "    \"dependency\": SPACY_PREFIXX + \"dependency\",\n",
    "    \"dependency_head\": SPACY_PREFIXX + \"dependency_head\",\n",
    "    \"dependency_children\": SPACY_PREFIXX + \"dependency_children\",\n",
    "    \"morphology\": SPACY_PREFIXX + \"morphology\",\n",
    "    \"is_alpha\": SPACY_PREFIXX + \"is_alpha\",\n",
    "    \"is_stop\": SPACY_PREFIXX + \"is_stop\",\n",
    "    \"is_pronoun\": SPACY_PREFIXX + \"is_pronoun\",\n",
    "    \"trailing_space\": SPACY_PREFIXX + \"trailing_space\",\n",
    "}\n",
    "METAMAP_PREFIXX = \"[mm]\"\n",
    "METAMAP_COLUMN_NAME = {\n",
    "    \"phrase\": METAMAP_PREFIXX + \"metamap_phrase\",\n",
    "    \"syntax_chunk\": METAMAP_PREFIXX + \"syntax_chunk|syntax_type|pos\",\n",
    "    \"concept\": METAMAP_PREFIXX + \"concept_group|CUI|prefered_name(hit_synonym)|categories|isHead|isNegated\",\n",
    "    \"neg_trigger\": METAMAP_PREFIXX + \"negation_group|target_token_indices|target_CUI\",\n",
    "    \"negated_target\": METAMAP_PREFIXX + \"negation_group|trigger_token_indices|target_CUI\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start CoreNLP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from stanza.server import CoreNLPClient, StartServer\n",
    "\n",
    "# CORENLP_CUSTOM_PROPS = {\n",
    "#     \"annotators\": \"tokenize, ssplit, pos, lemma, ner, depparse, coref\",\n",
    "#     \"coref.algorithm\": \"statistical\",\n",
    "# }\n",
    "# client = CoreNLPClient(\n",
    "#     memory=\"8G\", threads=8, endpoint=\"http://localhost:8801\", be_quiet=True, properties=CORENLP_CUSTOM_PROPS,\n",
    "#     start_server=StartServer.FORCE_START\n",
    "# )\n",
    "# client.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Lock\n",
    "from IPython.display import display, HTML\n",
    "import json, time\n",
    "import spacy\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "\n",
    "# mp.cpu_count()\n",
    "METAMAP_CORES = 8\n",
    "STANZA_CORES = 8\n",
    "CORENLP_CORES = 8\n",
    "BATCH_SIZE = 1\n",
    "DATA_START_POS = 8690  # 8690\n",
    "DATA_END_POS = 8691\n",
    "# DATA_END_INDEX = DATA_SIZE\n",
    "\n",
    "SECTION_FLAG = \"findings\"\n",
    "\n",
    "STANZA_PROCESSOR_DICT = {\n",
    "    \"tokenize\": \"mimic\",\n",
    "    \"pos\": \"mimic\",\n",
    "    \"lemma\": \"mimic\",\n",
    "    \"depparse\": \"mimic\",\n",
    "    # 'sentiment':'sstplus', # Sentiment scores of 0, 1, or 2 (negative, neutral, positive).\n",
    "    \"constituency\": \"wsj\",  # wsj, wsj_bert, wsj_roberta\n",
    "    \"ner\": \"radiology\",\n",
    "}\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_md\", disable=[\"ner\"])\n",
    "nlp_stanza = stanza.Pipeline(\n",
    "    \"en\", processors=STANZA_PROCESSOR_DICT, package=None, download_method=DownloadMethod.REUSE_RESOURCES, verbose=False\n",
    ")  # logging_level='WARN'\n",
    "\n",
    "executor = ProcessPoolExecutor(max_workers=METAMAP_CORES)\n",
    "all_task = [\n",
    "    executor.submit(run_metamap, startIndex, BATCH_SIZE)\n",
    "    for startIndex in range(DATA_START_POS, DATA_END_POS, BATCH_SIZE)\n",
    "]\n",
    "\n",
    "# time0 = time.time()\n",
    "for future in as_completed(all_task):\n",
    "    # time1 = time.time()\n",
    "    # print(f\"Get response from Metamap in: {time1-time0}s\")\n",
    "    # Metamap\n",
    "    metamap_output, idx_inteval, input_list = future.result()\n",
    "    id_subList = id_list[idx_inteval[0] : idx_inteval[1]]\n",
    "    metamap_json_output = list(metamap_output.split(\"\\n\"))[1]  # Only the second line is the required JSON string.\n",
    "    metamap_json_obj = json.loads(metamap_json_output)\n",
    "    parsed_obj_batch = parseMetamapJSON(metamap_json_obj, id_subList)\n",
    "    # print(classToJSON(parsed_obj_batch))\n",
    "\n",
    "    # Stanza\n",
    "    # time2 = time.time()\n",
    "    # in_docs = []\n",
    "    # for record in parsed_obj_batch.records:\n",
    "    #     stanzaDoc = stanza.Document([], text=record.getFindingSection().text)\n",
    "    #     stanzaDoc._sid = record.sid\n",
    "    #     in_docs.append(stanzaDoc)\n",
    "    # # Call the neural pipeline on this list of documents\n",
    "    # # The output is also a list of stanza.Document objects, each output corresponding to an input Document object\n",
    "    # out_docs = nlp_stanza(in_docs)\n",
    "    # # print([i._sid for i in out_docs])\n",
    "    # time3 = time.time()\n",
    "    # print(f\"Finish Stanza batch process in: {time3-time2}s\")\n",
    "\n",
    "    # SpaCy\n",
    "    time4 = time.time()\n",
    "    text_tuples = [(text, {\"record\": record}) for text, record in zip(input_list, parsed_obj_batch.records)]\n",
    "    for doc, context in nlp_spacy.pipe(text_tuples, as_tuples=True):\n",
    "        data = {\n",
    "            SPACY_COLUMN_NAME[\"token\"]: [tok.text for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"token_offset\"]: getTokenOffset(doc.text, doc),\n",
    "            SPACY_COLUMN_NAME[\"sentence_group\"]: align(doc, doc.sents),\n",
    "            SPACY_COLUMN_NAME[\"noun_chunk\"]: align(doc, doc.noun_chunks),\n",
    "            SPACY_COLUMN_NAME[\"lemma\"]: [tok.lemma_ for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"pos_core\"]: [f\"[{tok.pos_}]{spacy.explain(tok.pos_)}\" for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"pos_feature\"]: [f\"[{tok.tag_}]{spacy.explain(tok.tag_)}\" for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"dependency\"]: [f\"[{tok.dep_}]{spacy.explain(tok.dep_)}\" for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"dependency_head\"]: [tok.head.text for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"dependency_children\"]: [[child for child in tok.children] for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"morphology\"]: [tok.morph for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"is_alpha\"]: [tok.is_alpha for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"is_stop\"]: [tok.is_stop for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"is_pronoun\"]: [True if tok.pos_ == \"PRON\" else False for tok in doc],\n",
    "            SPACY_COLUMN_NAME[\"trailing_space\"]: [True if tok.whitespace_ else False for tok in doc],\n",
    "        }\n",
    "        spacyOutput = pd.DataFrame(data=data)\n",
    "\n",
    "        # Metamap\n",
    "        metamapRecord = context[\"record\"]  # the Record obj resolved from metamap output\n",
    "        # print(metamapRecord.sid)\n",
    "        # display(HTML(spacyOutput.to_html()))\n",
    "        phraseInfo, syntaxChunkInfo, conceptInfo, negTriggerInfo, negTargetInfo = formatMetamapRecord(metamapRecord)\n",
    "\n",
    "        metamapOutput = pd.DataFrame(\n",
    "            {\n",
    "                METAMAP_COLUMN_NAME[\"phrase\"]: align_byIndex(doc, phraseInfo),\n",
    "                METAMAP_COLUMN_NAME[\"syntax_chunk\"]: align_byIndex_individually_withData_noOverlap(\n",
    "                    doc, syntaxChunkInfo\n",
    "                ),\n",
    "                METAMAP_COLUMN_NAME[\"concept\"]: align_byIndex_individually_withData(doc, conceptInfo),\n",
    "                METAMAP_COLUMN_NAME[\"neg_trigger\"]: align_byIndex_individually_withData(doc, negTriggerInfo),\n",
    "                METAMAP_COLUMN_NAME[\"negated_target\"]: align_byIndex_individually_withData(doc, negTargetInfo),\n",
    "            }\n",
    "        )\n",
    "        output = spacyOutput.join(metamapOutput)\n",
    "\n",
    "        # CoreNLP\n",
    "        # time41 = time.time()\n",
    "        # corenlp_doc = client.annotate(metamapRecord.getFindingSection().text)\n",
    "        # time42 = time.time()\n",
    "        # print(f\"Get response from CoreNLP for single record in: {time42-time41}s\")\n",
    "\n",
    "    # time5 = time.time()\n",
    "    # print(f\"Finish Spacy+CoreNLP batch process in: {time5-time4}s\")\n",
    "    # time0 = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check coreNLP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "corenlp_out = requests.post(\n",
    "    'http://[::]:8801/?properties={\"annotators\":\"coref\",\"coref.algorithm\":\"statistical\",\"outputFormat\":\"json\"}',\n",
    "    data=findings_list[8690].encode(),\n",
    ").text\n",
    "\n",
    "print(corenlp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp_json = json.loads(corenlp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORENLP_PREFIXX = \"[co]\"\n",
    "CORENLP_COLUMN_NAME = {\n",
    "    \"token\": CORENLP_PREFIXX + \"token\",\n",
    "    \"pos\": CORENLP_PREFIXX + \"pos\",\n",
    "    \"lemma\": CORENLP_PREFIXX + \"lemma\",\n",
    "    \"corefMentionIndex\": CORENLP_PREFIXX + \"corefMentionIndex\",\n",
    "}\n",
    "\n",
    "tokenOffset_spacy = spacyOutput.loc[:, SPACY_COLUMN_NAME[\"token_offset\"]].tolist()\n",
    "\n",
    "\n",
    "def align_byIndex_individually(length, inputIndexGroups):\n",
    "    alignment = [-1] * length\n",
    "    for id, indexGroup in enumerate(inputIndexGroups):\n",
    "        for index in indexGroup:\n",
    "            if alignment[index] == -1:\n",
    "                alignment[index] = [id]\n",
    "            else:\n",
    "                alignment[index].append(id)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def align_byIndex_individually_withData_noOverlap1(length, inputIndexGroups_withData):\n",
    "    alignment = [-1] * length\n",
    "    for id, indexGroup_withData in enumerate(inputIndexGroups_withData):\n",
    "        indexGroup = indexGroup_withData[\"indices\"]\n",
    "        extra_str = indexGroup_withData[\"extra_str\"]\n",
    "        for index in indexGroup:\n",
    "            alignment[index] = f\"{id}|{extra_str}\"\n",
    "    return alignment\n",
    "\n",
    "def align_byIndex_individually_withData_dictInList(tokNum, inputDictList):\n",
    "    alignment = [-1] * tokNum\n",
    "    for elementDict in inputDictList:\n",
    "        index = elementDict['index']\n",
    "        extra_str = elementDict['extra_str']\n",
    "        if alignment[index] == -1:\n",
    "            alignment[index] = [extra_str]\n",
    "        else:\n",
    "            alignment[index].append(extra_str)\n",
    "    return alignment\n",
    "\n",
    "referTo_spacy = [\n",
    "    resolveTokenIndices_byPosition(tokenOffset_spacy, token['characterOffsetBegin'], token['characterOffsetEnd'] - token['characterOffsetBegin'])[0]\n",
    "    for sentence in corenlp_json['sentences']\n",
    "    for token in sentence['tokens']\n",
    "]\n",
    "\n",
    "sentenceFirstTokenIndex_offset = [0]\n",
    "tokenTotalNum = 0\n",
    "for sentId, sentence in enumerate(corenlp_json['sentences']):\n",
    "    tokenNum = len(sentence['tokens'])\n",
    "    tokenTotalNum += tokenNum\n",
    "    nextOffset = sentenceFirstTokenIndex_offset[sentId] + tokenNum\n",
    "    sentenceFirstTokenIndex_offset.append(nextOffset)\n",
    "\n",
    "dep_list = []\n",
    "depPlus_list = []\n",
    "depPlusPlus_list = []\n",
    "for sentId, sentence in enumerate(corenlp_json['sentences']):\n",
    "    for basicDep in sentence['basicDependencies']:\n",
    "        depTag = basicDep['dep']\n",
    "        headTok = basicDep['governorGloss']\n",
    "        headTokIdx = basicDep['governor'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        headTokIdx_inSpacy = referTo_spacy[headTokIdx]\n",
    "        currentTok = basicDep['dependentGloss']\n",
    "        currentTokIdx = basicDep['dependent'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        currentTokIdx_inSpacy = referTo_spacy[currentTokIdx]\n",
    "        # Make the root dep token point to itself, just like what spacy did.\n",
    "        if depTag == \"ROOT\" and headTok == \"ROOT\" and basicDep['governor'] == 0:\n",
    "            headTok = currentTok\n",
    "            headTokIdx = currentTokIdx\n",
    "            headTokIdx_inSpacy = currentTokIdx_inSpacy\n",
    "        dep_list.append({\n",
    "            'tag':depTag,\n",
    "            'currTok':currentTok,\n",
    "            'currIdx':currentTokIdx,\n",
    "            'currIdx_algin': currentTokIdx_inSpacy,\n",
    "            'headTok':headTok,\n",
    "            'headIdx':headTokIdx,\n",
    "            'headIdx_align':headTokIdx_inSpacy,\n",
    "            'index':currentTokIdx,\n",
    "            'extra_str':f\"{depTag}|{headTok}|{headTokIdx_inSpacy}\",\n",
    "        })\n",
    "    for depPlus in sentence['enhancedDependencies']:\n",
    "        depTag = depPlus['dep']\n",
    "        headTok = depPlus['governorGloss']\n",
    "        headTokIdx = depPlus['governor'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        headTokIdx_inBase = referTo_spacy[headTokIdx]\n",
    "        currentTok = depPlus['dependentGloss']\n",
    "        currentTokIdx = depPlus['dependent'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        currentTokIdx_inBase = referTo_spacy[currentTokIdx]\n",
    "        # Make the root dep token point to itself, just like what spacy did.\n",
    "        if depTag == \"ROOT\" and headTok == \"ROOT\" and depPlus['governor'] == 0:\n",
    "            headTok = currentTok\n",
    "            headTokIdx = currentTokIdx\n",
    "            headTokIdx_inBase = currentTokIdx_inBase\n",
    "        depPlus_list.append({\n",
    "            'index':currentTokIdx,\n",
    "            'extra_str':f\"{depTag}|{headTok}|{headTokIdx_inBase}\", # We use headTokIdx_inBase as all the rows/tokens will finally align to df_base (df_spacy)\n",
    "        })\n",
    "    for depPlusPlus in sentence['enhancedPlusPlusDependencies']:\n",
    "        depTag = depPlusPlus['dep']\n",
    "        headTok = depPlusPlus['governorGloss']\n",
    "        headTokIdx = depPlusPlus['governor'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        headTokIdx_inBase = referTo_spacy[headTokIdx]\n",
    "        currentTok = depPlusPlus['dependentGloss']\n",
    "        currentTokIdx = depPlusPlus['dependent'] - 1 + sentenceFirstTokenIndex_offset[sentId]\n",
    "        currentTokIdx_inBase = referTo_spacy[currentTokIdx]\n",
    "        # Make the root dep token point to itself, just like what spacy did.\n",
    "        if depTag == \"ROOT\" and headTok == \"ROOT\" and depPlusPlus['governor'] == 0:\n",
    "            headTok = currentTok\n",
    "            headTokIdx = currentTokIdx\n",
    "            headTokIdx_inBase = currentTokIdx_inBase\n",
    "        depPlusPlus_list.append({\n",
    "            'index':currentTokIdx,\n",
    "            'extra_str':f\"{depTag}|{headTok}|{headTokIdx_inBase}\", # We use headTokIdx_inBase as all the rows/tokens will finally align to df_base (df_spacy)\n",
    "        })\n",
    "# dep_list.sort(key=lambda ele:ele['currIdx'])\n",
    "\n",
    "corefGroups = []\n",
    "corefMetionGroups_withData = []\n",
    "for corefChain in corenlp_json['corefs'].values():\n",
    "    corefGroup = []\n",
    "    for mention in corefChain:\n",
    "        sentenceFirstIndexStart = sentenceFirstTokenIndex_offset[mention['sentNum']-1]\n",
    "        beginIndex = sentenceFirstIndexStart + mention['startIndex']- 1\n",
    "        endIndex = sentenceFirstIndexStart + mention['endIndex'] - 1  # The index of the next token of the target token\n",
    "        mentionIndices = [i for i in range(beginIndex, endIndex)]\n",
    "        corefGroup.append(mentionIndices)\n",
    "        mentionType = mention['type']\n",
    "        corefMetionGroups_withData.append(\n",
    "            {\"indices\": mentionIndices, \"extra_str\": mentionType,}\n",
    "        )\n",
    "    corefGroup_flatten = [indices for mention in corefGroup for indices in mention]\n",
    "    corefGroups.append(corefGroup_flatten)\n",
    "\n",
    "df_corenlp_length = tokenTotalNum\n",
    "tokenOffset_spacy = spacyOutput.loc[:, SPACY_COLUMN_NAME[\"token_offset\"]].tolist()\n",
    "\n",
    "df_corenlp = pd.DataFrame(\n",
    "    {\n",
    "        # 'referTo_spacy':referTo_spacy,\n",
    "        CORENLP_COLUMN_NAME[\"token\"]: [\n",
    "            token['originalText'] for sentence in corenlp_json['sentences'] for token in sentence['tokens']\n",
    "        ],\n",
    "        CORENLP_COLUMN_NAME[\"pos\"]: [token['pos'] for sentence in corenlp_json['sentences'] for token in sentence['tokens']],\n",
    "        CORENLP_COLUMN_NAME[\"lemma\"]: [token['lemma'] for sentence in corenlp_json['sentences'] for token in sentence['tokens']],\n",
    "        \"corefId|corefType\": align_byIndex_individually_withData_noOverlap1(\n",
    "            df_corenlp_length, corefMetionGroups_withData\n",
    "        ),\n",
    "        \"corefGroup\": align_byIndex_individually(df_corenlp_length, corefGroups),\n",
    "        \"dep_tag\":[item['tag'] for item in dep_list],\n",
    "        \"dep_head\":[f\"{item['headTok']}|{item['headIdx_align']}\" for item in dep_list],\n",
    "        \"dep\":align_byIndex_individually_withData_dictInList(df_corenlp_length, dep_list),\n",
    "        \"dep+\":align_byIndex_individually_withData_dictInList(df_corenlp_length, depPlus_list),\n",
    "        \"dep++\":align_byIndex_individually_withData_dictInList(df_corenlp_length, depPlusPlus_list),\n",
    "        # })\n",
    "    },\n",
    "    index=referTo_spacy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(df_corenlp.to_html()))\n",
    "# df_new = output.join(df_corenlp)\n",
    "# display(HTML(df_new.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check metamap output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output[SPACY_COLUMN_NAME[\"pos_core\"]].str.contains(\"PRON\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"/home/yuxiangliao/PhD/output/s53741303_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(output.to_html()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:corenlp]",
   "language": "python",
   "name": "conda-env-corenlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
