{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve the annotated data\n",
    "\n",
    "Run prepare_ann_data.ipynb to get necessary resources\n",
    "\n",
    "Put the annotated data into /resources/radgraph_plus\n",
    "\n",
    "Check /src/config/graph_annotation_process/resolve_brat.yaml for settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_file\n",
    "FILE_CHECKER = FileChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine:\n",
      "  work_dir: /home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting\n",
      "  fast_coref_dir: /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref\n",
      "work_dir: ${machine.work_dir}\n",
      "src_dir: ${work_dir}/src\n",
      "output_dir: ${work_dir}/output\n",
      "resource_dir: ${work_dir}/resources\n",
      "base_output_dir: ${work_dir}/output\n",
      "mimic_cxr_output_dir: ${base_output_dir}/mimic_cxr\n",
      "log_dir: ${work_dir}/logs/${hydra.job.config_name}\n",
      "logging_level: INFO\n",
      "ignore_source_path: ${work_dir}/config/ignore/common_ignore\n",
      "fast_coref_dir: ${machine.fast_coref_dir}\n",
      "coref_scorer_dir: ${machine.fast_coref_dir}/coref_resources/reference-coreference-scorers\n",
      "input:\n",
      "  base_dir: ${resource_dir}/radgraph_plus\n",
      "  annotator_L: ${input.base_dir}/annotator_L\n",
      "  annotator_X: ${input.base_dir}/annotator_X\n",
      "output:\n",
      "  base_dir: ${base_output_dir}/radgraph_plus\n",
      "brat_source:\n",
      "  for_ann_dir: ${base_output_dir}/radgraph/brat_data_for_annotation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"radgraph_to_brat\"):\n",
    "        config = compose(config_name=\"graph_annotation_process\", overrides=[\"graph_annotation_process@_global_=resolve_brat\"])\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "Have one annotator's result so far.\n",
    "\n",
    "Automatically detect whether a report is annotated or not.\n",
    "\n",
    "For .txt files, only the first line is the vaild reports. \n",
    "For .ann files, ignore the indices that are not in the first line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all files to be resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict:dict[str,set] = defaultdict(set)\n",
    "for root_path, dir_list, file_list in os.walk(config.input.annotator_L):\n",
    "    for file_name in FILE_CHECKER.filter(file_list):\n",
    "        file_name_prefix = file_name[:-4]\n",
    "        data_dict[root_path].add(file_name_prefix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and get newly annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the reports are not annotated yet, so we need to distinguish them.\n",
    "def get_new_ann_results(root_path, file_prefix, ann_lines):\n",
    "    old_label_list = get_old_labels(root_path,file_prefix)\n",
    "    new_lines = [line for line in ann_lines if line.split(\"\\t\")[0] not in old_label_list]\n",
    "    return new_lines\n",
    "    \n",
    "def get_old_labels(root_path,file_prefix):\n",
    "    dataset_name = root_path.split(os.sep)[-2]\n",
    "    datasplit_name = root_path.split(os.sep)[-1]\n",
    "    old_label_dir = os.path.join(config.brat_source.for_ann_dir, dataset_name, \"label_in_use\", datasplit_name)\n",
    "    with open(os.path.join(old_label_dir, file_prefix+\".txt\"),\"r\",encoding=\"utf-8\") as f:\n",
    "        old_labels = f.readlines()\n",
    "    return [i.strip() for i in old_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data classes and resolving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "\n",
    "class AnnClass(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, _id, _type) -> None:\n",
    "        self.id = _id\n",
    "        self.type = _type\n",
    "        \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def resolve_line(cls, ann_line:str) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_ann_str(self) -> str:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_json_dict(self) -> dict:\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "class AnnEntityClass(AnnClass):\n",
    "    def __init__(self, _id, _type, _start_index, _end_index, _token_str) -> None:\n",
    "        super().__init__(_id, _type)\n",
    "        self.start_index = _start_index\n",
    "        self.end_index = _end_index\n",
    "        self.token_str = _token_str\n",
    "    \n",
    "    @classmethod\n",
    "    def resolve_line(cls, ann_line:str):\n",
    "        pattern = r\"(.*)\\t(.*) (\\d*) (\\d*)\\t(.*)\"\n",
    "        match_obj = re.match(pattern, ann_line.strip())\n",
    "        return cls(*match_obj.groups())\n",
    "        \n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def get_json_dict(self) -> dict:\n",
    "        return {\n",
    "            \"tokens\": self.token_str,\n",
    "            \"label\": self.type,\n",
    "            \"start_idx\": self.start_index,\n",
    "            \"end_index\": self.end_index\n",
    "        }\n",
    "        \n",
    "    def __eq__(self, o):\n",
    "        if isinstance(o, AnnEntityClass):\n",
    "            return self.id == o.id\n",
    "        else:\n",
    "            return self.id == o\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "class AnnRelationClass(AnnClass):\n",
    "    def __init__(self, _id, _type, _arg1, _arg2) -> None:\n",
    "        super().__init__(_id, _type)\n",
    "        self.arg1 = _arg1\n",
    "        self.arg2 = _arg2\n",
    "        self._sorted_arg1 = None\n",
    "        self._sorted_arg2 = None\n",
    "    \n",
    "    @classmethod\n",
    "    def resolve_line(cls, ann_line:str):\n",
    "        pattern = r\"(.*)\\t(.*) Arg1:(.*) Arg2:(.*)\"\n",
    "        match_obj = re.match(pattern, ann_line.strip())\n",
    "        return cls(*match_obj.groups())\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "    \n",
    "    def get_json_dict(self) -> dict:\n",
    "        return {\n",
    "            \"label\": self.type,\n",
    "            \"entity1\": self._sorted_arg1,\n",
    "            \"entity2\": self._sorted_arg2\n",
    "        }\n",
    "\n",
    "class AnnAttributeClass(AnnClass):\n",
    "    def __init__(self, _id, _type, _referred_id, _type_content=\"\") -> None:\n",
    "        super().__init__(_id, _type)\n",
    "        self.referred_id = _referred_id\n",
    "        self.type_content = _type_content\n",
    "        self._sorted_referred_id = None\n",
    "        \n",
    "    @classmethod\n",
    "    def resolve_line(cls, ann_line:str):\n",
    "        pattern = r\"(.*)\\t(.*)\"\n",
    "        match_obj = re.match(pattern, ann_line.strip())\n",
    "        return cls(match_obj.group(1), *match_obj.group(2).split(\" \"))\n",
    "        \n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} {self.referred_id} {self.type_content}\\n\"\n",
    "    \n",
    "    def get_json_dict(self) -> dict:\n",
    "        return {\n",
    "            \"label\": f\"{self.type}:{self.type_content}\" if self.type_content else self.type,\n",
    "            \"entity\": self._sorted_referred_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def resolve_lines(ann_lines:list) -> tuple[list,list,list]:\n",
    "    entity_obj_list = []\n",
    "    relation_obj_list = []\n",
    "    attribute_obj_list = []\n",
    "    for line in ann_lines:\n",
    "        try:\n",
    "            if line[0] == \"T\":\n",
    "                entity_obj_list.append(AnnEntityClass.resolve_line(line))\n",
    "            elif line[0] == \"R\":\n",
    "                relation_obj_list.append(AnnRelationClass.resolve_line(line))\n",
    "            elif line[0] == \"A\":\n",
    "                attribute_obj_list.append(AnnAttributeClass.resolve_line(line))\n",
    "            else:\n",
    "                raise ValueError(\"Not recoginzed line\", line)\n",
    "        except AttributeError as e:\n",
    "            traceback.print_exc()\n",
    "            print(line)\n",
    "    return entity_obj_list, relation_obj_list, attribute_obj_list\n",
    "\n",
    "def check_index_clash(report:str, entity_obj_list:list[AnnEntityClass]):\n",
    "    for entity_obj in entity_obj_list:\n",
    "        if entity_obj.token_str != report[int(entity_obj.start_index):int(entity_obj.end_index)]:\n",
    "            raise ValueError(f\"`{entity_obj.token_str}` != `{report[int(entity_obj.start_index):int(entity_obj.end_index)]}` at [{entity_obj.start_index}:{entity_obj.end_index}]\")\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert class obj to json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def format_output(report:str, entity_obj_list:list[AnnEntityClass], relation_obj_list:list[AnnRelationClass], attribute_obj_list:list[AnnAttributeClass]):\n",
    "    report_dict = dict()\n",
    "    report_dict[\"text\"] = report\n",
    "    annotation_dict = dict()\n",
    "    report_dict[\"labeler_1\"] = annotation_dict\n",
    "    sorted_entity_obj_list = sorted(entity_obj_list, key=lambda obj: (int(obj.start_index), int(obj.end_index)))\n",
    "    update_sorted_entity_idx_to_objs(sorted_entity_obj_list, relation_obj_list, attribute_obj_list)\n",
    "    annotation_dict[\"entities\"] = ann_objs_to_dict(sorted_entity_obj_list)\n",
    "    annotation_dict[\"relations\"] = ann_objs_to_dict(sorted(relation_obj_list, key=lambda obj: (int(obj.arg1[1:]), int(obj.arg2[1:]))))\n",
    "    annotation_dict[\"attributes\"] = ann_objs_to_dict(sorted(attribute_obj_list, key=lambda obj: int(obj.referred_id[1:])))\n",
    "    return report_dict\n",
    "\n",
    "def update_sorted_entity_idx_to_objs(sorted_entity_obj_list:list[AnnEntityClass], relation_obj_list:list[AnnRelationClass], attribute_obj_list:list[AnnAttributeClass]):\n",
    "    for obj in relation_obj_list:\n",
    "        obj._sorted_arg1 = str(sorted_entity_obj_list.index(obj.arg1))\n",
    "        obj._sorted_arg2 = str(sorted_entity_obj_list.index(obj.arg2))\n",
    "    for obj in attribute_obj_list:\n",
    "        obj._sorted_referred_id = str(sorted_entity_obj_list.index(obj.referred_id))\n",
    "\n",
    "def ann_objs_to_dict(sorted_ann_obj_list:list[AnnClass]):\n",
    "    temp_dict = dict()\n",
    "    for i, obj in enumerate(sorted_ann_obj_list):\n",
    "        temp_dict[str(i)] = obj.get_json_dict()\n",
    "    return temp_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'MIMIC-CXR_test': 50, 'MIMIC-CXR_dev': 20})\n"
     ]
    }
   ],
   "source": [
    "processing_progress = defaultdict(int)\n",
    "for root_path, file_prefix_list in data_dict.items():\n",
    "    datasplit_name = os.path.basename(root_path)\n",
    "    dataset_name = os.path.basename(os.path.dirname(root_path))\n",
    "    output_dir = os.path.join(config.output.base_dir, dataset_name)\n",
    "    check_and_create_dirs(output_dir)\n",
    "    # check_and_remove_file(os.path.join(output_dir, datasplit_name+\".json\")) # Re-create files\n",
    "    data_split_dict = {}\n",
    "    for file_prefix in file_prefix_list:\n",
    "        # Read ann file\n",
    "        with open(os.path.join(root_path, file_prefix+\".ann\"),\"r\",encoding=\"utf-8\") as f_ann:\n",
    "            ann_lines = f_ann.readlines()\n",
    "        new_ann_lines = get_new_ann_results(root_path, file_prefix, ann_lines)\n",
    "        # When the ann file has new annotation data.\n",
    "        if new_ann_lines:\n",
    "            entity_obj_list, relation_obj_list, attribute_obj_list = resolve_lines(new_ann_lines)\n",
    "            # Read txt file\n",
    "            with open(os.path.join(root_path, file_prefix+\".txt\"),\"r\",encoding=\"utf-8\") as f_ann:\n",
    "                report = f_ann.readline()\n",
    "                report = report.strip()\n",
    "            check_index_clash(report, entity_obj_list) # raise ValueError if not valid\n",
    "            report_dict = format_output(report,entity_obj_list, relation_obj_list, attribute_obj_list)\n",
    "            report_dict[\"data_source\"] = dataset_name\n",
    "            report_dict[\"data_split\"] = datasplit_name\n",
    "            data_split_dict[file_prefix] = report_dict\n",
    "            processing_progress[f\"{dataset_name}_{datasplit_name}\"] += 1\n",
    "    with open(os.path.join(output_dir, datasplit_name+\".json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data_split_dict,indent=4))\n",
    "print(processing_progress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structured_reporting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
