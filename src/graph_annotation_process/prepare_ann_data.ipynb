{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for annotation\n",
    "\n",
    "We are annotating the same data as RadGraph, including 50 MIMIC-CXR reports (model test set), and 50 CheXpert reports (for testing generalization)\n",
    "\n",
    "The config file is in \"../config/graph_annotation_process.yaml\"\n",
    "\n",
    "Prepare:\n",
    "1. Put the RadGraph test.json files into /{project_root}/resources/radgraph/\n",
    "2. Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine:\n",
      "  work_dir: /Users/liao/myProjects/VSCode_workspace/structured_reporting\n",
      "  fast_coref_dir: /Users/liao/myProjects/VSCode_workspace/fast-coref\n",
      "work_dir: ${machine.work_dir}\n",
      "src_dir: ${work_dir}/src\n",
      "output_dir: ${work_dir}/output\n",
      "resource_dir: ${work_dir}/resources\n",
      "base_output_dir: ${work_dir}/output\n",
      "mimic_cxr_output_dir: ${base_output_dir}/mimic_cxr\n",
      "i2b2_output_dir: ${base_output_dir}/i2b2\n",
      "radgraph_output_dir: ${base_output_dir}/radgraph\n",
      "log_dir: ${work_dir}/logs/${hydra.job.config_name}\n",
      "logging_level: INFO\n",
      "ignore_source_path: ${work_dir}/config/ignore/common_ignore\n",
      "fast_coref_dir: ${machine.fast_coref_dir}\n",
      "coref_scorer_dir: ${machine.fast_coref_dir}/coref_resources/reference-coreference-scorers\n",
      "input:\n",
      "  base_dir: ${resource_dir}/radgraph\n",
      "  train_file: ${input.base_dir}/train.json\n",
      "  dev_file: ${input.base_dir}/dev.json\n",
      "  test_file: ${input.base_dir}/test.json\n",
      "output:\n",
      "  base_dir: ${radgraph_output_dir}\n",
      "  for_inspection_dir: ${output.base_dir}/originalData_bratFormatted\n",
      "  for_ann_dir: ${output.base_dir}/brat_data_for_annotation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"radgraph_to_brat\"):\n",
    "        config = compose(config_name=\"graph_annotation_process\", overrides=[\"graph_annotation_process@_global_=radgraph\"])\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_list(doc_tokens_list):\n",
    "    pos_list = []\n",
    "    curr_pos = 0\n",
    "    for tok in doc_tokens_list:\n",
    "        pos_list.append(curr_pos)\n",
    "        curr_pos = curr_pos + len(tok)+1\n",
    "    return pos_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class AnnEntityClass:\n",
    "    def __init__(self,incremental_id) -> None:\n",
    "        self.id = f\"T{next(incremental_id)}\"\n",
    "        self.type = \"\"\n",
    "        self.start_index = \"\"\n",
    "        self.end_index = \"\"\n",
    "        self.token_str = \"\"\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "\n",
    "class AnnRelationClass:\n",
    "    __incremental_id = itertools.count(1)\n",
    "\n",
    "    def __init__(self,incremental_id) -> None:\n",
    "        self.id = f\"R{next(incremental_id)}\"\n",
    "        self.type = \"\"\n",
    "        self.arg1 = \"\"\n",
    "        self.arg2 = \"\"\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test.json to brat format\n",
    "\n",
    "Having two annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = config.input.test_file\n",
    "\n",
    "with open(input_file_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    radgraph_test = f.readlines()\n",
    "data_dict = json.loads(\"\".join(radgraph_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_mapper: dict[str, str] = {\"ANAT-DP\": \"Anatomy\", \"OBS-DP\": \"Observation-Present\", \"OBS-DA\": \"Observation-Absent\", \"OBS-U\": \"Observation-Uncertain\"}\n",
    "relation_type_mapper: dict[str, str] = {\"modify\": \"modify\", \"suggestive_of\": \"suggestive_of\", \"located_at\": \"located_at\"}\n",
    "\n",
    "\n",
    "for doc_id, doc_info in data_dict.items():\n",
    "    output_file_name = f'{doc_info[\"data_source\"]}_{doc_id.replace(\"/\",\"_\").strip(\".txt\")}'\n",
    "    \n",
    "    output_dir = os.path.join(config.output.for_ann_dir, doc_info[\"data_source\"], \"brat_data\", doc_info[\"data_split\"])\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # We want to show the unlabelled data and two labelers' result in the same file. \n",
    "    # So we need to offset two labelers' token pos\n",
    "    doc_tokens_list = doc_info[\"text\"].split(\" \")\n",
    "    doc_tokens_pos_list0 = get_pos_list(doc_tokens_list)\n",
    "    doc_tokens_pos_list1 = [i+len(doc_info[\"text\"])+2 for i in doc_tokens_pos_list0]\n",
    "    doc_tokens_pos_list2 = [i+len(doc_info[\"text\"])+2 for i in doc_tokens_pos_list1]\n",
    "    pos_list_mapper:dict[str, list] = {\"labeler_1\":doc_tokens_pos_list1, \"labeler_2\":doc_tokens_pos_list2}\n",
    "\n",
    "    # Create txt file\n",
    "    with open(os.path.join(output_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"])\n",
    "\n",
    "    # Create ann file\n",
    "    ann_entitiy_list: list[AnnEntityClass] = []\n",
    "    ann_relation_list: list[AnnRelationClass] = []\n",
    "    \n",
    "    entity_incremental_id = itertools.count(1)\n",
    "    relation_incremental_id = itertools.count(1)\n",
    "    for labeler_id,doc_tokens_pos_list in pos_list_mapper.items():\n",
    "        # Temperarly save the entiity relations, since one entity may link forward to another unrecorded entity.\n",
    "        # (A, modify, B) == A -> modify -> B\n",
    "        temp_ann_entitiy_dict: dict[str,AnnEntityClass] = {}\n",
    "        temp_raw_relation_list: list[tuple[str,str,str]] = []\n",
    "\n",
    "        entities_dict = doc_info[labeler_id][\"entities\"]\n",
    "        for entity_id, entity_info in entities_dict.items():\n",
    "            ann_entity = AnnEntityClass(entity_incremental_id)\n",
    "            ann_entity.type = entity_type_mapper[entity_info[\"label\"]]\n",
    "            ann_entity.token_str = entity_info[\"tokens\"]\n",
    "            ann_entity.start_index = doc_tokens_pos_list[entity_info[\"start_ix\"]]\n",
    "            ann_entity.end_index = ann_entity.start_index + len(ann_entity.token_str)\n",
    "            for relation in entity_info[\"relations\"]:\n",
    "                temp_raw_relation_list.append((entity_id, relation[0], relation[1]))\n",
    "            temp_ann_entitiy_dict[entity_id] = ann_entity\n",
    "        ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "        \n",
    "        for entity1_idstr, relation_type, entity2_idstr in temp_raw_relation_list:\n",
    "            ann_relation = AnnRelationClass(relation_incremental_id)\n",
    "            ann_relation.type = relation_type\n",
    "            ann_relation.arg1 = temp_ann_entitiy_dict[entity1_idstr].id\n",
    "            ann_relation.arg2 = temp_ann_entitiy_dict[entity2_idstr].id\n",
    "            ann_relation_list.append(ann_relation)\n",
    "    \n",
    "    # Dir to save the labels' id that are created for showing only, \n",
    "    # which should be ignored when resolving the finial ann data.\n",
    "    label_inUse_dir = os.path.join(config.output.for_ann_dir, doc_info[\"data_source\"], \"label_in_use\", doc_info[\"data_split\"])\n",
    "    if not os.path.exists(label_inUse_dir):\n",
    "        os.makedirs(label_inUse_dir)\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file_name+\".ann\"), \"w\", encoding=\"utf-8\") as f1, \\\n",
    "        open(os.path.join(label_inUse_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f2:\n",
    "        for label_obj in ann_entitiy_list+ann_relation_list:\n",
    "            f1.write(label_obj.get_ann_str())\n",
    "            f2.write(str(label_obj.id)+\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert train/dev.json to brat format\n",
    "\n",
    "Having one annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_mapper: dict[str, str] = {\"ANAT-DP\": \"Anatomy\", \"OBS-DP\": \"Observation-Present\", \"OBS-DA\": \"Observation-Absent\", \"OBS-U\": \"Observation-Uncertain\"}\n",
    "relation_type_mapper: dict[str, str] = {\"modify\": \"modify\", \"suggestive_of\": \"suggestive_of\", \"located_at\": \"located_at\"}\n",
    "\n",
    "\n",
    "for input_file_path in [config.input.train_file,config.input.dev_file]:\n",
    "    with open(input_file_path,\"r\",encoding=\"utf-8\") as f:\n",
    "        radgraph_test = f.readlines()\n",
    "        data_dict = json.loads(\"\".join(radgraph_test))\n",
    "\n",
    "    for doc_id, doc_info in data_dict.items():\n",
    "        output_file_name = f'{doc_info[\"data_source\"]}_{doc_id.replace(\"/\",\"_\").strip(\".txt\")}'\n",
    "        \n",
    "        output_dir = os.path.join(config.output.for_ann_dir, doc_info[\"data_source\"], \"brat_data\", doc_info[\"data_split\"])\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # We want to show two labelers' result in the same file. So we need to offset labeler 2's token pos\n",
    "        doc_tokens_list = doc_info[\"text\"].split(\" \")\n",
    "        doc_tokens_pos_list = [i+len(doc_info[\"text\"])+2 for i in get_pos_list(doc_tokens_list)]\n",
    "\n",
    "        # Create txt file\n",
    "        with open(os.path.join(output_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"])\n",
    "\n",
    "        # Create ann file\n",
    "        ann_entitiy_list: list[AnnEntityClass] = []\n",
    "        ann_relation_list: list[AnnRelationClass] = []\n",
    "        \n",
    "        entity_incremental_id = itertools.count(1)\n",
    "        relation_incremental_id = itertools.count(1)\n",
    "        \n",
    "        # Temperarly save the entiity relations, since one entity may link forward to another unrecorded entity.\n",
    "        # (A, modify, B) == A -> modify -> B\n",
    "        temp_ann_entitiy_dict: dict[str,AnnEntityClass] = {}\n",
    "        temp_raw_relation_list: list[tuple[str,str,str]] = []\n",
    "\n",
    "        entities_dict = doc_info[\"entities\"]\n",
    "        for entity_id, entity_info in entities_dict.items():\n",
    "            ann_entity = AnnEntityClass(entity_incremental_id)\n",
    "            ann_entity.type = entity_type_mapper[entity_info[\"label\"]]\n",
    "            ann_entity.token_str = entity_info[\"tokens\"]\n",
    "            ann_entity.start_index = doc_tokens_pos_list[entity_info[\"start_ix\"]]\n",
    "            ann_entity.end_index = ann_entity.start_index + len(ann_entity.token_str)\n",
    "            for relation in entity_info[\"relations\"]:\n",
    "                temp_raw_relation_list.append((entity_id, relation[0], relation[1]))\n",
    "            temp_ann_entitiy_dict[entity_id] = ann_entity\n",
    "        ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "        \n",
    "        for entity1_idstr, relation_type, entity2_idstr in temp_raw_relation_list:\n",
    "            ann_relation = AnnRelationClass(relation_incremental_id)\n",
    "            ann_relation.type = relation_type\n",
    "            ann_relation.arg1 = temp_ann_entitiy_dict[entity1_idstr].id\n",
    "            ann_relation.arg2 = temp_ann_entitiy_dict[entity2_idstr].id\n",
    "            ann_relation_list.append(ann_relation)\n",
    "\n",
    "        # Dir to save the labels' id that are created for showing only, \n",
    "        # which should be ignored when resolving the finial ann data.\n",
    "        label_inUse_dir = os.path.join(config.output.for_ann_dir, doc_info[\"data_source\"], \"label_in_use\", doc_info[\"data_split\"])\n",
    "        if not os.path.exists(label_inUse_dir):\n",
    "            os.makedirs(label_inUse_dir)\n",
    "\n",
    "        with open(os.path.join(output_dir, output_file_name+\".ann\"), \"w\", encoding=\"utf-8\") as f1, \\\n",
    "            open(os.path.join(label_inUse_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f2:\n",
    "            for label_obj in ann_entitiy_list+ann_relation_list:\n",
    "                f1.write(label_obj.get_ann_str())\n",
    "                f2.write(str(label_obj.id)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
