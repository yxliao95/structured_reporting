{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
    "from common_utils.coref_utils import resolve_mention_and_group_num\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs\n",
    "from preprocess_i2b2 import aggregrate_files, I2b2Token, get_file_name_prefix, clean_and_split_line\n",
    "\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"i2b2_for_brat\"):\n",
    "        config = compose(config_name=\"data_preprocessing\", overrides=[\"data_preprocessing@_global_=i2b2\",\"machine=mac\", \"data_dir=/Users/liao/Desktop/DBMI_c2b2_2011_coref\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregrate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(\"/Users/liao/Desktop/DBMI_c2b2_2011_coref\",\"temp\")\n",
    "check_and_create_dirs(temp_dir)\n",
    "docs_dir, chains_dir = aggregrate_files(config, temp_dir)\n",
    "\n",
    "# Check that the files are matched.\n",
    "doc_files = os.listdir(docs_dir)\n",
    "chain_files = os.listdir(chains_dir)\n",
    "assert len(doc_files) == len(chain_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 424/424 [00:00<00:00, 8636.29it/s]\n",
      "  0%|          | 0/424 [00:00<?, ?it/s]Process SpawnProcess-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/liao/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/liao/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/liao/opt/anaconda3/lib/python3.9/concurrent/futures/process.py\", line 237, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/Users/liao/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'batch_processing' on <module '__main__' (built-in)>\n",
      "  0%|          | 0/424 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6_/5km70phn76z130blxtnmg0880000gn/T/ipykernel_69835/474994829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_task\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mdoc_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mSTART_EVENT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "# Process each files\n",
    "def batch_processing(doc_file_path, chain_file_path) -> tuple[str, str, list[list[I2b2Token]]]:\n",
    "    \"\"\" Resolve a single i2b2 document, including a .txt file and a .chains file. \"\"\"\n",
    "    doc_id = get_file_name_prefix(doc_file_path, \".txt\")\n",
    "\n",
    "    # Resolve doc file\n",
    "    sentence_list: list[list[I2b2Token]] = []\n",
    "    with open(doc_file_path, \"r\", encoding=\"UTF-8-sig\") as doc:\n",
    "        tokenId_docwise = 0\n",
    "        # for sentence_id, doc_line in enumerate(doc.readlines()):\n",
    "        #     token_list: list[I2b2Token] = []\n",
    "        #     for tokenId_sentencewise, token_str in enumerate(clean_and_split_line(doc_line, debug_doc=doc_id, debug_sent=sentence_id)):\n",
    "        #         token_list.append(I2b2Token(doc_id, sentence_id, tokenId_sentencewise, tokenId_docwise, token_str))\n",
    "        #         tokenId_docwise += 1\n",
    "        #     sentence_list.append(token_list)\n",
    "\n",
    "    # Resolve chain file (coref cluster)\n",
    "    with open(chain_file_path, \"r\", encoding=\"UTF-8-sig\") as chain:\n",
    "        for cluster_id, cluster in enumerate(chain.readlines()):\n",
    "            for coref in cluster.split(\"||\")[0:-1]:  # Drop the last one, which is the type of the coref\n",
    "                token_range: list[str, str] = coref.split(\" \")[-2:]\n",
    "                start = token_range[0]\n",
    "                end = token_range[1]\n",
    "                # if start == end:\n",
    "                #     sentId, tokId = start.split(\":\")\n",
    "                #     mark = f\"({cluster_id})\"\n",
    "                #     sentence_list[int(sentId) - 1][int(tokId)].add_coref_conllmark(mark)\n",
    "                # else:\n",
    "                #     sentId, tokId = start.split(\":\")\n",
    "                #     startMark = f\"({cluster_id}\"\n",
    "                #     sentence_list[int(sentId) - 1][int(tokId)].add_coref_conllmark(startMark)\n",
    "\n",
    "                #     sentId, tokId = end.split(\":\")\n",
    "                #     endMark = f\"{cluster_id})\"\n",
    "                #     sentence_list[int(sentId) - 1][int(tokId)].add_coref_conllmark(endMark)\n",
    "\n",
    "    return doc_file_path, doc_id, sentence_list\n",
    "\n",
    "all_task = []\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    # Submit task\n",
    "    for _file_name in tqdm(doc_files):\n",
    "        # Input files\n",
    "        doc_file_path = os.path.join(docs_dir, _file_name)\n",
    "        chain_file_path = os.path.join(chains_dir, _file_name + config.input.chain_suffix)\n",
    "        all_task.append(executor.submit(batch_processing, doc_file_path, chain_file_path))\n",
    "\n",
    "    # Notify tasks to start\n",
    "    START_EVENT.set()\n",
    "\n",
    "    # When a submitted task finished, the output is received here.\n",
    "    if all_task:\n",
    "        for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
    "            doc_file_path, doc_id, sentence_list = future.result()\n",
    "    START_EVENT.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
