{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
    "from common_utils.coref_utils import resolve_mention_and_group_num\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
    "from preprocess_i2b2 import aggregrate_files, I2b2Token, get_file_name_prefix, clean_and_split_line\n",
    "\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"i2b2_for_brat\"):\n",
    "        config = compose(config_name=\"data_preprocessing\", overrides=[\"data_preprocessing@_global_=i2b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregrate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(config.temp.dir)\n",
    "check_and_create_dirs(temp_dir)\n",
    "docs_dir, chains_dir = aggregrate_files(config, temp_dir)\n",
    "\n",
    "check_and_remove_dirs(os.path.join(temp_dir, \"docs\",\".ipynb_checkpoints\"), True)\n",
    "# Check that the files are matched.\n",
    "doc_files = os.listdir(docs_dir)\n",
    "chain_files = os.listdir(chains_dir)\n",
    "assert len(doc_files) == len(chain_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnMentionClass:\n",
    "    def __init__(self) -> None:\n",
    "        self.id = \"\"\n",
    "        self.type = \"Mention\"\n",
    "        self.start_index = \"\"\n",
    "        self.end_index = \"\"\n",
    "        self.token_str_list = []\n",
    "    \n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} {self.start_index} {self.end_index}\\t{' '.join(self.token_str_list)}\\n\"\n",
    "\n",
    "class AnnCoreferenceClass:\n",
    "    def __init__(self) -> None:\n",
    "        self.id = \"\"\n",
    "        self.type = \"Coreference\"\n",
    "        self.anaphora = \"\"\n",
    "        self.antecedent = \"\"\n",
    "    \n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.type} Anaphora:{self.anaphora} Antecedent:{self.antecedent}\\t\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 424/424 [00:00<00:00, 25748.32it/s]\n",
      "100%|██████████| 424/424 [00:01<00:00, 354.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each files\n",
    "def batch_processing(doc_file_path, chain_file_path):\n",
    "    \"\"\" Resolve a single i2b2 document, including a .txt file and a .chains file. \"\"\"\n",
    "    doc_id = get_file_name_prefix(doc_file_path, \".txt\")\n",
    "\n",
    "    # Resolve doc file\n",
    "    sentence_list: list[list[I2b2Token]] = []\n",
    "    with open(doc_file_path, \"r\", encoding=\"UTF-8-sig\") as doc:\n",
    "        tokenId_docwise = 0\n",
    "        for sentence_id, doc_line in enumerate(doc.readlines()):\n",
    "            token_list: list[I2b2Token] = []\n",
    "            for tokenId_sentencewise, token_str in enumerate(clean_and_split_line(doc_line, debug_doc=doc_id, debug_sent=sentence_id)):\n",
    "                i2b2_token = I2b2Token(doc_id, sentence_id, tokenId_sentencewise, tokenId_docwise, token_str)\n",
    "                token_list.append(i2b2_token)\n",
    "                tokenId_docwise += 1\n",
    "            sentence_list.append(token_list)\n",
    "\n",
    "    # Resolve chain file (coref cluster)\n",
    "    with open(chain_file_path, \"r\", encoding=\"UTF-8-sig\") as chain:\n",
    "        coref_group_list:list[list[list[I2b2Token]]] = []\n",
    "        for cluster_id, cluster in enumerate(chain.readlines()):\n",
    "            coref_group:list[list[I2b2Token]] = []\n",
    "\n",
    "            for coref in cluster.split(\"||\")[0:-1]:  # Drop the last one, which is the type of the coref\n",
    "                mention_list:list[I2b2Token] = []\n",
    "                token_range: list[str, str] = coref.split(\" \")[-2:]\n",
    "                start = token_range[0]\n",
    "                end = token_range[1]\n",
    "                if start == end:\n",
    "                    sentId, tokId = start.split(\":\")\n",
    "                    mention_list.append(sentence_list[int(sentId) - 1][int(tokId)])\n",
    "                else:\n",
    "                    sentId_start, tokId_start = start.split(\":\")\n",
    "                    sentId_end, tokId_end = end.split(\":\")\n",
    "                    if sentId_start == sentId_end:\n",
    "                        mention_list.extend(sentence_list[int(sentId_start) - 1][int(tokId_start):int(tokId_end)+1])\n",
    "                    else:\n",
    "                        temp_list = sentence_list[int(sentId_start) - 1][int(tokId_start):]\n",
    "                        i = 1\n",
    "                        while int(sentId_start) + i <= int(tokId_end):\n",
    "                            if int(sentId_start) + i == int(tokId_end):\n",
    "                                temp_list.extend(sentence_list[int(sentId_start + i) - 1][:int(tokId_end)+1])\n",
    "                            else:\n",
    "                                temp_list.extend(sentence_list[int(sentId_start + i) - 1][:])\n",
    "                            i += 1\n",
    "                        mention_list.extend(temp_list)\n",
    "            \n",
    "                coref_group.append(mention_list)\n",
    "\n",
    "            coref_group_list.append(coref_group)\n",
    "\n",
    "\n",
    "\n",
    "    output_dir = os.path.join(config.output_base_dir, \"brat_visualization\")\n",
    "    check_and_create_dirs(output_dir)\n",
    "    offset = 0\n",
    "    with open(os.path.join(output_dir,f\"{doc_id}.txt\"), \"w\", encoding=\"UTF-8\") as f:\n",
    "        docStr_list:list[str] = []\n",
    "        for sentence in sentence_list:\n",
    "\n",
    "            sentenceStr_list:list[str] = []\n",
    "            for i2b2_toekn in sentence:\n",
    "                i2b2_toekn.offset = offset\n",
    "                offset += len(i2b2_toekn.tokenStr) + 1\n",
    "                sentenceStr_list.append(i2b2_toekn.tokenStr)\n",
    "\n",
    "            docStr_list.append(\" \".join(sentenceStr_list))\n",
    "        \n",
    "        f.write(\"\\n\".join(docStr_list)) # Offset is correct, as there is no tralling whitespaces but have \\n\n",
    "\n",
    "\n",
    "    mention_id, pair_id = 0, 0\n",
    "    with open(os.path.join(output_dir,f\"{doc_id}.ann\"), \"w\", encoding=\"UTF-8\") as f:\n",
    "        for _coref_list in coref_group_list:\n",
    "\n",
    "            ann_mention_list:list[AnnMentionClass] = []\n",
    "            for _mention_list in _coref_list:\n",
    "\n",
    "                ann_mention_class = AnnMentionClass()\n",
    "                ann_mention_class.id = f\"T{mention_id}\"\n",
    "                ann_mention_class.start_index = _mention_list[0].offset\n",
    "                ann_mention_class.end_index = _mention_list[-1].offset + len(_mention_list[-1].tokenStr)\n",
    "                ann_mention_class.token_str_list.append(_mention_list[0].tokenStr)\n",
    "\n",
    "                if len(_mention_list) > 1:\n",
    "                    for i2b2_token in _mention_list[1:]:\n",
    "                        ann_mention_class.token_str_list.append(i2b2_token.tokenStr)\n",
    "                \n",
    "                f.write(ann_mention_class.get_ann_str())\n",
    "                mention_id+=1\n",
    "\n",
    "                ann_mention_list.append(ann_mention_class)\n",
    "\n",
    "            for _id, ann_mention_class in enumerate(ann_mention_list):\n",
    "                if _id == 0:\n",
    "                    continue\n",
    "\n",
    "                ann_coref_class = AnnCoreferenceClass()\n",
    "                ann_coref_class.id = f\"R{pair_id}\"\n",
    "                ann_coref_class.anaphora = ann_mention_list[_id-1].id\n",
    "                ann_coref_class.antecedent = ann_mention_list[_id].id\n",
    "                \n",
    "                f.write(ann_coref_class.get_ann_str())\n",
    "                pair_id += 1\n",
    "\n",
    "\n",
    "all_task = []\n",
    "with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "    # Submit task\n",
    "    for _file_name in tqdm(doc_files):\n",
    "        # Input files\n",
    "        doc_file_path = os.path.join(docs_dir, _file_name)\n",
    "        chain_file_path = os.path.join(chains_dir, _file_name + config.input.chain_suffix)\n",
    "        all_task.append(executor.submit(batch_processing, doc_file_path, chain_file_path))\n",
    "\n",
    "    # Notify tasks to start\n",
    "    START_EVENT.set()\n",
    "\n",
    "    # When a submitted task finished, the output is received here.\n",
    "    if all_task:\n",
    "        for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
    "            future.result()\n",
    "\n",
    "    START_EVENT.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('corenlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
