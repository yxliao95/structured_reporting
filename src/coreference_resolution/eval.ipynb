{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on MIMIC-CXR manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import display, HTML\n",
    "from common_utils.coref_utils import ConllToken\n",
    "# display(HTML(df.to_html()))\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
    "from common_utils.coref_utils import resolve_mention_and_group_num\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs, check_and_remove_file\n",
    "from coreference_resolution.data_preprocessing.mimic_cxr_csv2conll import copy_and_paste_conll\n",
    "from statistic.coref_socring import invoke_conll_script, resolve_conll_script_output\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conll_score(conll_file_gt, conll_file_pred):\n",
    "    print(\"gt:\", conll_file_gt)\n",
    "    print(\"pred:\", conll_file_pred)\n",
    "    scorer_path = \"./wrong_conll_scorer_example/reference-coreference-scorers/scorer.pl\"\n",
    "    overall_f1 = []\n",
    "    for metric in ['muc', 'bcub', 'ceafe']:\n",
    "        out, err = invoke_conll_script(scorer_path, metric, conll_file_gt, conll_file_pred)\n",
    "        mention_recall, mention_precision, mention_f1, coref_recall, coref_precision, coref_f1 = resolve_conll_script_output(out)\n",
    "        overall_f1.append(coref_f1)\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(f\"mention_recall, mention_precision, mention_f1: {mention_recall}, {mention_precision}, {mention_f1}\")\n",
    "        print(f\"coref_recall, coref_precision, coref_f1: {coref_recall}, {coref_precision}, {coref_f1}\")\n",
    "\n",
    "    print(f\"Overall F1: {sum(overall_f1) / len(overall_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_csv_to_conll(section_name, doc_id, output_file_path, input_file_path_or_df, coref_group_conll_colName, sentence_group_colName,token_colName):\n",
    "    \"\"\" The whitespace tok contained in the csv file will be removed when being converted to conll file \"\"\"\n",
    "    BEGIN = f\"#begin document ({doc_id}_{section_name}); part 0\\n\"\n",
    "    SENTENCE_SEPARATOR = \"\\n\"\n",
    "    END = \"#end document\\n\"\n",
    "\n",
    "    # Resolve CSV file\n",
    "    sentenc_list: list[list[ConllToken]] = []\n",
    "    if isinstance(input_file_path_or_df, str):\n",
    "        df = pd.read_csv(input_file_path_or_df, index_col=0, na_filter=False)\n",
    "    else:\n",
    "        df = input_file_path_or_df\n",
    "\n",
    "    sentence_id = 0\n",
    "    while True:\n",
    "        token_list: list[ConllToken] = []\n",
    "        df_sentence = df[df.loc[:, sentence_group_colName] == sentence_id].reset_index()\n",
    "        if df_sentence.empty:\n",
    "            break\n",
    "        for _idx, data in df_sentence.iterrows():\n",
    "            # Skip all whitespces like \"\\n\", \"\\n \" and \" \".\n",
    "            if str(data[token_colName]).strip() == \"\":\n",
    "                continue\n",
    "            conllToken = ConllToken(doc_id+\"_\"+section_name, sentence_id, _idx, data[token_colName])\n",
    "            coref_col_cell = data[coref_group_conll_colName]\n",
    "            if isinstance(coref_col_cell, str) and coref_col_cell != \"-1\":\n",
    "                conllToken.add_coref_label(\"|\".join(ast.literal_eval(coref_col_cell)))\n",
    "            token_list.append(conllToken)\n",
    "        sentenc_list.append(token_list)\n",
    "        sentence_id += 1\n",
    "\n",
    "    with open(output_file_path, \"a\", encoding=\"UTF-8\") as out:\n",
    "        out.write(BEGIN)\n",
    "        for sent in sentenc_list:\n",
    "            # Skip empty sentence\n",
    "            if len(sent) == 1 and sent[0].tokenStr == \"\":\n",
    "                continue\n",
    "            for tok in sent:\n",
    "                out.write(tok.get_conll_str() + \"\\n\")\n",
    "            out.write(SENTENCE_SEPARATOR)\n",
    "        out.write(END)\n",
    "        out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_list_to_conll(output_file_path, doc_id, section_name, old_sentenc_list):\n",
    "    BEGIN = f\"#begin document ({doc_id}_{section_name}); part 0\\n\"\n",
    "    SENTENCE_SEPARATOR = \"\\n\"\n",
    "    END = \"#end document\\n\"\n",
    "\n",
    "    sentenc_list: list[list[ConllToken]] = []\n",
    "    for sentence_id, old_token_list in enumerate(old_sentenc_list):\n",
    "        token_list: list[ConllToken] = []\n",
    "        for tok_idx, tok in enumerate(old_token_list):\n",
    "            # Skip all whitespces like \"\\n\", \"\\n \" and \" \".\n",
    "            if tok.tokenStr.strip() == \"\":\n",
    "                continue\n",
    "            conllToken = ConllToken(doc_id+\"_\"+section_name, sentence_id, tok_idx, tok.tokenStr)\n",
    "            conllToken.corefLabel = tok.corefLabel\n",
    "            token_list.append(conllToken)\n",
    "        sentenc_list.append(token_list)\n",
    "\n",
    "    with open(output_file_path, \"a\", encoding=\"UTF-8\") as out:\n",
    "        out.write(BEGIN)\n",
    "        for sent in sentenc_list:\n",
    "            # Skip empty sentence\n",
    "            if len(sent) == 1 and sent[0].tokenStr == \"\":\n",
    "                continue\n",
    "            for tok in sent:\n",
    "                out.write(tok.get_conll_str() + \"\\n\")\n",
    "            out.write(SENTENCE_SEPARATOR)\n",
    "        out.write(END)\n",
    "        out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_singleton(df_pred, coref_group_conll_colName):\n",
    "    # remove singleton from df_pred\n",
    "    corefGroup_counter = Counter()\n",
    "    for conll_corefGroup_list_str in df_pred[~df_pred.loc[:, coref_group_conll_colName].isin([\"-1\", -1.0, np.nan])].loc[:, coref_group_conll_colName].to_list():\n",
    "        for conll_corefGroup_str in ast.literal_eval(conll_corefGroup_list_str):\n",
    "            result = re.search(r\"(\\d+)\\)\", conll_corefGroup_str)  # An coref mention always end with \"number)\"\n",
    "            if result:\n",
    "                corefGroup_counter.update([int(result.group(1))])\n",
    "\n",
    "    non_singletone_counter: list[tuple] = list(filter(lambda item: item[1] > 1, corefGroup_counter.items()))\n",
    "    coref_group_list_notSingleton = [int(k) for k, v in non_singletone_counter]\n",
    "    \n",
    "    # iter df rows, keep only the non_singleton coref id, and remove the others.\n",
    "    for idx, item in df_pred.iterrows():\n",
    "        conll_corefGroup_list_str = item.get(coref_group_conll_colName)\n",
    "        new_conll_corefGroup_str_list = []\n",
    "        # Remove singleton id\n",
    "        if conll_corefGroup_list_str in [\"-1\", -1.0, np.nan]:\n",
    "            continue\n",
    "        for conll_corefGroup_str in ast.literal_eval(conll_corefGroup_list_str):\n",
    "            res = re.match(r\"\\(?(\\d+)\\)?\",conll_corefGroup_str)\n",
    "            coref_group_id = int(res.groups()[0])\n",
    "            if coref_group_id in coref_group_list_notSingleton:\n",
    "                new_conll_corefGroup_str_list.append(conll_corefGroup_str)\n",
    "        df_pred.loc[idx,coref_group_conll_colName] = str(new_conll_corefGroup_str_list) if new_conll_corefGroup_str_list else -1\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_tok_as_realworld(df_base, spacy_nametyle):\n",
    "    \"\"\" \n",
    "    Format the input token without any additional processing. \n",
    "    For example, the whitespaces before and after sentences are all remained.\n",
    "    \"\"\"\n",
    "    tok_list = df_base.loc[:, spacy_nametyle.token].to_list()\n",
    "    sentGroup_list = df_base.loc[:, spacy_nametyle.sentence_group].to_list()\n",
    "    sent_tok_2d_list: list[list[str]] = []\n",
    "    for tok, sent_id in zip(tok_list, sentGroup_list):\n",
    "        tok = str(tok)  # In i2b2, some of the tokens might incorrectly be recognized as float type.\n",
    "        if len(sent_tok_2d_list) == sent_id:\n",
    "            sent_tok_2d_list.append([tok])\n",
    "        else:\n",
    "            sent_tok_2d_list[sent_id].append(tok)\n",
    "    return sent_tok_2d_list\n",
    "\n",
    "def format_input_tok_same_as_traingset(df_base, spacy_nametyle):\n",
    "    \"\"\" \n",
    "    Format the input token by using the same approach as creating training sets for fast-coref models \n",
    "    For example, we skipped all whitespces like \"\\n\", \"\\n \" and \" \".\n",
    "    \"\"\"\n",
    "    sent_tok_2d_list: list[list[str]] = []\n",
    "    sentence_id = 0\n",
    "    index_map= [] # map the input token index to the spacy token index.\n",
    "    curr_spacy_index = 0\n",
    "    while True:\n",
    "        token_list2: list[str] = []\n",
    "        df_sentence = df_base[df_base.loc[:, spacy_nametyle.sentence_group] == sentence_id].reset_index()\n",
    "        if df_sentence.empty:\n",
    "            break\n",
    "        for _idx, data in df_sentence.iterrows():\n",
    "            # Skip all whitespces like \"\\n\", \"\\n \" and \" \".\n",
    "            curr_spacy_index += 1\n",
    "            if str(data[spacy_nametyle.token]).strip() == \"\":\n",
    "                continue\n",
    "            conllToken = data[spacy_nametyle.token]\n",
    "            token_list2.append(conllToken)\n",
    "            index_map.append(curr_spacy_index-1)\n",
    "            \n",
    "        sent_tok_2d_list.append(token_list2)\n",
    "        sentence_id += 1\n",
    "    return sent_tok_2d_list, index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_spacy(tok_indices_in_spacy, label_aligned_to_input, input_tok_list, spacy_tok_list):\n",
    "    aligned_to_spacy_tok = [-1] * len(spacy_tok_list)\n",
    "    for idx_in_spacy, label, input_tok in zip(tok_indices_in_spacy, label_aligned_to_input, input_tok_list):\n",
    "        assert input_tok == spacy_tok_list[idx_in_spacy]\n",
    "        aligned_to_spacy_tok[idx_in_spacy] = label\n",
    "    return aligned_to_spacy_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify this if gt file has changed###\n",
    "output_dir = \"../../resources/eval\"\n",
    "source_input_csv_dir = \"../../output/mimic_cxr/manual_test_set/round1x2\"\n",
    "source_input_conll_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth/round1x2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of gt docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict_allDoc:dict[str,list] = {}\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    gt_all_dir = os.path.join(source_input_csv_dir, section_name)\n",
    "    gt_dict_allDoc[section_name] = [i.rstrip(\".csv\") for i in FILE_CHECKER.filter(os.listdir(gt_all_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of gt docs that has coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict_hasCoref:dict[str,list] = {}\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    gt_all_dir = os.path.join(source_input_conll_dir, section_name)\n",
    "    gt_dict_hasCoref[section_name] = [i.rstrip(\".conll\") for i in FILE_CHECKER.filter(os.listdir(gt_all_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general gt conll file\n",
    "\n",
    "The conll file is equivalent to the conll file used in fast-coref models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_conll_file_gt = os.path.join(output_dir, \"gt_200_noWhich.conll\")\n",
    "check_and_remove_file(output_conll_file_gt)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in doc_list:\n",
    "        input_conll_file = os.path.join(source_input_conll_dir, section_name, f\"{doc_id}.conll\")\n",
    "        if os.path.exists(input_conll_file):\n",
    "            copy_and_paste_conll(input_conll_file, output_conll_file_gt)\n",
    "        else:\n",
    "            input_csv_file = os.path.join(source_input_csv_dir, section_name, f\"{doc_id}.csv\")\n",
    "            from_csv_to_conll(section_name, doc_id, output_conll_file_gt, input_csv_file, \"[gt]coref_group_conll\", \"[sp]sentence_group\",\"[sp]token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1-1: ensemble (majority voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Eval (has coref / no coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of predict docs that has coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict_hasCoref: dict[str, list[str]] = {}\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    ensemble_dir = os.path.join(\"../../output/mimic_cxr/coref/individual_conll\", section_name)\n",
    "    pred_dict_hasCoref[section_name] = [i.rstrip(\".conll\") for i in FILE_CHECKER.filter(os.listdir(ensemble_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = {}\n",
    "false_positive = {} # predict true, actual false\n",
    "false_negative = {} # predict false, actual true\n",
    "true_negative = {}\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    pred_hasCoref = set(pred_dict_hasCoref[section_name])\n",
    "    gt_hasCoref = set(gt_dict_hasCoref[section_name])\n",
    "    gt_all = set(gt_dict_allDoc[section_name])\n",
    "\n",
    "    true_positive[section_name] = pred_hasCoref.intersection(gt_hasCoref)\n",
    "    false_positive[section_name] = gt_all.intersection(pred_hasCoref) - gt_hasCoref\n",
    "    false_negative[section_name] = gt_hasCoref - pred_hasCoref\n",
    "    true_negative[section_name] = gt_all - pred_hasCoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    print(section_name)\n",
    "    print(\"true_positive\",len(true_positive[section_name]))\n",
    "    print(\"false_positive\",len(false_positive[section_name]))\n",
    "    print(\"false_negative\",len(false_negative[section_name]))\n",
    "    print(\"true_negative\",len(true_negative[section_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'micro':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "'weighted':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "'binary':\n",
    "Only report results for the class specified by pos_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "y_true = [\"has_coref\" if doc_id in gt_dict_hasCoref[section_name] else \"no_coref\" for section_name, doc_list in gt_dict_allDoc.items() for doc_id in doc_list]\n",
    "y_pred = [\"has_coref\" if doc_id in pred_dict_hasCoref[section_name] else \"no_coref\" for section_name, doc_list in gt_dict_allDoc.items() for doc_id in doc_list]\n",
    "confusion_arr = confusion_matrix(y_true, y_pred, labels=[\"has_coref\",\"no_coref\"])\n",
    "# TP FN\n",
    "# FP TN\n",
    "print(confusion_arr)\n",
    "print()\n",
    "\n",
    "micro_precision_recall_f1 = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "macro_precision_recall_f1 = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "weigthed_precision_recall_f1 = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "binary_precision_recall_f1 = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[\"has_coref\",\"no_coref\"])\n",
    "precision,recall,f1,support = binary_precision_recall_f1\n",
    "\n",
    "print(\"precision, recall, f1, support(class_ele_num):\")\n",
    "print(\"micro:\", micro_precision_recall_f1)\n",
    "print(\"macro:\", macro_precision_recall_f1)\n",
    "print(\"weigthed macro:\", weigthed_precision_recall_f1)\n",
    "for i,j in zip([\"has_coref\",\"no_coref\"],np.matrix([precision,recall,f1,support]).getT()):\n",
    "    print(\"binary\",i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure() #this creates a new figure on which your plot will appear\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = sns.heatmap(confusion_arr, xticklabels=[\"has_coref\", \"no_coref\"], yticklabels=[\"has_coref\", \"no_coref\"], annot=True, fmt='.2f')\n",
    "\n",
    "ax.set_xlabel('predict')\n",
    "ax.set_ylabel('ground-truth')\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoNLL F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only TP (42 doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_conll_file_gt = os.path.join(output_dir, \"gt_42_noWhich.conll\")\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_42.conll\")\n",
    "check_and_remove_file(output_conll_file_gt)\n",
    "check_and_remove_file(output_conll_file_pred)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    for doc_id in true_positive[section_name]:\n",
    "        # gt\n",
    "        input_conll_file = os.path.join(\"../../output/mimic_cxr/coref/individual_conll_ground_truth\", section_name, f\"{doc_id}.conll\")\n",
    "        copy_and_paste_conll(input_conll_file, output_conll_file_gt)\n",
    "        # pred\n",
    "        input_conll_file = os.path.join(\"../../output/mimic_cxr/coref/individual_conll\", section_name, f\"{doc_id}.conll\")\n",
    "        copy_and_paste_conll(input_conll_file, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP + FP (42+2), which is the ensemble application scenario\n",
    "\n",
    "gt_44 (tp) + pred_44 (tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_conll_file_gt = os.path.join(output_dir, \"gt_44_noWhich.conll\")\n",
    "check_and_remove_file(output_conll_file_gt)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = list(true_positive[section_name]) # false_positive files are not existing here, thus ignore\n",
    "    for doc_id in doc_list:\n",
    "        input_conll_file = os.path.join(\"../../output/mimic_cxr/coref/individual_conll_ground_truth\", section_name, f\"{doc_id}.conll\")\n",
    "        copy_and_paste_conll(input_conll_file, output_conll_file_gt)\n",
    "\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_44.conll\")\n",
    "check_and_remove_file(output_conll_file_pred)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = list(true_positive[section_name]) + list(false_positive[section_name])\n",
    "    for doc_id in doc_list:\n",
    "        input_conll_file = os.path.join(\"../../output/mimic_cxr/coref/individual_conll\", section_name, f\"{doc_id}.conll\")\n",
    "        copy_and_paste_conll(input_conll_file, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 100x: all (200), which is the actural conll scoring scenario.\n",
    "\n",
    "1. Step 1: get fast_coref_xxx model outputs (csv files in /nlp_ensemble/temp_for_eval), then\n",
    "2. Step 2: get majority_voting pipeline outputs (csv files in /coref_voting/temp_for_eval), then\n",
    "3. Step 3: accordingly generate conll file and do evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Need to generate /nlp_ensemble/fast_coref_xxx csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config %s metrics:\n",
      "- MUC\n",
      "- Bcub\n",
      "- CEAFE\n",
      "keep_singletons: true\n",
      "seed: 45\n",
      "train: true\n",
      "eval_all: false\n",
      "use_wandb: true\n",
      "paths:\n",
      "  resource_dir: ${infra.work_dir}/../../coref_resources\n",
      "  base_data_dir: ${paths.resource_dir}/data\n",
      "  conll_scorer: ${paths.resource_dir}/reference-coreference-scorers/scorer.pl\n",
      "  base_model_dir: ${infra.work_dir}/../models\n",
      "  model_dir: .//../models/coref_joint_11c4b92806f1db7437083841caffe8da\n",
      "  best_model_dir: .//../models/coref_joint_11c4b92806f1db7437083841caffe8da/best\n",
      "  model_filename: model.pth\n",
      "  model_name: null\n",
      "  model_name_prefix: coref_\n",
      "  model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_joint_11c4b92806f1db7437083841caffe8da/model.pth\n",
      "  best_model_path: /share/data/speech/shtoshni/research/fast-coref/models/coref_joint_11c4b92806f1db7437083841caffe8da/best/model.pth\n",
      "  doc_encoder_dirname: doc_encoder\n",
      "datasets:\n",
      "  litbank:\n",
      "    name: LitBank\n",
      "    cluster_threshold: 1\n",
      "    canonical_cluster_threshold: 1\n",
      "    cross_val_split: 0\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 80\n",
      "    num_dev_docs: 10\n",
      "    num_test_docs: 10\n",
      "    has_conll: true\n",
      "  ontonotes:\n",
      "    name: OntoNotes\n",
      "    cluster_threshold: 2\n",
      "    canonical_cluster_threshold: 2\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 1000\n",
      "    num_dev_docs: 343\n",
      "    num_test_docs: 348\n",
      "    has_conll: true\n",
      "    singleton_file: ontonotes/ment_singletons_longformer_speaker/30.jsonlines\n",
      "  preco:\n",
      "    name: PreCo\n",
      "    cluster_threshold: 1\n",
      "    canonical_cluster_threshold: 1\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 1000\n",
      "    num_dev_docs: 500\n",
      "    num_test_docs: 500\n",
      "model:\n",
      "  doc_encoder:\n",
      "    transformer:\n",
      "      name: longformer\n",
      "      model_size: large\n",
      "      model_str: /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint\n",
      "      max_encoder_segment_len: 4096\n",
      "      max_segment_len: 4096\n",
      "    chunking: independent\n",
      "    finetune: true\n",
      "    add_speaker_tokens: true\n",
      "    speaker_start: '[SPEAKER_START]'\n",
      "    speaker_end: '[SPEAKER_END]'\n",
      "  memory:\n",
      "    mem_type:\n",
      "      name: unbounded\n",
      "      max_ents: null\n",
      "      eval_max_ents: null\n",
      "    emb_size: 20\n",
      "    mlp_size: 3000\n",
      "    mlp_depth: 1\n",
      "    sim_func: hadamard\n",
      "    entity_rep: wt_avg\n",
      "    num_feats: 2\n",
      "  mention_params:\n",
      "    max_span_width: 20\n",
      "    ment_emb: attn\n",
      "    use_gold_ments: false\n",
      "    use_topk: false\n",
      "    top_span_ratio: 0.4\n",
      "    emb_size: 20\n",
      "    mlp_size: 3000\n",
      "    mlp_depth: 1\n",
      "    ment_emb_to_size_factor:\n",
      "      attn: 3\n",
      "      endpoint: 2\n",
      "      max: 1\n",
      "  metadata_params:\n",
      "    use_genre_feature: false\n",
      "    default_genre: nw\n",
      "    genres:\n",
      "    - bc\n",
      "    - bn\n",
      "    - mz\n",
      "    - nw\n",
      "    - pt\n",
      "    - tc\n",
      "    - wb\n",
      "optimizer:\n",
      "  init_lr: 0.0003\n",
      "  fine_tune_lr: 1.0e-05\n",
      "  max_gradient_norm: 1.0\n",
      "  lr_decay: linear\n",
      "trainer:\n",
      "  dropout_rate: 0.3\n",
      "  label_smoothing_wt: 0.1\n",
      "  ment_loss: all\n",
      "  normalize_loss: false\n",
      "  max_evals: 20\n",
      "  to_save_model: true\n",
      "  log_frequency: 500\n",
      "  patience: 10\n",
      "  eval_per_k_steps: 5000\n",
      "  num_training_steps: 100000\n",
      "infra:\n",
      "  is_local: false\n",
      "  job_time: 14280\n",
      "  job_id: 72531651\n",
      "  work_dir: ./\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n",
      "100%|██████████| 100/100 [01:58<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
    "from inference.tokenize_doc import tokenize_and_segment_doc\n",
    "from nlp_ensemble.nlp_menbers.play_fastcoref import inference, resolve_output\n",
    "from common_utils.nlp_utils import align_byIndex_individually_nestedgruop, align_coref_groups_in_conll_format\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"nlp_ensemble\"):\n",
    "        config = compose(config_name=\"nlp_ensemble\", overrides=[\"+nlp_ensemble@_global_=mimic_cxr\"])\n",
    "\n",
    "# Init model\n",
    "### Modify ###\n",
    "model_dir = \"../../../../git_clone_repos/fast-coref/models/joint_best\"\n",
    "config.fastcoref_joint.output_dir = \"../../output/mimic_cxr/nlp_ensemble/temp_for_eval/fast_coref_best\"\n",
    "\n",
    "config.fastcoref_joint.model_dir = os.path.join(model_dir,\"best\") if os.path.exists(os.path.join(model_dir,\"best\")) else model_dir\n",
    "model, subword_tokenizer, max_segment_len = play_fastcoref.init_coref_model(config)\n",
    "\n",
    "\n",
    "spacy_nametyle = config.name_style.spacy.column_name\n",
    "fastcoref_joint_nametyle = config.name_style.fastcoref_joint.column_name\n",
    "\n",
    "# output_conll_file_pred = os.path.join(output_dir, \"pred_joint_best.conll\")\n",
    "# check_and_remove_file(output_conll_file_pred)\n",
    "\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in tqdm(doc_list):\n",
    "        spacy_csv_file = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\", section_name, f\"{doc_id}.csv\")\n",
    "        # Load preprocessed tokens from csv files.\n",
    "        df_base = pd.read_csv(spacy_csv_file, index_col=0)\n",
    "        sent_tok_2d_list, tok_indices_in_spacy = format_input_tok_same_as_traingset(df_base, spacy_nametyle)\n",
    "\n",
    "        # Using longformer tokenizer to generate subtokens and form the input data.\n",
    "        tokenized_doc = tokenize_and_segment_doc(sent_tok_2d_list, subword_tokenizer, max_segment_len=max_segment_len)\n",
    "\n",
    "        # Get model output\n",
    "        pred_mentions, mention_scores, gt_actions, pred_actions = inference(model, tokenized_doc)\n",
    "\n",
    "        # Resolve model output\n",
    "        coref_group_list = resolve_output(tokenized_doc, pred_mentions, pred_actions, ignore_singleton = True)\n",
    "        \n",
    "        # To dataframe\n",
    "        spacy_tok_list = df_base.loc[:, spacy_nametyle.token].to_list()\n",
    "        spacy_sentGroup_list = df_base.loc[:, spacy_nametyle.sentence_group].to_list()\n",
    "        input_tok_list = [tok for sent in sent_tok_2d_list for tok in sent]\n",
    "        \n",
    "        coref_group_aligned_to_input_tok = align_byIndex_individually_nestedgruop(len(input_tok_list), coref_group_list)\n",
    "        coref_group_aligned_to_spacy_tok = align_to_spacy(tok_indices_in_spacy, coref_group_aligned_to_input_tok, input_tok_list, spacy_tok_list)\n",
    "\n",
    "        coref_group_conll_aligned_to_input_tok = align_coref_groups_in_conll_format(len(input_tok_list), coref_group_list)\n",
    "        coref_group_conll_aligned_to_spacy_tok = align_to_spacy(tok_indices_in_spacy, coref_group_conll_aligned_to_input_tok, input_tok_list, spacy_tok_list)\n",
    "        \n",
    "        df_fastcoref_joint = pd.DataFrame(\n",
    "            {\n",
    "                fastcoref_joint_nametyle[\"token_from_spacy\"]: [str(i) for i in spacy_tok_list],\n",
    "                fastcoref_joint_nametyle[\"sentence_group\"]: [int(i) for i in spacy_sentGroup_list],\n",
    "                fastcoref_joint_nametyle[\"coref_group\"]: [str(i) for i in coref_group_aligned_to_spacy_tok],\n",
    "                fastcoref_joint_nametyle[\"coref_group_conll\"]: [str(i) for i in coref_group_conll_aligned_to_spacy_tok],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Overwrite csv\n",
    "        csv_output_dir = os.path.join(config.fastcoref_joint.output_dir, section_name)\n",
    "        check_and_create_dirs(csv_output_dir)\n",
    "        df_fastcoref_joint.to_csv(os.path.join(csv_output_dir, doc_id+\".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Need to generate majority_voting gt (csv) files when only have fast_coref_xxx outputs (csv)\n",
    "\n",
    "modify /config/coreference_resolution/coref_voting/mimic_cxr.yaml -> ${input.in_use}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.18it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from common_utils.coref_utils import shuffle_list\n",
    "import coref_voting\n",
    "from coref_voting import DocClass, MentionClass, compute_voting_result, get_output_df\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
    "\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"majority_voting\"):\n",
    "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+coreference_resolution/coref_voting@_global_=mimic_cxr\"])\n",
    "\n",
    "### Modify. Also check /config/coreference_resolution/coref_voting/mimic_cxr.yaml -> ${input.in_use} ###\n",
    "config.input.source.in_use = [\"ml\",\"rb\",\"fj\"] # ml, rb, fj, gt, fj_x, fj_x2\n",
    "config.input.source.coref_models.fj_x.dir = \"../../output/mimic_cxr/nlp_ensemble/temp_for_eval/fast_model_102\"\n",
    "config.input.source.coref_models.fj_x2.dir = \"../../output/mimic_cxr/nlp_ensemble/temp_for_eval/fast_coref_best\"\n",
    "config.input.source.coref_models.gt.dir = \"../../output/mimic_cxr/manual_test_set/round1x2\"\n",
    "mv_output_base_dir = os.path.join(\"../../output/mimic_cxr/coref_voting/temp_for_eval/ml_rb_fj\")\n",
    "\n",
    "def do_majority_voting(config, spacy_file_path, section_name, file_name):\n",
    "    \"\"\" Voting on one document \"\"\"\n",
    "\n",
    "    START_EVENT.wait()\n",
    "\n",
    "    # Read spacy output as alignment base\n",
    "    df_spacy = pd.read_csv(spacy_file_path, index_col=0, na_filter=False)\n",
    "    # Some of the i2b2 raw files are utf-8 start with DOM, but we didn't remove the DOM character, thus we fix it here.\n",
    "    df_spacy.iloc[0] = df_spacy.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
    "\n",
    "    docObj: DocClass = coref_voting.resolve_voting_info(config, df_spacy, section_name, file_name)\n",
    "    valid_mention_group: list[set[MentionClass]] = compute_voting_result(config, docObj)\n",
    "    df_out = get_output_df(config, df_spacy, valid_mention_group, docObj)\n",
    "\n",
    "    mv_output_dir = os.path.join(mv_output_base_dir, section_name)\n",
    "    check_and_create_dirs(mv_output_dir)\n",
    "    output_file_path = os.path.join(mv_output_dir, file_name)\n",
    "\n",
    "    df_out.to_csv(output_file_path)\n",
    "\n",
    "    return f\"{file_name} done.\"\n",
    "\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    with ProcessPoolExecutor(max_workers=config.thread.workers) as executor:\n",
    "        all_task = []\n",
    "        doc_list = gt_dict_allDoc[section_name]\n",
    "        for doc_id in doc_list:\n",
    "            file_name = doc_id + \".csv\"\n",
    "            spacy_out_dir = os.path.join(config.input.source.baseline_model.dir, section_name)\n",
    "            spacy_file_path = os.path.join(spacy_out_dir, file_name)\n",
    "            all_task.append(executor.submit(do_majority_voting, config, spacy_file_path, section_name, file_name))\n",
    "        \n",
    "         # Notify tasks to start\n",
    "        START_EVENT.set()\n",
    "\n",
    "        if all_task:\n",
    "            for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
    "                msg = future.result()\n",
    "\n",
    "        executor.shutdown(wait=True, cancel_futures=False)\n",
    "        START_EVENT.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Have majority_voting gt files already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: ../../resources/eval/gt_200_noWhich.conll\n",
      "pred: ../../resources/eval/pred_mv.conll\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 51.09, 88.11, 64.68\n",
      "coref_recall, coref_precision, coref_f1: 46.1, 81.31, 58.84\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 51.09, 88.11, 64.68\n",
      "coref_recall, coref_precision, coref_f1: 46.51, 84.1, 59.9\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 51.09, 88.11, 64.68\n",
      "coref_recall, coref_precision, coref_f1: 50.09, 84.21, 62.82\n",
      "Overall F1: 60.52\n"
     ]
    }
   ],
   "source": [
    "### Modify ###\n",
    "# mv_output_base_dir = os.path.join(\"../../output/mimic_cxr/coref_voting/temp_for_eval/ml_gt_fjbest\")\n",
    "\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_mv.conll\")\n",
    "check_and_remove_file(output_conll_file_pred)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in doc_list:\n",
    "        input_csv_file = os.path.join(mv_output_base_dir, section_name, f\"{doc_id}.csv\")\n",
    "        from_csv_to_conll(section_name, doc_id, output_conll_file_pred, input_csv_file, \"[mv]coref_group_conll\", \"[sp]sentence_group\",\"[sp]token\")\n",
    "\n",
    "output_conll_file_gt = os.path.join(output_dir, \"gt_200_noWhich.conll\")\n",
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1-2: scoref, dcoref, fast_coref_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"statistic\"):\n",
    "        config = compose(config_name=\"nlp_ensemble\", overrides=[\"+statistic/coref_scoring@_global_=mimic_cxr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common way for testing scoref, dcoref, fast_coref_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.50it/s]\n",
      "100%|██████████| 100/100 [00:16<00:00,  5.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from statistic.coref_socring import align_spacy_to_ground_truth, align_to_spacy, convert_non_spacy_token_csv_to_conll_format, convert_spacy_token_csv_to_conll_format, find_cloest_index\n",
    "\n",
    "### Modify this ###\n",
    "csv_file_dir = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input)\"\n",
    "model_cfg = config.input.source.models.get(\"fj\") # ml -> scoref, rb -> dcoref, fj -> fast_coref_joint\n",
    "model_cfg.dir = csv_file_dir\n",
    "model_cfg.name = \"fcoref\"\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_xxx.conll\")\n",
    "\n",
    "check_and_remove_file(output_conll_file_pred)\n",
    "gt_cfg = config.input.ground_truth\n",
    "scorer_cfg = config.scorer\n",
    "spacy_cfg = config.input.spacy\n",
    "\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in tqdm(doc_list):\n",
    "        pred_file_path = os.path.join(csv_file_dir, section_name, f\"{doc_id}.csv\")\n",
    "        df_pred = pd.read_csv(pred_file_path, index_col=0, na_filter=False)\n",
    "        gt_file_path = os.path.join(source_input_csv_dir, section_name, f\"{doc_id}.csv\")\n",
    "        df_gt = pd.read_csv(gt_file_path, index_col=0, na_filter=False)\n",
    "        spacy_file_path = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\", section_name, f\"{doc_id}.csv\")\n",
    "        df_spacy = pd.read_csv(spacy_file_path, index_col=0, na_filter=False)\n",
    "\n",
    "        # Some of the i2b2 raw files are utf-8 start with DOM, but we didn't remove the DOM character, thus we fix it here.\n",
    "        df_gt.iloc[0] = df_gt.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
    "        df_pred.iloc[0] = df_pred.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
    "        df_spacy.iloc[0] = df_spacy.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
    "\n",
    "        df_pred = remove_singleton(df_pred, model_cfg.target_column.coref_group_conll)\n",
    "\n",
    "        # Generate conll format predicted files\n",
    "        if model_cfg.align_to_spacy:\n",
    "\n",
    "            # Algin to spacy first, then align spacy to gt\n",
    "            model2spacy_tok_indices, coref_index_appearance_count_dict = align_to_spacy(config, model_cfg, df_spacy, df_pred)\n",
    "\n",
    "            gt_token_list = df_gt.loc[:, gt_cfg.target_column.token_for_alignment].tolist()\n",
    "            spacy_token_list = df_spacy.loc[:, spacy_cfg.target_column.token].tolist()\n",
    "            spacy2gt_tok_indices, _ = align_spacy_to_ground_truth(gt_token_list, spacy_token_list)\n",
    "            sentence_list_pred = convert_non_spacy_token_csv_to_conll_format(config, model_cfg, section_name, doc_id, coref_index_appearance_count_dict, model2spacy_tok_indices,\n",
    "                                                                             spacy2gt_tok_indices, df_pred, df_spacy)\n",
    "        else:\n",
    "            # Directly align to ground-truth\n",
    "            gt_token_list = df_gt.loc[:, gt_cfg.target_column.token_for_alignment].tolist()\n",
    "            spacy_token_list = df_pred.loc[:, model_cfg.target_column.token].tolist()\n",
    "            # Some token has conll label but does not exist in gt, that is what `empty_token_idx_with_conll_label_dict` is used for.\n",
    "            spacy2gt_tok_indices, empty_token_idx_with_conll_label_dict = align_spacy_to_ground_truth(\n",
    "                gt_token_list, spacy_token_list, df_spacy=df_spacy, spacy_cfg=spacy_cfg, df_pred=df_pred, model_cfg=model_cfg)\n",
    "            target_token_index_and_conll_label_dict = find_cloest_index(spacy2gt_tok_indices, empty_token_idx_with_conll_label_dict)\n",
    "            sentence_list_pred = convert_spacy_token_csv_to_conll_format(config, model_cfg, section_name, doc_id, spacy2gt_tok_indices, df_pred, target_token_index_and_conll_label_dict)\n",
    "\n",
    "\n",
    "        # Write conll\n",
    "        from_list_to_conll(output_conll_file_pred, doc_id, section_name, sentence_list_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: ../../resources/eval/gt_200_noWhich.conll\n",
      "pred: ../../resources/eval/pred_xxx.conll\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 57.32, 73.6, 64.44\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 58.47, 75.68, 65.97\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 61.43, 78.66, 68.99\n",
      "Overall F1: 66.46666666666665\n"
     ]
    }
   ],
   "source": [
    "### Modify this ###\n",
    "output_conll_file_gt = os.path.join(output_dir, \"gt_200_noWhich.conll\")\n",
    "\n",
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way for testing fast_coref_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: ../../resources/eval/gt_200_noWhich.conll\n",
      "pred: ../../resources/eval/pred_fjbest.conll\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 57.32, 73.6, 64.44\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 58.47, 75.68, 65.97\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 62.68, 80.38, 70.44\n",
      "coref_recall, coref_precision, coref_f1: 61.43, 78.66, 68.99\n",
      "Overall F1: 66.46666666666665\n"
     ]
    }
   ],
   "source": [
    "### Modify this ###\n",
    "_input_csv_source_dir = \"../../output/mimic_cxr/nlp_ensemble/temp_for_eval/fast_coref_best\"\n",
    "_output_file_name = \"pred_fjbest.conll\"\n",
    "\n",
    "gt_cfg = config.input.ground_truth\n",
    "scorer_cfg = config.scorer\n",
    "spacy_cfg = config.input.spacy\n",
    "\n",
    "### Modify this with expected output name ###\n",
    "output_conll_file_pred = os.path.join(output_dir, _output_file_name)\n",
    "check_and_remove_file(output_conll_file_pred)\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in doc_list:\n",
    "        ### Modify this with correct csv file path ###\n",
    "        input_csv_file = os.path.join(_input_csv_source_dir, section_name, f\"{doc_id}.csv\")\n",
    "        df_pred = pd.read_csv(input_csv_file, index_col=0, na_filter=False)\n",
    "        df_pred = remove_singleton(df_pred, \"[fj]coref_group_conll\")\n",
    "        from_csv_to_conll(section_name, doc_id, output_conll_file_pred, df_pred, \"[fj]coref_group_conll\", \"[fj]sentence_group\",\"[fj]token_from_spacy\")\n",
    "\n",
    "### Modify this with correct ground_truth conll file ###\n",
    "output_conll_file_gt = os.path.join(output_dir, \"gt_200_noWhich.conll\")\n",
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"nlp_ensemble\"):\n",
    "        config = compose(config_name=\"nlp_ensemble\", overrides=[\"+nlp_ensemble@_global_=mimic_cxr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config %s metrics:\n",
      "- MUC\n",
      "- Bcub\n",
      "- CEAFE\n",
      "keep_singletons: false\n",
      "seed: 45\n",
      "train: true\n",
      "use_wandb: false\n",
      "override_encoder: true\n",
      "override_memory: false\n",
      "copy_from_pretrained_model: false\n",
      "continue_training: false\n",
      "paths:\n",
      "  resource_dir: ${infra.project_dir}/coref_resources\n",
      "  base_data_dir: ${paths.resource_dir}/data\n",
      "  conll_scorer: ${paths.resource_dir}/reference-coreference-scorers/scorer.pl\n",
      "  base_model_dir: ${infra.project_dir}/models\n",
      "  model_dir: /scratch/c.c21051562/workspace/fast-coref/models/coref_joint_train_onto_i2b2_mimic_206\n",
      "  best_model_dir: /scratch/c.c21051562/workspace/fast-coref/models/coref_joint_train_onto_i2b2_mimic_206/best\n",
      "  model_filename: model.pth\n",
      "  model_name: joint_train_onto_i2b2_mimic_206\n",
      "  model_name_prefix: coref_\n",
      "  model_path: /scratch/c.c21051562/workspace/fast-coref/models/coref_joint_train_onto_i2b2_mimic_206/model.pth\n",
      "  best_model_path: /scratch/c.c21051562/workspace/fast-coref/models/coref_joint_train_onto_i2b2_mimic_206/best/model.pth\n",
      "  doc_encoder_dirname: doc_encoder\n",
      "  pretrain_model_dir: ${paths.base_model_dir}/joint_best\n",
      "datasets:\n",
      "  ontonotes:\n",
      "    name: OntoNotes\n",
      "    cluster_threshold: 2\n",
      "    canonical_cluster_threshold: 2\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 1000\n",
      "    num_dev_docs: 343\n",
      "    num_test_docs: 348\n",
      "    has_conll: true\n",
      "    singleton_file: null\n",
      "  i2b2:\n",
      "    name: i2b2\n",
      "    cluster_threshold: 2\n",
      "    canonical_cluster_threshold: 2\n",
      "    cross_val_split: 0\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 296\n",
      "    num_dev_docs: 84\n",
      "    num_test_docs: 44\n",
      "    has_conll: true\n",
      "  mimic_cxr_1k:\n",
      "    name: mimic_cxr_1k\n",
      "    cluster_threshold: 2\n",
      "    canonical_cluster_threshold: 2\n",
      "    targeted_eval: false\n",
      "    num_train_docs: 997\n",
      "    num_dev_docs: 403\n",
      "    num_test_docs: 44\n",
      "    has_conll: true\n",
      "    singleton_file: null\n",
      "model:\n",
      "  doc_encoder:\n",
      "    transformer:\n",
      "      name: longformer\n",
      "      model_size: large\n",
      "      model_str: /home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/models/longformer_coreference_joint\n",
      "      max_encoder_segment_len: 4096\n",
      "      max_segment_len: 4096\n",
      "    chunking: independent\n",
      "    finetune: false\n",
      "    add_speaker_tokens: true\n",
      "    speaker_start: '[SPEAKER_START]'\n",
      "    speaker_end: '[SPEAKER_END]'\n",
      "  memory:\n",
      "    mem_type:\n",
      "      name: unbounded\n",
      "      max_ents: null\n",
      "      eval_max_ents: null\n",
      "    emb_size: 20\n",
      "    mlp_size: 3000\n",
      "    mlp_depth: 1\n",
      "    sim_func: hadamard\n",
      "    entity_rep: wt_avg\n",
      "    num_feats: 2\n",
      "  mention_params:\n",
      "    max_span_width: 20\n",
      "    ment_emb: attn\n",
      "    use_gold_ments: false\n",
      "    use_topk: false\n",
      "    top_span_ratio: 0.4\n",
      "    emb_size: 20\n",
      "    mlp_size: 3000\n",
      "    mlp_depth: 1\n",
      "    ment_emb_to_size_factor:\n",
      "      attn: 3\n",
      "      endpoint: 2\n",
      "      max: 1\n",
      "  metadata_params:\n",
      "    use_genre_feature: false\n",
      "    default_genre: nw\n",
      "    genres:\n",
      "    - bc\n",
      "    - bn\n",
      "    - mz\n",
      "    - nw\n",
      "    - pt\n",
      "    - tc\n",
      "    - wb\n",
      "optimizer:\n",
      "  init_lr: 0.0003\n",
      "  fine_tune_lr: 1.0e-05\n",
      "  max_gradient_norm: 1.0\n",
      "  lr_decay: linear\n",
      "trainer:\n",
      "  dropout_rate: 0.3\n",
      "  label_smoothing_wt: 0.1\n",
      "  ment_loss: all\n",
      "  normalize_loss: false\n",
      "  max_evals: 20\n",
      "  to_save_model: true\n",
      "  log_frequency: 1000\n",
      "  patience: 10\n",
      "  eval_per_k_steps: 5000\n",
      "  num_training_steps: 100000\n",
      "  max_training_segments: 1\n",
      "infra:\n",
      "  is_local: false\n",
      "  job_time: 72000\n",
      "  job_id: 206\n",
      "  project_dir: /scratch/c.c21051562/workspace/fast-coref\n",
      "  work_dir: ${project_dir}/src/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "\n",
    "### Modify ###\n",
    "model_dir = \"../../../../git_clone_repos/fast-coref/models/coref_joint_train_onto_i2b2_301/\"\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_model206.conll\")\n",
    "\n",
    "config.fastcoref_joint.model_dir = os.path.join(model_dir,\"best\") if os.path.exists(os.path.join(model_dir,\"best\")) else model_dir\n",
    "model, subword_tokenizer, max_segment_len = play_fastcoref.init_coref_model(config)\n",
    "check_and_remove_file(output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variable: model_dir and output_conll_file_pred with correct path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  ../../../../git_clone_repos/fast-coref/models/coref_joint_train_onto_i2b2_mimic_206/best\n"
     ]
    }
   ],
   "source": [
    "print(\"Using: \",config.fastcoref_joint.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is unstable when the input are slightly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the real-world input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inference.tokenize_doc import tokenize_and_segment_doc\n",
    "# from model.utils import action_sequences_to_clusters\n",
    "# from coref_utils.utils import get_mention_to_cluster,filter_clusters\n",
    "# from nlp_ensemble.nlp_menbers.play_fastcoref import inference, resolve_output\n",
    "# from common_utils.nlp_utils import align_byIndex_individually_nestedgruop, align_coref_groups_in_conll_format\n",
    "\n",
    "# spacy_nametyle = config.name_style.spacy.column_name\n",
    "# fastcoref_joint_nametyle = config.name_style.fastcoref_joint.column_name\n",
    "\n",
    "# section_name = \"impression\"\n",
    "# doc_id = \"s51246808\"\n",
    "\n",
    "# spacy_csv_file = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\", section_name, f\"{doc_id}.csv\")\n",
    "# # Load preprocessed tokens from csv files.\n",
    "# df_base = pd.read_csv(spacy_csv_file, index_col=0)\n",
    "# sent_tok_2d_list: list[list[str]] = format_input_tok_as_realworld(df_base, spacy_nametyle)\n",
    "\n",
    "# # Using longformer tokenizer to generate subtokens and form the input data.\n",
    "# tokenized_doc = tokenize_and_segment_doc(sent_tok_2d_list, subword_tokenizer, max_segment_len=max_segment_len)\n",
    "# print(sent_tok_2d_list)\n",
    "\n",
    "# # Get model output\n",
    "# pred_mentions, mention_scores, gt_actions, pred_actions = inference(model, tokenized_doc)\n",
    "# # Process predicted clusters\n",
    "# raw_predicted_clusters = action_sequences_to_clusters(\n",
    "#     pred_actions, pred_mentions\n",
    "# )\n",
    "# predicted_clusters = filter_clusters(\n",
    "#     raw_predicted_clusters, threshold=2\n",
    "# )\n",
    "# mention_to_predicted = get_mention_to_cluster(predicted_clusters)\n",
    "\n",
    "# for i in predicted_clusters:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equals to the traing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_nametyle = config.name_style.spacy.column_name\n",
    "# fastcoref_joint_nametyle = config.name_style.fastcoref_joint.column_name\n",
    "\n",
    "# section_name = \"impression\"\n",
    "# doc_id = \"s51246808\"\n",
    "\n",
    "# spacy_csv_file = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\", section_name, f\"{doc_id}.csv\")\n",
    "\n",
    "# df_base = pd.read_csv(spacy_csv_file, index_col=0)\n",
    "# sent_tok_2d_list: list[list[str]] = format_input_tok_same_as_traingset(df_base, spacy_nametyle)\n",
    "    \n",
    "# print(sent_tok_2d_list)\n",
    "\n",
    "# # Using longformer tokenizer to generate subtokens and form the input data.\n",
    "# tokenized_doc = tokenize_and_segment_doc(sent_tok_2d_list, subword_tokenizer, max_segment_len=max_segment_len)\n",
    "\n",
    "# # Get model output\n",
    "# pred_mentions, mention_scores, gt_actions, pred_actions = inference(model, tokenized_doc)\n",
    "# # Process predicted clusters\n",
    "# raw_predicted_clusters = action_sequences_to_clusters(\n",
    "#     pred_actions, pred_mentions\n",
    "# )\n",
    "# predicted_clusters = filter_clusters(\n",
    "#     raw_predicted_clusters, threshold=2\n",
    "# )\n",
    "# mention_to_predicted = get_mention_to_cluster(predicted_clusters)\n",
    "# for i in predicted_clusters:\n",
    "#     print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:53<00:00,  1.14s/it]\n",
      "100%|██████████| 100/100 [01:53<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from inference.tokenize_doc import tokenize_and_segment_doc\n",
    "from nlp_ensemble.nlp_menbers.play_fastcoref import inference, resolve_output\n",
    "from common_utils.nlp_utils import align_byIndex_individually_nestedgruop, align_coref_groups_in_conll_format\n",
    "\n",
    "spacy_nametyle = config.name_style.spacy.column_name\n",
    "fastcoref_joint_nametyle = config.name_style.fastcoref_joint.column_name\n",
    "\n",
    "# output_conll_file_pred = os.path.join(output_dir, \"pred_joint_best.conll\")\n",
    "# check_and_remove_file(output_conll_file_pred)\n",
    "\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    doc_list = gt_dict_allDoc[section_name]\n",
    "    for doc_id in tqdm(doc_list):\n",
    "        \n",
    "        spacy_csv_file = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\", section_name, f\"{doc_id}.csv\")\n",
    "        # Load preprocessed tokens from csv files.\n",
    "        df_base = pd.read_csv(spacy_csv_file, index_col=0)\n",
    "        sent_tok_2d_list, tok_indices_in_spacy = format_input_tok_same_as_traingset(df_base, spacy_nametyle)\n",
    "\n",
    "        # Using longformer tokenizer to generate subtokens and form the input data.\n",
    "        tokenized_doc = tokenize_and_segment_doc(sent_tok_2d_list, subword_tokenizer, max_segment_len=max_segment_len)\n",
    "\n",
    "        # Get model output\n",
    "        pred_mentions, mention_scores, gt_actions, pred_actions = inference(model, tokenized_doc)\n",
    "\n",
    "        # Resolve model output\n",
    "        coref_group_list = resolve_output(tokenized_doc, pred_mentions, pred_actions, ignore_singleton = True)\n",
    "        \n",
    "        # To dataframe\n",
    "        spacy_tok_list = df_base.loc[:, spacy_nametyle.token].to_list()\n",
    "        spacy_sentGroup_list = df_base.loc[:, spacy_nametyle.sentence_group].to_list()\n",
    "        input_tok_list = [tok for sent in sent_tok_2d_list for tok in sent]\n",
    "        \n",
    "        coref_group_aligned_to_input_tok = align_byIndex_individually_nestedgruop(len(input_tok_list), coref_group_list)\n",
    "        coref_group_aligned_to_spacy_tok = align_to_spacy(tok_indices_in_spacy, coref_group_aligned_to_input_tok, input_tok_list, spacy_tok_list)\n",
    "\n",
    "        coref_group_conll_aligned_to_input_tok = align_coref_groups_in_conll_format(len(input_tok_list), coref_group_list)\n",
    "        coref_group_conll_aligned_to_spacy_tok = align_to_spacy(tok_indices_in_spacy, coref_group_conll_aligned_to_input_tok, input_tok_list, spacy_tok_list)\n",
    "        \n",
    "        df_fastcoref_joint = pd.DataFrame(\n",
    "            {\n",
    "                fastcoref_joint_nametyle[\"token_from_spacy\"]: [str(i) for i in spacy_tok_list],\n",
    "                fastcoref_joint_nametyle[\"sentence_group\"]: [int(i) for i in spacy_sentGroup_list],\n",
    "                fastcoref_joint_nametyle[\"coref_group\"]: [str(i) for i in coref_group_aligned_to_spacy_tok],\n",
    "                fastcoref_joint_nametyle[\"coref_group_conll\"]: [str(i) for i in coref_group_conll_aligned_to_spacy_tok],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Overwrite csv\n",
    "        from_csv_to_conll(section_name, doc_id, output_conll_file_pred, df_fastcoref_joint, \"[fj]coref_group_conll\", \"[fj]sentence_group\",\"[fj]token_from_spacy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval:\n",
    "\n",
    "If gt and pred conll files are existing, we can specify the path and run the following script directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: ../../resources/eval/gt_200_noWhich.conll\n",
      "pred: ../../resources/eval/pred_model206.conll\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 61.68, 80.3, 69.77\n",
      "coref_recall, coref_precision, coref_f1: 57.0, 73.49, 64.21\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 61.68, 80.3, 69.77\n",
      "coref_recall, coref_precision, coref_f1: 57.65, 75.9, 65.53\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 61.68, 80.3, 69.77\n",
      "coref_recall, coref_precision, coref_f1: 59.84, 78.83, 68.03\n",
      "Overall F1: 65.92333333333333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "output_conll_file_gt = os.path.join(output_dir, \"gt_200_noWhich.conll\")\n",
    "# output_conll_file_pred = os.path.join(output_dir, \"pred_model305.conll\")\n",
    "compute_conll_score(output_conll_file_gt, output_conll_file_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['python', './wrong_conll_scorer_example/coval/scorer.py', '../../resources/eval/gt_100_noWhich.conll', '../../resources/eval/pred_model204.conll']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb Cell 63\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.97.37.223/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m overall_f1 \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.97.37.223/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m command \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m, scorer_path, output_conll_file_gt, output_conll_file_pred]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.97.37.223/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m result \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mrun(command, capture_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.97.37.223/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m out \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.97.37.223/home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/src/coreference_resolution/eval.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m err \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/corenlp/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['python', './wrong_conll_scorer_example/coval/scorer.py', '../../resources/eval/gt_100_noWhich.conll', '../../resources/eval/pred_model204.conll']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "output_conll_file_gt = os.path.join(output_dir, \"gt_100_noWhich.conll\")\n",
    "output_conll_file_pred = os.path.join(output_dir, \"pred_model204.conll\")\n",
    "scorer_path = \"./wrong_conll_scorer_example/coval/scorer.py\"\n",
    "overall_f1 = []\n",
    "command = [\"python\", scorer_path, output_conll_file_gt, output_conll_file_pred]\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, check=True)\n",
    "out = result.stdout.decode('utf-8')\n",
    "err = result.stderr.decode('utf-8')\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('corenlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
