{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve the MIMIC-CXR manual annotation\n",
    "\n",
    "Output the conll formatted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_SINGLETON = False\n",
    "brat_source_dir = \"../../output/brat_annotation/round4_500_1234r3\"\n",
    "output_base_dir = \"../../output/mimic_cxr/manual_training_set/round4_500_1234r3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "# display(HTML(df.to_html()))\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
    "from common_utils.coref_utils import resolve_mention_and_group_num\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
    "\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()\n",
    "\n",
    "mpl.style.use(\"default\")\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"nlp_ensemble\"):\n",
    "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+nlp_ensemble@_global_=mimic_cxr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_and_remove_dirs(output_base_dir, True)\n",
    "check_and_create_dirs(output_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from BRAT output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sublist,source_list) -> tuple[int,int]:\n",
    "    \"\"\" Returns: start index, end index (inclusive) \"\"\"\n",
    "    sll=len(sublist)\n",
    "    for ind in (i for i,e in enumerate(source_list) if e==sublist[0]):\n",
    "        if source_list[ind:ind+sll]==sublist:\n",
    "            return ind,ind+sll-1\n",
    "\n",
    "# greeting = [\"\\n\",\"\"]\n",
    "# print(find_sub_list([\"\"], greeting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratMention:\n",
    "    def __init__(self,id,start,end, mention_str) -> None:\n",
    "        self.id = id\n",
    "        self.tok_start = start\n",
    "        self.tok_end = end # Not inclusive\n",
    "        self.group_id_list = [] # Will only have one element, as brat ann scheme not allow to assign one mention to multi coref cluster (for now)\n",
    "        self.mention_str = mention_str\n",
    "    \n",
    "    def __eq__(self, __o: object) -> bool:\n",
    "        if isinstance(__o, BratMention):\n",
    "            return self.id == __o.id\n",
    "        else:\n",
    "            return self.id == __o\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.id.__hash__()\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.id}({self.tok_start},{self.tok_end})\"\n",
    "\n",
    "class BratCorefGroup:\n",
    "    def __init__(self) -> None:\n",
    "        self.coref_group:list[set[BratMention]] = []\n",
    "\n",
    "\n",
    "    def add(self, ment_a:BratMention, ment_b:BratMention):\n",
    "        to_be_merged = set()\n",
    "        for group_id, mention_set in enumerate(self.coref_group):\n",
    "            if ment_a in mention_set or ment_b in mention_set:\n",
    "                to_be_merged.add(group_id)\n",
    "                mention_set.update([ment_a,ment_b])\n",
    "        if len(to_be_merged) == 0:\n",
    "            # Not exist in curr group, thus create new group\n",
    "            self.coref_group.append({ment_a,ment_b})\n",
    "        elif len(to_be_merged) > 1:\n",
    "            # Exist in multiple groups, thus need to merge\n",
    "            new_group = set()\n",
    "            to_be_removed = list(to_be_merged.copy())\n",
    "            while to_be_merged:\n",
    "                new_group = new_group.union(self.coref_group[to_be_merged.pop()])\n",
    "            for index in sorted(to_be_removed, reverse=True):\n",
    "                del self.coref_group[index]\n",
    "            self.coref_group.append(new_group)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        out = []\n",
    "        for group in self.coref_group:\n",
    "            out.append(\",\".join(map(str, group)))\n",
    "        return \"|\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError warning: doc_id: s53641457, brat label: [COPD], spacy token: [COPD.No]\n",
      "Output:  ../../output/mimic_cxr/manual_training_set/round4_500_1234r3/findings\n",
      "Output:  ../../output/mimic_cxr/manual_training_set/round4_500_1234r3/impression\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "for section_name in [\"findings\",\"impression\"]:\n",
    "    brat_dir = os.path.join(brat_source_dir, section_name)\n",
    "    spacy_dir = os.path.join(\"../../output/mimic_cxr/nlp_ensemble/spacy\",section_name)\n",
    "    for doc_id in [f.rstrip(\".txt\") for f in FILE_CHECKER.filter(os.listdir(brat_dir)) if \".txt\" in f]:\n",
    "        # if doc_id != \"s55536649\" or section_name != \"impression\":\n",
    "        #     continue\n",
    "        \n",
    "        # brat outputs\n",
    "        with open(os.path.join(brat_dir, doc_id+\".txt\"), \"r\", encoding=\"UTF-8\") as f:\n",
    "            txt_file_str = \"\".join(f.readlines())\n",
    "        with open(os.path.join(brat_dir, doc_id+\".ann\"), \"r\", encoding=\"UTF-8\") as f:\n",
    "            ann_file_list = f.readlines()\n",
    "\n",
    "        # The source of the brat txt files.\n",
    "        df_spacy = pd.read_csv(os.path.join(spacy_dir, f\"{doc_id}.csv\"), index_col=0, na_filter=False)\n",
    "        # Sometime a token is whitespace, which would make the split() not work as expecting. Thus we use other symbol\n",
    "        df_sentence = df_spacy.groupby(['[sp]sentence_group'])['[sp]token'].apply('#@#'.join).reset_index()\n",
    "        sentences_withoutstrip = [str(_series.get(\"[sp]token\")) for _, _series in df_sentence.iterrows()]\n",
    "        sentence_tok_id = [arr for key, arr in df_spacy.groupby(['[sp]sentence_group']).indices.items()]\n",
    "        \n",
    "        # Align to spacy. Also read the brat token offset.\n",
    "        idx = 0\n",
    "        brat_offset = 0\n",
    "        df_brat = pd.DataFrame(columns=[\"brat_tok\",\"spacy_index\",\"brat_offset\"])\n",
    "        for sent_id, (sentence_str, id_list) in enumerate(zip(sentences_withoutstrip, sentence_tok_id)):\n",
    "            tok_list_spacy = sentence_str.split(\"#@#\")\n",
    "            tok_list_brat = sentence_str.strip().strip(\"#@#\").strip(\"\").split(\"#@#\") # When generating brat txt, whitespaces are stripped.\n",
    "\n",
    "            if len(tok_list_brat) == 1 and tok_list_brat[0] == \"\":\n",
    "                start, end = 0, 0\n",
    "            else:\n",
    "                start, end = find_sub_list(tok_list_brat, tok_list_spacy)\n",
    "            \n",
    "            prev_brat_tok = \"\"\n",
    "            for brat_tok, spacy_idx in zip(tok_list_brat, id_list[start:end+1]):\n",
    "                brat_offset = brat_offset + len(prev_brat_tok) + txt_file_str[brat_offset+len(prev_brat_tok):].index(brat_tok)\n",
    "                if not df_brat.empty and brat_offset == df_brat.iloc[-1][\"brat_offset\"]:\n",
    "                    brat_offset += 1 # In case brat_tok is \"\". And doing this do not affect the next tok\n",
    "                df_brat.loc[idx] = (brat_tok, spacy_idx, brat_offset)\n",
    "                idx+=1\n",
    "                prev_brat_tok = brat_tok\n",
    "                \n",
    "        df_aligned = df_spacy.merge(df_brat, how=\"outer\", left_index=True, right_on=\"spacy_index\").reset_index().drop(columns=[\"index\"])\n",
    "        df_aligned = df_aligned.loc[:,[\"[sp]token\",\"[sp]token_offset\",\"[sp]sentence_group\",\"brat_tok\",\"spacy_index\", \"brat_offset\"]]\n",
    "        df_aligned[\"[gt]coref_group\"] = [-1] * len(df_aligned)\n",
    "        df_aligned[\"[gt]coref_group_conll\"] = [-1] * len(df_aligned)\n",
    "\n",
    "        # Resolve brat files\n",
    "        mention_list:list[BratMention] = []\n",
    "        brat_coref_obj = BratCorefGroup()\n",
    "        for line in [line.strip() for line in ann_file_list]:\n",
    "            line_info_list = line.split(\"\\t\")\n",
    "            # print(line_info_list)\n",
    "            if line[0] == \"T\":\n",
    "                # Mention\n",
    "                mention_id = line_info_list[0]\n",
    "                ment_start = line_info_list[1].split(\" \")[1]\n",
    "                ment_end = line_info_list[1].split(\" \")[-1]\n",
    "                mention_str = line_info_list[2]\n",
    "                mention_list.append(BratMention(mention_id,ment_start,ment_end, mention_str))\n",
    "            elif line[0] == \"R\":\n",
    "                # relation\n",
    "                relation_id = line_info_list[0]\n",
    "                mention_a_id = line_info_list[1].split(\" \")[1].split(\":\")[-1]\n",
    "                mention_b_id = line_info_list[1].split(\" \")[2].split(\":\")[-1]\n",
    "                mention_a = mention_list[mention_list.index(mention_a_id)]\n",
    "                mention_b = mention_list[mention_list.index(mention_b_id)]\n",
    "                brat_coref_obj.add(mention_a, mention_b)\n",
    "\n",
    "        # Assign coref group id to mentions\n",
    "        for coref_id, coref_group in enumerate(brat_coref_obj.coref_group):\n",
    "            for mention in coref_group:\n",
    "                mention.group_id_list.append(coref_id)\n",
    "           \n",
    "        # Assign coref group id to singleton mention\n",
    "        all_coreferent_mention = [mention for coref_group in brat_coref_obj.coref_group for mention in coref_group]\n",
    "        next_coref_id = len(brat_coref_obj.coref_group)\n",
    "        for mention in mention_list:\n",
    "            if mention not in all_coreferent_mention:\n",
    "                mention.group_id_list.append(next_coref_id)\n",
    "                next_coref_id += 1\n",
    "\n",
    "        # Put conll labels into df\n",
    "        if INCLUDE_SINGLETON:\n",
    "            source_mention_list = mention_list\n",
    "        else:\n",
    "            source_mention_list = all_coreferent_mention\n",
    "            \n",
    "        for mention in source_mention_list:\n",
    "            row_condition = (df_aligned['brat_offset'] >= int(mention.tok_start)) & (df_aligned['brat_offset'] < int(mention.tok_end))\n",
    "            # df_aligned.loc[row_condition, \"[gt]coref_group\"] = int(coref_id)\n",
    "            target_rows = df_aligned.loc[row_condition, \"[gt]coref_group\"]\n",
    "            mention_str = \"\"\n",
    "            if len(target_rows) == 1: # mention has only one token\n",
    "                target_idx = df_aligned.loc[row_condition].iloc[0].name\n",
    "                if df_aligned.loc[target_idx,\"[gt]coref_group\"] == -1:\n",
    "                    df_aligned.loc[target_idx,\"[gt]coref_group\"] = str(mention.group_id_list)\n",
    "                    df_aligned.loc[target_idx, \"[gt]coref_group_conll\"] = str([f\"({coref_id})\" for coref_id in mention.group_id_list])\n",
    "                else:\n",
    "                    # Append new element to exiting list\n",
    "                    group_id_list = ast.literal_eval(df_aligned.loc[target_idx,\"[gt]coref_group\"])\n",
    "                    group_id_list.extend(mention.group_id_list)\n",
    "                    df_aligned.loc[target_idx,\"[gt]coref_group\"] = str(list(set(group_id_list)))\n",
    "                    \n",
    "                    group_conll_str_list = ast.literal_eval(df_aligned.loc[target_idx,\"[gt]coref_group_conll\"])\n",
    "                    group_conll_str_list.extend([f\"({coref_id})\" for coref_id in mention.group_id_list])\n",
    "                    df_aligned.loc[target_idx, \"[gt]coref_group_conll\"] = str(group_conll_str_list)\n",
    "                    \n",
    "                mention_str = \" \".join(df_aligned.loc[row_condition].get(\"[sp]token\").to_list())\n",
    "            elif len(target_rows) > 1: # mention has more than one token\n",
    "                # coref_group\n",
    "                for index, row_series in df_aligned.loc[row_condition].iterrows():\n",
    "                    if row_series.loc[\"[gt]coref_group\"] == -1:\n",
    "                        df_aligned.loc[index,\"[gt]coref_group\"] = str(mention.group_id_list)\n",
    "                    else:\n",
    "                        # Append new element to exiting list\n",
    "                        group_id_list = ast.literal_eval(row_series.loc[\"[gt]coref_group\"])\n",
    "                        group_id_list.extend(mention.group_id_list)\n",
    "                        df_aligned.loc[index,\"[gt]coref_group\"] = str(list(set(group_id_list)))\n",
    "                \n",
    "                # coref_group_conll   \n",
    "                first_idx = df_aligned.loc[row_condition].iloc[0].name\n",
    "                last_idx = df_aligned.loc[row_condition].iloc[-1].name\n",
    "                \n",
    "                if df_aligned.loc[first_idx, \"[gt]coref_group_conll\"] == -1:\n",
    "                    df_aligned.loc[first_idx, \"[gt]coref_group_conll\"] = str([f\"({coref_id}\" for coref_id in mention.group_id_list])\n",
    "                else:\n",
    "                    group_conll_str_list = ast.literal_eval(df_aligned.loc[first_idx, \"[gt]coref_group_conll\"])\n",
    "                    group_conll_str_list.extend([f\"({coref_id}\" for coref_id in mention.group_id_list])\n",
    "                    df_aligned.loc[first_idx, \"[gt]coref_group_conll\"] = str(group_conll_str_list)\n",
    "                \n",
    "                if df_aligned.loc[last_idx, \"[gt]coref_group_conll\"] == -1:\n",
    "                    df_aligned.loc[last_idx, \"[gt]coref_group_conll\"] = str([f\"{coref_id})\" for coref_id in mention.group_id_list])\n",
    "                else:\n",
    "                    group_conll_str_list = ast.literal_eval(df_aligned.loc[last_idx, \"[gt]coref_group_conll\"])\n",
    "                    group_conll_str_list.extend([f\"{coref_id})\" for coref_id in mention.group_id_list])\n",
    "                    df_aligned.loc[last_idx, \"[gt]coref_group_conll\"] = str(group_conll_str_list)\n",
    "                \n",
    "                mention_str = \" \".join(df_aligned.loc[first_idx:last_idx].get(\"[sp]token\").to_list())\n",
    "                        \n",
    "            try:\n",
    "                assert mention_str == mention.mention_str\n",
    "            except AssertionError as err:\n",
    "                print(f\"AssertionError warning: doc_id: {doc_id}, brat label: [{mention.mention_str}], spacy token: [{mention_str}]\")\n",
    "                # raise err\n",
    "        # display(HTML(df_aligned.to_html()))\n",
    "\n",
    "        # Write CSV files\n",
    "        output_dir = os.path.join(output_base_dir,section_name)\n",
    "        check_and_create_dirs(output_dir)\n",
    "        df_out = df_aligned.loc[:,[\"[sp]token\",\"[sp]sentence_group\",\"[gt]coref_group\",\"[gt]coref_group_conll\"]]\n",
    "        # display(HTML(df_out.to_html()))\n",
    "        df_out.to_csv(os.path.join(output_dir, f\"{doc_id}.csv\"))\n",
    "        \n",
    "    print(\"Output: \",output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('corenlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
