{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generate fast-coref model output as csv format"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare\n",
                "\n",
                "We dont want to process all the documents, instead, we only need 1000/343/348 data for train/dev/test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../../src\")\n",
                "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
                "\n",
                "import os\n",
                "import ast\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "from collections import defaultdict\n",
                "from IPython.display import display, HTML\n",
                "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
                "from multiprocessing import Event\n",
                "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
                "from common_utils.coref_utils import resolve_mention_and_group_num, shuffle_list, ConllToken, check_and_make_dir, get_data_split, get_file_name_prefix, get_porportion_and_name, remove_all, resolve_mention_and_group_num, shuffle_list\n",
                "from common_utils.file_checker import FileChecker\n",
                "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
                "\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "FILE_CHECKER = FileChecker()\n",
                "START_EVENT = Event()\n",
                "\n",
                "SEED_NUM = 42\n",
                "\n",
                "mpl.style.use(\"default\")\n",
                "\n",
                "SMALL_SIZE = 12\n",
                "MEDIUM_SIZE = 14\n",
                "BIGGER_SIZE = 16\n",
                "\n",
                "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
                "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
                "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
                "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
                "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
                "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
                "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hydra import compose, initialize\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "config = None\n",
                "with initialize(version_base=None, config_path=\"../config\", job_name=\"nlp_ensemble\"):\n",
                "        config = compose(config_name=\"data_preprocessing\", overrides=[\"+nlp_ensemble@_global_=mimic_cxr\"])\n",
                "        \n",
                "section_name_cfg = config.name_style.mimic_cxr.section_name\n",
                "output_section_cfg = config.output.section\n",
                "input_path = config.input.path\n",
                "data_size, pid_list, sid_list, section_list = load_mimic_cxr_bySection(input_path, output_section_cfg, section_name_cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sort\n",
                "s_list, f_list, i_list, pfi_list, fai_list = zip(*sorted(zip(sid_list, section_list[0][1], section_list[1][1], section_list[2][1], section_list[3][1])))\n",
                "sid_list = s_list\n",
                "section_list = [\n",
                "        (\"findings\", f_list),\n",
                "        (\"impression\", i_list),\n",
                "        (\"provisional_findings_impression\", pfi_list),\n",
                "        (\"findings_and_impression\",fai_list)\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "section_corefNum_docs_dict_file = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input).statistic\"\n",
                "\n",
                "### Modify ###\n",
                "scoref_dir = \"../../output/mimic_cxr/nlp_ensemble/corenlp/scoref\"\n",
                "dcoref_dir = \"../../output/mimic_cxr/nlp_ensemble/corenlp/dcoref\"\n",
                "fcoref_dir = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input)\"\n",
                "\n",
                "def batch_processing(section_name, sid, spacy_input_path):\n",
                "    START_EVENT.wait()\n",
                "    # df_spacy = pd.read_csv(spacy_input_path, index_col=0, na_filter=False)\n",
                "    # df_scoref = pd.read_csv(os.path.join(scoref_dir,section_name,sid+\".csv\"), index_col=0, na_filter=False)\n",
                "    # df_dcoref = pd.read_csv(os.path.join(dcoref_dir,section_name,sid+\".csv\"), index_col=0, na_filter=False)\n",
                "    df_fcoref = pd.read_csv(os.path.join(fcoref_dir,section_name,sid+\".csv\"), index_col=0, na_filter=False)\n",
                "\n",
                "    # token_list = df_spacy.loc[:,\"[sp]token\"].to_list()\n",
                "    # token_num = len(token_list)\n",
                "\n",
                "    # _, scoref_group_num = resolve_mention_and_group_num(df_scoref, \"[co][ml]coref_group_conll\")\n",
                "    # _, dcoref_group_num = resolve_mention_and_group_num(df_dcoref, \"[co][rb]coref_group_conll\")\n",
                "    _, fcoref_group_num = resolve_mention_and_group_num(df_fcoref, \"[fj]coref_group_conll\")\n",
                "    token_num,scoref_group_num,dcoref_group_num = 0,0,0\n",
                "\n",
                "    return sid, token_num, scoref_group_num, dcoref_group_num, fcoref_group_num\n",
                "\n",
                "section_corefNum_docs_dict = {}\n",
                "\n",
                "if not os.path.exists(section_corefNum_docs_dict_file):\n",
                "    section_doc_numData_dict:dict[str,dict[str,dict[str,int]]] = {}\n",
                "    section_scatter_data_list = {}\n",
                "    for section_entry in os.scandir(\"../../output/mimic_cxr/nlp_ensemble/spacy\"):\n",
                "        if section_entry.is_dir():\n",
                "            print(\"Processing section:\", section_entry.name)\n",
                "            section_doc_numData_dict[section_entry.name]:dict[str,dict[str,int]] = {}\n",
                "\n",
                "            tasks = []\n",
                "            scatter_data_list:list[dict] = []\n",
                "            with ProcessPoolExecutor(max_workers=14) as executor:\n",
                "                for report_entry in tqdm(os.scandir(section_entry.path)):\n",
                "                    if FILE_CHECKER.ignore(os.path.abspath(report_entry.path)):\n",
                "                        continue\n",
                "                    sid = report_entry.name.rstrip(\".csv\")\n",
                "                    tasks.append(executor.submit(batch_processing,section_entry.name, sid, report_entry.path))\n",
                "\n",
                "                START_EVENT.set()\n",
                "\n",
                "                # Receive results from multiprocessing.\n",
                "                for future in tqdm(as_completed(tasks), total=len(tasks)):\n",
                "                    sid, token_num, scoref_group_num, dcoref_group_num, fcoref_group_num = future.result()\n",
                "                    numData = {\n",
                "                        \"tokNum\":token_num,\n",
                "                        \"sNum\": scoref_group_num,\n",
                "                        \"dNum\": dcoref_group_num,\n",
                "                        \"fNum\": fcoref_group_num,\n",
                "                        \"avgNum\": (scoref_group_num + dcoref_group_num + fcoref_group_num) / 3\n",
                "                    }\n",
                "                    # For later statistic\n",
                "                    section_doc_numData_dict[section_entry.name][sid]:dict[str,int] = numData\n",
                "                    # For scatter plot\n",
                "                    scatter_data_list.append(numData)\n",
                "\n",
                "                START_EVENT.clear()\n",
                "\n",
                "            section_scatter_data_list[section_entry.name] = scatter_data_list\n",
                "    \n",
                "    for section_name, doc_numData_dict in section_doc_numData_dict.items():\n",
                "        section_corefNum_docs_dict[section_name] = defaultdict(list)\n",
                "        for doc_id, numData_dict in doc_numData_dict.items():\n",
                "            section_corefNum_docs_dict[section_name][numData_dict[\"fNum\"]].append(doc_id)\n",
                "    \n",
                "    with open(section_corefNum_docs_dict_file,\"w\") as f:\n",
                "        f.write(json.dumps(section_corefNum_docs_dict))\n",
                "else:\n",
                "    with open(section_corefNum_docs_dict_file,\"r\") as f:\n",
                "        a = f.readlines()\n",
                "    temp = json.loads(\"\".join(a))\n",
                "    for section_name, corefNum_docs_dict in temp.items():\n",
                "        section_corefNum_docs_dict[section_name] = {}\n",
                "        for corefNumStr, docList in corefNum_docs_dict.items():\n",
                "            section_corefNum_docs_dict[section_name][int(corefNumStr)] = docList"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "  \"findings\": {\n",
                        "    \"0\": 135615,\n",
                        "    \"1\": 16800,\n",
                        "    \"2\": 2804,\n",
                        "    \"3\": 590,\n",
                        "    \"4\": 154,\n",
                        "    \"5\": 27,\n",
                        "    \"6\": 13,\n",
                        "    \"7\": 6,\n",
                        "    \"8\": 2\n",
                        "  },\n",
                        "  \"impression\": {\n",
                        "    \"0\": 173650,\n",
                        "    \"1\": 13016,\n",
                        "    \"2\": 2155,\n",
                        "    \"3\": 506,\n",
                        "    \"4\": 100,\n",
                        "    \"5\": 28,\n",
                        "    \"6\": 8,\n",
                        "    \"7\": 1\n",
                        "  }\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "section_corefNum_docNum_dict = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    corefNum_docs_dict = section_corefNum_docs_dict[section_name]\n",
                "    section_corefNum_docNum_dict[section_name] = {}\n",
                "    corefNum = 0\n",
                "    while True:\n",
                "        if corefNum not in corefNum_docs_dict:\n",
                "            break\n",
                "        section_corefNum_docNum_dict[section_name][corefNum] = len(corefNum_docs_dict[corefNum])\n",
                "        corefNum += 1\n",
                "print(json.dumps(section_corefNum_docNum_dict, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 846/846\n",
                "expected_sampling = {\n",
                "  \"findings\": 846,\n",
                "  \"impression\": 846\n",
                "}\n",
                "\n",
                "target_sampling = {\n",
                "  \"findings\": {\n",
                "    1: None, # 846 - rest\n",
                "    2: 214,\n",
                "    3: 214,\n",
                "    4: 154,\n",
                "    5: 27,\n",
                "    6: 13,\n",
                "    7: 6,\n",
                "    8: 2\n",
                "  },\n",
                "  \"impression\": {\n",
                "    1: None, # 846 - rest\n",
                "    2: 236,\n",
                "    3: 236,\n",
                "    4: 100,\n",
                "    5: 28,\n",
                "    6: 8,\n",
                "    7: 1\n",
                "  }\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hydra import compose, initialize\n",
                "from omegaconf import OmegaConf\n",
                "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
                "config = None\n",
                "with initialize(version_base=None, config_path=\"../config\", job_name=\"coreference_resolution\"):\n",
                "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+coreference_resolution/data_preprocessing@_global_=mimic_cxr\"])\n",
                "shuffle_seed = config.shuffle_seed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose docs for model training, excluded docs that used in gt\n",
                "docId_testset_list = [i.rstrip(\".csv\") for i in FILE_CHECKER.filter(os.listdir(os.path.join(\"../../output/mimic_cxr/manual_test_set/round1x2\", section_name)))]\n",
                "trainset_docs_list = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    trainset_docs_list[section_name] = defaultdict(list)\n",
                "    section_all_docNum = 0\n",
                "    for groupNum in sorted(target_sampling[section_name], reverse=True):\n",
                "        docNum = target_sampling[section_name][groupNum]\n",
                "        if groupNum == 1:\n",
                "            docNum = expected_sampling[section_name] - section_all_docNum\n",
                "        candidate_docId_list = section_corefNum_docs_dict[section_name][groupNum]\n",
                "        candidate_docId_list_exclude = [x for x in candidate_docId_list if x not in docId_testset_list]\n",
                "        candidate_docId_list_shuffle = shuffle_list(candidate_docId_list_exclude, shuffle_seed)\n",
                "        trainset_docs_list[section_name][groupNum] = candidate_docId_list_shuffle[0:docNum]\n",
                "        section_all_docNum += len(trainset_docs_list[section_name][groupNum])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The actual sampling details"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "findings 8 2\n",
                        "findings 7 6\n",
                        "findings 6 13\n",
                        "findings 5 26\n",
                        "findings 4 154\n",
                        "findings 3 214\n",
                        "findings 2 214\n",
                        "findings 1 217\n",
                        "impression 7 1\n",
                        "impression 6 8\n",
                        "impression 5 26\n",
                        "impression 4 99\n",
                        "impression 3 236\n",
                        "impression 2 236\n",
                        "impression 1 240\n"
                    ]
                }
            ],
            "source": [
                "for section_name, data_dict in trainset_docs_list.items():\n",
                "    for group_num, doc_list in data_dict.items():\n",
                "        print(section_name, group_num, len(doc_list))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Opt 1: Generate ensemble csv files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from common_utils.coref_utils import shuffle_list\n",
                "import coref_voting\n",
                "from coref_voting import DocClass, MentionClass, compute_voting_result, get_output_df\n",
                "from hydra import compose, initialize\n",
                "from omegaconf import OmegaConf\n",
                "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
                "\n",
                "\n",
                "config = None\n",
                "with initialize(version_base=None, config_path=\"../config\", job_name=\"majority_voting\"):\n",
                "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+coreference_resolution/coref_voting@_global_=mimic_cxr\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Modify ###\n",
                "config.input.source.coref_models.fj.dir = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input)\"\n",
                "\n",
                "config.input.source.in_use = [\"ml\",\"rb\",\"fj\"] # ml, rb, fj, gt, fj_x, fj_x2\n",
                "mv_output_base_dir = os.path.join(\"../../output/mimic_cxr/coref_voting/temp_for_silver/ml_rb_fj_stripped_1k\")\n",
                "for section_name in [\"findings\",\"impression\"]:\n",
                "    mv_output_dir = os.path.join(mv_output_base_dir, section_name)\n",
                "    check_and_create_dirs(mv_output_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 846/846 [03:46<00:00,  3.73it/s]\n",
                        "100%|██████████| 846/846 [03:21<00:00,  4.21it/s]\n"
                    ]
                }
            ],
            "source": [
                "def do_majority_voting(config, spacy_file_path, section_name, file_name):\n",
                "    \"\"\" Voting on one document \"\"\"\n",
                "    START_EVENT.wait()\n",
                "\n",
                "    # Read spacy output as alignment base\n",
                "    df_spacy = pd.read_csv(spacy_file_path, index_col=0, na_filter=False)\n",
                "    # Some of the i2b2 raw files are utf-8 start with DOM, but we didn't remove the DOM character, thus we fix it here.\n",
                "    df_spacy.iloc[0] = df_spacy.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
                "\n",
                "    docObj: DocClass = coref_voting.resolve_voting_info(config, df_spacy, section_name, file_name)\n",
                "    valid_mention_group: list[set[MentionClass]] = compute_voting_result(config, docObj)\n",
                "    df_out = get_output_df(config, df_spacy, valid_mention_group, docObj)\n",
                "    \n",
                "    output_file_path = os.path.join(mv_output_base_dir, section_name, file_name)\n",
                "    df_out.to_csv(output_file_path)\n",
                "\n",
                "    return file_name\n",
                "\n",
                "for section_name in [\"findings\",\"impression\"]:\n",
                "    with ProcessPoolExecutor(max_workers=config.thread.workers) as executor:\n",
                "        all_task = []\n",
                "        doc_list = [docid for _, docs in trainset_docs_list[section_name].items() for docid in docs]\n",
                "        \n",
                "        for doc_id in doc_list:\n",
                "            file_name = doc_id + \".csv\"\n",
                "            spacy_out_dir = os.path.join(config.input.source.baseline_model.dir, section_name)\n",
                "            spacy_file_path = os.path.join(spacy_out_dir, file_name)\n",
                "            all_task.append(executor.submit(do_majority_voting, config, spacy_file_path, section_name, file_name))\n",
                "        \n",
                "         # Notify tasks to start\n",
                "        START_EVENT.set()\n",
                "\n",
                "        if all_task:\n",
                "            for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
                "                file_name = future.result()\n",
                "\n",
                "        executor.shutdown(wait=True, cancel_futures=False)\n",
                "        START_EVENT.clear()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "statistics \n",
                "\n",
                "(re-run the following scripts wihtin this section, if not getting enough docs for silver-ensemble)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1007/1007 [00:00<00:00, 4812.06it/s]\n",
                        "100%|██████████| 1035/1035 [00:00<00:00, 4964.25it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "findings 1 406\n",
                        "findings 2 241\n",
                        "findings 3 126\n",
                        "findings 0 161\n",
                        "findings 5 16\n",
                        "findings 4 48\n",
                        "findings 7 2\n",
                        "findings 6 7\n",
                        "impression 1 442\n",
                        "impression 2 285\n",
                        "impression 0 189\n",
                        "impression 3 84\n",
                        "impression 4 26\n",
                        "impression 5 6\n",
                        "impression 6 3\n",
                        "findings 1007\n",
                        "impression 1035\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "section_corefNum_docs_dict_file = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input).statistic\"\n",
                "\n",
                "### Modify ###\n",
                "ensemble_dir = mv_output_base_dir\n",
                "\n",
                "def batch_processing(section_name, sid):\n",
                "    START_EVENT.wait()\n",
                "    df_fcoref = pd.read_csv(os.path.join(ensemble_dir,section_name,sid+\".csv\"), index_col=0, na_filter=False)\n",
                "    _, fcoref_group_num = resolve_mention_and_group_num(df_fcoref, \"[mv]coref_group_conll\")\n",
                "    return sid,  fcoref_group_num\n",
                "\n",
                "section_corefNum_ensembleDoc_dict = {}\n",
                "\n",
                "section_scatter_data_list = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    section_corefNum_ensembleDoc_dict[section_name] = defaultdict(list)\n",
                "    \n",
                "    tasks = []\n",
                "    with ProcessPoolExecutor(max_workers=14) as executor:\n",
                "        for doc_filename in FILE_CHECKER.filter(os.listdir(os.path.join(ensemble_dir, section_name))):\n",
                "            sid = doc_filename.rstrip(\".csv\")\n",
                "            tasks.append(executor.submit(batch_processing, section_name, sid))\n",
                "\n",
                "        START_EVENT.set()\n",
                "\n",
                "        # Receive results from multiprocessing.\n",
                "        for future in tqdm(as_completed(tasks), total=len(tasks)):\n",
                "            sid, fcoref_group_num = future.result()\n",
                "            # For later statistic\n",
                "            section_corefNum_ensembleDoc_dict[section_name][fcoref_group_num].append(sid)\n",
                "\n",
                "        START_EVENT.clear()\n",
                "\n",
                "for section_name, data_dict in section_corefNum_ensembleDoc_dict.items():\n",
                "    for group_num, doc_list in data_dict.items():\n",
                "        print(section_name, group_num, len(doc_list))\n",
                "        \n",
                "for section_name, data_dict in section_corefNum_ensembleDoc_dict.items():\n",
                "    a = 0\n",
                "    for group_num, doc_list in data_dict.items():\n",
                "        a += len(doc_list)\n",
                "    print(section_name, a)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After doing ensemble, some of the target docs might become no_coref, and will be ignored later. \n",
                "\n",
                "Therefore, we need to add more docs that have >1_coref to make the final training set to be 1000 docs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0\n",
                        "0\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'findings': {1: 0, 2: 0, 3: 0}, 'impression': {1: 0, 2: 0, 3: 0}}"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import math\n",
                "\n",
                "section_rest_sampling = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    num_needed = expected_sampling[section_name] - len([doc for coref_num, doc_list in section_corefNum_ensembleDoc_dict[section_name].items() if coref_num != 0 for doc in doc_list])\n",
                "    print(num_needed)\n",
                "    section_rest_sampling[section_name] = {1: math.ceil(num_needed/3), 2: math.ceil(num_needed/3), 3: num_needed - math.ceil(num_needed/3)*2}\n",
                "section_rest_sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 16583/16583 [00:22<00:00, 725.31it/s]  \n",
                        "100%|██████████| 2590/2590 [00:21<00:00, 118.74it/s] \n",
                        "100%|██████████| 376/376 [00:26<00:00, 14.30it/s] \n",
                        "100%|██████████| 12776/12776 [00:24<00:00, 517.38it/s]  \n",
                        "100%|██████████| 1919/1919 [00:21<00:00, 88.42it/s]  \n",
                        "100%|██████████| 270/270 [00:26<00:00, 10.16it/s] \n"
                    ]
                }
            ],
            "source": [
                "def do_majority_voting2(config, spacy_file_path, section_name, file_name):\n",
                "    \"\"\" Voting on one document \"\"\"\n",
                "    START_EVENT.wait()\n",
                "    mv_output_dir = os.path.join(mv_output_base_dir, section_name)\n",
                "    check_and_create_dirs(mv_output_dir)\n",
                "    output_file_path = os.path.join(mv_output_dir, file_name)\n",
                "    \n",
                "    if os.path.exists(output_file_path):\n",
                "        df_out = pd.read_csv(output_file_path, index_col=0, na_filter=False)\n",
                "    else:\n",
                "        # Read spacy output as alignment base\n",
                "        df_spacy = pd.read_csv(spacy_file_path, index_col=0, na_filter=False)\n",
                "        # Some of the i2b2 raw files are utf-8 start with DOM, but we didn't remove the DOM character, thus we fix it here.\n",
                "        df_spacy.iloc[0] = df_spacy.iloc[0].apply(lambda x: x.replace(\"\\ufeff\", \"\").replace(\"\\xef\\xbb\\xbf\", \"\") if isinstance(x, str) else x)\n",
                "\n",
                "        docObj: DocClass = coref_voting.resolve_voting_info(config, df_spacy, section_name, file_name)\n",
                "        valid_mention_group: list[set[MentionClass]] = compute_voting_result(config, docObj)\n",
                "        df_out = get_output_df(config, df_spacy, valid_mention_group, docObj)\n",
                "\n",
                "    _, fcoref_group_num = resolve_mention_and_group_num(df_out, \"[mv]coref_group_conll\")\n",
                "    \n",
                "    return file_name, fcoref_group_num, df_out, output_file_path\n",
                "\n",
                "docId_testset_list = [i.rstrip(\".csv\") for i in FILE_CHECKER.filter(os.listdir(os.path.join(\"../../output/mimic_cxr/manual_test_set/round1x2\", section_name)))]\n",
                "\n",
                "doc_added = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    doc_added[section_name] = defaultdict(list)\n",
                "    for group_num, doc_num_needed in section_rest_sampling[section_name].items():\n",
                "        \n",
                "        # all docs under this groupNum\n",
                "        candidate_docId_list = section_corefNum_docs_dict[section_name][group_num]\n",
                "        # Exclude docs that used in test set\n",
                "        candidate_docId_list_exclude = [x for x in candidate_docId_list if x+\".csv\" not in docId_testset_list]\n",
                "        # exclude docs that already in target dir\n",
                "        exising_docs = os.listdir(os.path.join(mv_output_base_dir,section_name))\n",
                "        candidate_docId_list_exclude = [x for x in candidate_docId_list if x+\".csv\" not in exising_docs]\n",
                "        \n",
                "        with ProcessPoolExecutor(max_workers=config.thread.workers) as executor:\n",
                "            all_task = []\n",
                "            \n",
                "            for doc_id in candidate_docId_list_exclude:\n",
                "                file_name = doc_id + \".csv\"\n",
                "                spacy_out_dir = os.path.join(config.input.source.baseline_model.dir, section_name)\n",
                "                spacy_file_path = os.path.join(spacy_out_dir, file_name)\n",
                "                all_task.append(executor.submit(do_majority_voting2, config, spacy_file_path, section_name, file_name))\n",
                "                    \n",
                "            # Notify tasks to start\n",
                "            START_EVENT.set()\n",
                "\n",
                "            if all_task:\n",
                "                doc_num_added = 0\n",
                "                for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
                "                    if future.cancelled():\n",
                "                        continue\n",
                "                    file_name, fcoref_group_num, df_out, output_file_path = future.result()\n",
                "                    if fcoref_group_num > 0:\n",
                "                        doc_num_added += 1\n",
                "                        if doc_num_added <= doc_num_needed:\n",
                "                            df_out.to_csv(output_file_path)\n",
                "                            doc_added[section_name][group_num].append(file_name)\n",
                "                        else:\n",
                "                            for fu in all_task:\n",
                "                                fu.cancel()\n",
                "                            \n",
                "            START_EVENT.clear()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "findings 1 54\n",
                        "findings 2 54\n",
                        "findings 3 53\n",
                        "impression 1 63\n",
                        "impression 2 63\n",
                        "impression 3 63\n"
                    ]
                }
            ],
            "source": [
                "for section_name, data_dict in doc_added.items():\n",
                "    targetList = os.listdir(os.path.join(mv_output_base_dir,section_name))\n",
                "    for group_num, doc_list in data_dict.items():\n",
                "        print(section_name, group_num, len(doc_list))\n",
                "        for doccsv in doc_list:\n",
                "            if doccsv not in targetList:\n",
                "                print(doccsv)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Given csv files, generate individual conll files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hydra import compose, initialize\n",
                "from omegaconf import OmegaConf\n",
                "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
                "config = None\n",
                "config = None\n",
                "with initialize(version_base=None, config_path=\"../config\", job_name=\"coreference_resolution\"):\n",
                "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+coreference_resolution/data_preprocessing@_global_=mimic_cxr\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "### Modify ###\n",
                "# Choose to use \"model direct output\" or \"ensemble output\"\n",
                "# config.input_pred.base_dir = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input)\" # source csv dir\n",
                "# config.input_pred.column_name.coref_group_conll = \"[fj]coref_group_conll\"\n",
                "# config.input_pred.column_name.sentence_group = \"[fj]sentence_group\"\n",
                "# config.input_pred.column_name.token = \"[fj]token_from_spacy\"\n",
                "# config.temp_pred.base_dir = \"../../output/mimic_cxr/coref/temp_individual_conll/joint_best_(stripped_input)\" # target output dir\n",
                "\n",
                "config.input_pred.base_dir = \"../../output/mimic_cxr/coref_voting/temp_for_silver/ml_rb_fj_stripped_1k\" # source csv dir\n",
                "config.temp_pred.base_dir = \"../../output/mimic_cxr/coref/temp_individual_conll/majority_voting_(ml_rb_fj_stripped)\" # target output dir\n",
                "\n",
                "target_doc_files = {}\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    # For model direct output\n",
                "    # target_doc_files[section_name] = os.listdir(os.path.join(config.input_pred.base_dir, section_name))\n",
                "\n",
                "    # For ensemble output\n",
                "    target_doc_files[section_name] = [doc for _, docs in trainset_docs_list[section_name].items() for doc in docs]\n",
                "\n",
                "    \n",
                "config.input_pred.section = [\"findings\", \"impression\"]\n",
                "check_and_remove_dirs(config.temp_pred.base_dir,True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1007/1007 [00:00<00:00, 5383.59it/s]\n",
                        "100%|██████████| 1007/1007 [00:40<00:00, 24.67it/s]\n",
                        "100%|██████████| 1035/1035 [00:00<00:00, 5566.75it/s]\n",
                        "100%|██████████| 1035/1035 [00:36<00:00, 27.99it/s]\n"
                    ]
                }
            ],
            "source": [
                "def batch_processing(input_cfg, temp_cfg, section_name, input_file_path) -> int:\n",
                "    \"\"\" All whitespces like \"\\n\", \"\\n \" and \" \" are skipped. \n",
                "    Return:\n",
                "        True if this doc has at least one coref group. Otherwise False\n",
                "    \"\"\"\n",
                "    START_EVENT.wait()\n",
                "\n",
                "    doc_id = get_file_name_prefix(input_file_path, input_cfg.suffix)\n",
                "    BEGIN = f\"#begin document ({doc_id}_{section_name}); part 0\\n\"\n",
                "    SENTENCE_SEPARATOR = \"\\n\"\n",
                "    END = \"#end document\\n\"\n",
                "    output_file_path = os.path.join(temp_cfg.base_dir, section_name, f\"{doc_id}.conll\")\n",
                "\n",
                "    # Resolve CSV file\n",
                "    sentenc_list: list[list[ConllToken]] = []\n",
                "    df = pd.read_csv(input_file_path, index_col=0, na_filter=False)\n",
                "    _, coref_group_num = resolve_mention_and_group_num(df, input_cfg.column_name.coref_group_conll)\n",
                "\n",
                "    # Write .conll file only if doc has at least one coref group\n",
                "    if coref_group_num > 0:\n",
                "        sentence_id = 0\n",
                "        while True:\n",
                "            token_list: list[ConllToken] = []\n",
                "            df_sentence = df[df.loc[:, input_cfg.column_name.sentence_group] == sentence_id].reset_index()\n",
                "            if df_sentence.empty:\n",
                "                break\n",
                "            for _idx, data in df_sentence.iterrows():\n",
                "                # Skip all whitespces like \"\\n\", \"\\n \" and \" \".\n",
                "                if str(data[input_cfg.column_name.token]).strip() == \"\":\n",
                "                    continue\n",
                "                conllToken = ConllToken(doc_id+\"_\"+section_name, sentence_id, _idx, data[input_cfg.column_name.token])\n",
                "                coref_col_cell = data[input_cfg.column_name.coref_group_conll]\n",
                "                if isinstance(coref_col_cell, str) and coref_col_cell != \"-1\":\n",
                "                    conllToken.add_coref_label(\"|\".join(ast.literal_eval(coref_col_cell)))\n",
                "                token_list.append(conllToken)\n",
                "            sentenc_list.append(token_list)\n",
                "            sentence_id += 1\n",
                "        with open(output_file_path, \"w\", encoding=\"UTF-8\") as out:\n",
                "            out.write(BEGIN)\n",
                "            for sent in sentenc_list:\n",
                "                # Skip empty sentence\n",
                "                if len(sent) == 1 and sent[0].tokenStr == \"\":\n",
                "                    continue\n",
                "                for tok in sent:\n",
                "                    out.write(tok.get_conll_str() + \"\\n\")\n",
                "                out.write(SENTENCE_SEPARATOR)\n",
                "            out.write(END)\n",
                "\n",
                "    return doc_id, coref_group_num\n",
                "\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    section_temp_conll_dir = os.path.join(config.temp_pred.base_dir, section_name)\n",
                "    check_and_create_dirs(section_temp_conll_dir)\n",
                "    \n",
                "    doc_files = target_doc_files[section_name]\n",
                "    with ProcessPoolExecutor(max_workers=config.thread.workers) as executor:\n",
                "        all_task = []\n",
                "        for file_name in tqdm(doc_files):\n",
                "            # if len(all_task) > 100:\n",
                "            #     break\n",
                "            input_file_path = os.path.join(config.input_pred.base_dir, section_name, file_name)\n",
                "            all_task.append(executor.submit(batch_processing, config.input_pred, config.temp_pred, section_name, input_file_path))\n",
                "\n",
                "        # Notify tasks to start\n",
                "        START_EVENT.set()\n",
                "\n",
                "        corefGroupNum_docId_dict = defaultdict(list)\n",
                "        if all_task:\n",
                "            for future in tqdm(as_completed(all_task), total=len(all_task)):\n",
                "                doc_id, coref_group_num = future.result()\n",
                "\n",
                "        executor.shutdown(wait=True, cancel_futures=False)\n",
                "        START_EVENT.clear()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Given individual conll files, generate aggregrated conll"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1000,343,348\n",
                "data_split_num_list = {\n",
                "    \"findings\": {\n",
                "        \"train\": 500,\n",
                "        \"dev\": 172,\n",
                "        \"test\": 174\n",
                "    },\n",
                "    \"impression\": {\n",
                "        \"train\": 500,\n",
                "        \"dev\": 172,\n",
                "        \"test\": 174\n",
                "    }\n",
                "}\n",
                "### Modify ###\n",
                "output_conll_aggregrate_dir = \"../../output/mimic_cxr/coref/aggregrate_conll/train_silver_mv_1k\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Source input: ../../output/mimic_cxr/coref/temp_individual_conll/majority_voting_(ml_rb_fj_stripped)\n",
                        "findings train 0 500\n",
                        "findings dev 500 672\n",
                        "findings test 672 846\n",
                        "impression train 0 500\n",
                        "impression dev 500 672\n",
                        "impression test 672 846\n"
                    ]
                }
            ],
            "source": [
                "from data_preprocessing.mimic_cxr_csv2conll import copy_and_paste_conll\n",
                "\n",
                "check_and_remove_dirs(output_conll_aggregrate_dir, True)\n",
                "check_and_create_dirs(output_conll_aggregrate_dir)\n",
                "print(\"Source input:\",config.temp_pred.base_dir)\n",
                "for section_name in [\"findings\", \"impression\"]:\n",
                "    section_temp_conll_dir = os.path.join(config.temp_pred.base_dir, section_name)\n",
                "    docId_list_shuffle = shuffle_list(FILE_CHECKER.filter(os.listdir(section_temp_conll_dir)), config.shuffle_seed)\n",
                "    split_start = 0\n",
                "    for split in [\"train\",\"dev\",\"test\"]:\n",
                "        split_end = split_start + data_split_num_list[section_name][split]\n",
                "        print(section_name, split, split_start, split_end)\n",
                "        # Aggregrate one by one\n",
                "        for doc_filename in docId_list_shuffle[split_start:split_end]:\n",
                "            input_conll_file = os.path.join(config.temp_pred.base_dir, section_name, doc_filename)\n",
                "            output_conll_file = os.path.join(output_conll_aggregrate_dir,f\"{split}.conll\")\n",
                "            copy_and_paste_conll(input_conll_file, output_conll_file)\n",
                "            \n",
                "        split_start = split_end"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Given aggregrated conll files, generate jsonlines files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "config.data_split.train_silver.dir_name = \"train_silver_mv_1k\"\n",
                "config.longformer.source = [{'train': 'train_silver'}, {'dev': 'train_silver'}, {'test': 'train_silver'}]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: longformer, Segment length: 4096\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-13 21:46:00,751 - Wrote 1000 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/output/mimic_cxr/coref/longformer/train.4096.jsonlines\n",
                        "2022-12-13 21:46:01,718 - Wrote 344 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/output/mimic_cxr/coref/longformer/dev.4096.jsonlines\n",
                        "2022-12-13 21:46:02,719 - Wrote 348 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/structured_reporting/output/mimic_cxr/coref/longformer/test.4096.jsonlines\n"
                    ]
                }
            ],
            "source": [
                "from data_preprocessing import mimic_cxr_conll2jsonlines\n",
                "\n",
                "log_msg = mimic_cxr_conll2jsonlines.invoke(config)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.12 ('corenlp')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
