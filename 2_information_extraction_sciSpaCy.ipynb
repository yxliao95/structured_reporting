{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.abspath(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from loguru import logger\n",
    "\n",
    "LOG_ROOT = os.path.abspath(\"./\")\n",
    "LOG_FILE = LOG_ROOT + \"/logs/metamap_processing.log\"\n",
    "\n",
    "# Remove all handlers and reset stderr\n",
    "logger.remove(handler_id=None)\n",
    "logger.add(\n",
    "    LOG_FILE,\n",
    "    level=\"TRACE\",\n",
    "    mode=\"w\",\n",
    "    backtrace=False,\n",
    "    diagnose=True,\n",
    "    colorize=False,\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    ")\n",
    "logger.info(\"\\r\\n\" + \">\" * 29 + \"\\r\\n\" + \">>> New execution started >>>\" + \"\\r\\n\" + \">\" * 29)\n",
    "# To filter log level: TRACE=5, DEBUG=10, INFO=20, SUCCESS=25, WARNING=30, ERROR=40, CRITICAL=50\n",
    "logger.add(sys.stdout, level=\"INFO\", filter=lambda record: record[\"level\"].no < 40, colorize=True)\n",
    "logger.add(sys.stderr, level=\"ERROR\", backtrace=False, diagnose=True, colorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pid        sid  \\\n",
      "0       p10000032  s50414267   \n",
      "1       p10000032  s53189527   \n",
      "2       p10000032  s53911762   \n",
      "3       p10000032  s56699142   \n",
      "4       p10000764  s57375967   \n",
      "...           ...        ...   \n",
      "227830  p19999442  s58708861   \n",
      "227831  p19999733  s57132437   \n",
      "227832  p19999987  s55368167   \n",
      "227833  p19999987  s58621812   \n",
      "227834  p19999987  s58971208   \n",
      "\n",
      "                                                 findings  \\\n",
      "0       There is no focal consolidation, pleural effus...   \n",
      "1       The cardiac, mediastinal and hilar contours ar...   \n",
      "2       Single frontal view of the chest provided. \\n ...   \n",
      "3       The lungs are clear of focal consolidation, pl...   \n",
      "4       PA and lateral views of the chest provided.   ...   \n",
      "...                                                   ...   \n",
      "227830  ET tube ends 4.7 cm above the carina.  NG tube...   \n",
      "227831  The lungs are clear, and the cardiomediastinal...   \n",
      "227832  There has been interval extubation and improve...   \n",
      "227833  Portable supine AP view of the chest provided ...   \n",
      "227834  The ET tube terminates approximately 2.9 cm fr...   \n",
      "\n",
      "                                               impression  \\\n",
      "0                       No acute cardiopulmonary process.   \n",
      "1                   No acute cardiopulmonary abnormality.   \n",
      "2                         No acute intrathoracic process.   \n",
      "3                       No acute cardiopulmonary process.   \n",
      "4       Focal consolidation at the left lung base, pos...   \n",
      "...                                                   ...   \n",
      "227830  1.  Lines and tubes are in adequate position. ...   \n",
      "227831                   No acute cardiothoracic process.   \n",
      "227832                                                      \n",
      "227833  Appropriately positioned ET and NG tubes.  Bib...   \n",
      "227834  Slight interval worsening of right lower lung ...   \n",
      "\n",
      "       provisional_findings_impression findings_and_impression  \n",
      "0                                                               \n",
      "1                                                               \n",
      "2                                                               \n",
      "3                                                               \n",
      "4                                                               \n",
      "...                                ...                     ...  \n",
      "227830                                                          \n",
      "227831                                                          \n",
      "227832                                                          \n",
      "227833                                                          \n",
      "227834                                                          \n",
      "\n",
      "[227835 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "REPORT_PATH = \"/home/yuxiangliao/PhD/data/mimic_cxr_reports_core.json\"\n",
    "df = pandas.read_json(REPORT_PATH,orient=\"records\",lines=True)\n",
    "print(df)\n",
    "\n",
    "id_list = df.loc[:,'sid'].to_list()\n",
    "findings_list = df.loc[:,'findings'].to_list()\n",
    "impression_list = df.loc[:,'impression'].to_list()\n",
    "pfi_list = df.loc[:,'provisional_findings_impression'].to_list()\n",
    "fai_list = df.loc[:,'findings_and_impression'].to_list()\n",
    "\n",
    "DATA_SIZE = len(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"metamap_processor\")\n",
    "def metamap_processor_func(doc):\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "DATA_START_POS = 376\n",
    "DATA_END_POS = 377\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\",disable=['ner'])\n",
    "nlp.add_pipe(\"metamap_processor\", last=True)\n",
    "\n",
    "# Batch pocessing\n",
    "for startPos in range(DATA_START_POS,DATA_END_POS,BATCH_SIZE):\n",
    "    text_tuples = [(text,{\"sid\":sid}) for sid, text in zip(id_list[startPos:startPos+BATCH_SIZE],findings_list[startPos:startPos+BATCH_SIZE])]\n",
    "    \n",
    "    for doc, context in nlp.pipe(text_tuples, as_tuples=True):\n",
    "        print(nlp.pipe_names)\n",
    "        sid = context[\"sid\"]\n",
    "        nounChunks = [-1] * len(doc)\n",
    "        for id, chunk in enumerate(doc.noun_chunks):\n",
    "            nounChunks[chunk.start:chunk.end] = [id] * (chunk.end-chunk.start)\n",
    "        sentences = [-1] * len(doc)\n",
    "        for id, sent in enumerate(doc.sents):\n",
    "            sentences[sent.start:sent.end] = [id] * (sent.end-sent.start)\n",
    "        offset = [0]\n",
    "        for i,tok in enumerate(doc):\n",
    "            offset.append(text.find(tok.text,offset[i],len(text)))\n",
    "        offset = offset[1:]\n",
    "        data = {\n",
    "            'token': [tok for tok in doc],\n",
    "            'tokenOffset': offset,\n",
    "            'sentenceGroup': sentences,\n",
    "            'nounChunk': nounChunks,\n",
    "            'lemma': [tok.lemma_ for tok in doc],\n",
    "            'pos_core': [f\"[{tok.pos_}]{spacy.explain(tok.pos_)}\" for tok in doc],\n",
    "            'pos_feature': [f\"[{tok.tag_}]{spacy.explain(tok.tag_)}\" for tok in doc],\n",
    "            'dependency': [f\"[{tok.dep_}]{spacy.explain(tok.dep_)}\" for tok in doc],\n",
    "            'dependency_head': [tok.head.text for tok in doc],\n",
    "            'dependency_children': [[child for child in tok.children] for tok in doc],\n",
    "            'morphology': [tok.morph for tok in doc],\n",
    "            'is_alpha': [tok.is_alpha for tok in doc],\n",
    "            'is_stop': [tok.is_stop for tok in doc],\n",
    "            'is_pronoun': [True if tok.pos_ == 'PRON' else False for tok in doc],\n",
    "            'trailing_space': [True if tok.whitespace_ else False for tok in doc]\n",
    "        }\n",
    "        output = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tokenOffset</th>\n",
       "      <th>sentenceGroup</th>\n",
       "      <th>nounChunk</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos_core</th>\n",
       "      <th>pos_feature</th>\n",
       "      <th>dependency</th>\n",
       "      <th>dependency_head</th>\n",
       "      <th>dependency_children</th>\n",
       "      <th>morphology</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_pronoun</th>\n",
       "      <th>trailing_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>it</td>\n",
       "      <td>379</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>it</td>\n",
       "      <td>[PRON]pronoun</td>\n",
       "      <td>[PRP]pronoun, personal</td>\n",
       "      <td>[nsubj]nominal subject</td>\n",
       "      <td>appears</td>\n",
       "      <td>[]</td>\n",
       "      <td>(Case=Nom, Gender=Neut, Number=Sing, Person=3,...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token  tokenOffset  sentenceGroup  nounChunk lemma       pos_core  \\\n",
       "76    it          379              4         14    it  [PRON]pronoun   \n",
       "\n",
       "               pos_feature              dependency dependency_head  \\\n",
       "76  [PRP]pronoun, personal  [nsubj]nominal subject         appears   \n",
       "\n",
       "   dependency_children                                         morphology  \\\n",
       "76                  []  (Case=Nom, Gender=Neut, Number=Sing, Person=3,...   \n",
       "\n",
       "    is_alpha  is_stop  is_pronoun  trailing_space  \n",
       "76      True     True        True            True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output[output['pos_core'].str.contains(\"PRON\")].index.tolist()\n",
    "output[output['pos_core'].str.contains(\"PRON\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [0, 3, 12, 15, 24, 34, 36, 40, 46, 54, 63, 67, 77, 80, 87, 91, 94, 98, 105, 108, 111, 115, 119, 121, 125, 134, 137, 141, 154, 160, 163, 172, 175, 178, 182, 190, 192, 196, 201, 206, 215, 219, 224, 232, 236, 240, 248, 252, 261, 263, 267, 272, 276, 279, 283, 287, 290, 294, 299, 302, 312, 315, 325, 333, 336, 344, 349, 353, 363, 368, 370, 377, 379, 382, 390, 393, 401, 406, 410, 416, 424, 427, 431, 437, 444, 448, 454, 457, 464, 469, 472, 474, 477, 479, 481, 485, 495, 504, 507, 511, 516, 525, 534, 536, 546, 553, 556, 560, 570, 575]\n",
    "l2 = output.loc[:,'tokenOffset']\n",
    "print(l1)\n",
    "print(l2.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# record\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(output.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.serve(doc, style=\"dep\")\n",
    "# sentence_spans = list(doc.sents)\n",
    "# displacy.serve(sentence_spans, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain tag and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('Peri')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy]",
   "language": "python",
   "name": "conda-env-spacy-py"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
