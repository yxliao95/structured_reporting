{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from loguru import logger\n",
    "\n",
    "LOG_ROOT = os.path.abspath(\"./\")\n",
    "LOG_FILE = LOG_ROOT + \"/logs/metamap_processing.log\"\n",
    "\n",
    "# Remove all handlers and reset stderr\n",
    "logger.remove(handler_id=None)\n",
    "logger.add(\n",
    "    LOG_FILE,\n",
    "    level=\"TRACE\",\n",
    "    mode=\"w\",\n",
    "    backtrace=False,\n",
    "    diagnose=True,\n",
    "    colorize=False,\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    ")\n",
    "logger.info(\"\\r\\n\" + \">\" * 29 + \"\\r\\n\" + \">>> New execution started >>>\" + \"\\r\\n\" + \">\" * 29)\n",
    "# To filter log level: TRACE=5, DEBUG=10, INFO=20, SUCCESS=25, WARNING=30, ERROR=40, CRITICAL=50\n",
    "logger.add(sys.stdout, level=\"INFO\", filter=lambda record: record[\"level\"].no < 40, colorize=True)\n",
    "logger.add(sys.stderr, level=\"ERROR\", backtrace=False, diagnose=True, colorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Metamap\n",
    "\n",
    "Follow the following instructions:\n",
    "- Install Metamap2020: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/Installation.html \n",
    "- Install additional datasets (2022 Specialist Lexicon, 2022AA UMLS NLM Datasets): https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/additional-tools/DataSetDownload.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the servers started\n",
    "- taggerServer\n",
    "- DisambiguatorServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cmd = 'ps -ef | grep java'\n",
    "out = os.popen(cmd)\n",
    "print(out.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check metamap human readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "text =  \"There is no focal consolidation, pleural effusion or pneumothorax.  Cardiomediastinal silhouette and hilar contours are otherwise unremarkable.\"\n",
    "input_command = f\"echo -e {text}\"\n",
    "input_process = subprocess.Popen(shlex.split(input_command), stdout=subprocess.PIPE)\n",
    "meta_command = \"metamap -V NLM -Z 2022AA -A --silent -I\"\n",
    "metamap_process = subprocess.Popen(shlex.split(meta_command), stdout=subprocess.PIPE, stdin=input_process.stdout)\n",
    "output, error = metamap_process.communicate()\n",
    "print(output.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "REPORT_PATH = \"/home/yuxiangliao/PhD/data/mimic_cxr_reports_core.json\"\n",
    "df = pandas.read_json(REPORT_PATH,orient=\"records\",lines=True)\n",
    "print(df)\n",
    "\n",
    "id_list = df.loc[:,'sid'].to_list()\n",
    "findings_list = df.loc[:,'findings'].to_list()\n",
    "impression_list = df.loc[:,'impression'].to_list()\n",
    "pfi_list = df.loc[:,'provisional_findings_impression'].to_list()\n",
    "fai_list = df.loc[:,'findings_and_impression'].to_list()\n",
    "\n",
    "DATA_SIZE = len(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiprocessing in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct metama command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex\n",
    "# Documentation: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "def get_metamap_command():\n",
    "    command = format_command_arg(\"metamap\")\n",
    "    command += format_command_arg(\"-V NLM\")                # Data Version: -V (--mm data version) [Base, USAbase, NLM]\n",
    "    command += format_command_arg(\"-Z 2022AA\")             # Knowledge Source: -Z (--mm data year)\n",
    "    command += format_command_arg(\"-A\")                    # Data Model: [-A (--strict model), -C (--relaxed model)]\n",
    "    command += format_command_arg(\"--silent\")              # Hide Header Output: --silent\n",
    "    command += format_command_arg(\"--JSONn\")               # Output format: [-q (--machine output), --XMLf, --XMLn, --XMLf1, --XMLn1, --JSONf, --JSONn, -N (--fielded mmi output), -F (--formal tagger output)]\n",
    "    # command += \" --conj\"                                   # Turn on Conjunction Processing\n",
    "    # command += \" -y\"                                       # Word-Sense Disambiguation: -y (--word sense disambiguation)\n",
    "    # UDA_path = \"/home/yuxiangliao/PhD/UMLS/custom-resources/custom-word-replacement\"\n",
    "    # command += format_command_arg(f\"--UDA {UDA_path}\")     # User-Defined Acronyms/Abbreviations (word replacement): --UDA <file>\n",
    "    # semantic_types = \"virs,cgab,acab,ffas,bpoc,medd,tmco,qlco,qnco,bsoj,blor,fndg,sosy,topp,ortf,patf,dsyn,inpo\"\n",
    "    # commend += f\"-J {semantic_types}\"                      # Retain only Concepts with Specified Semantic Types: -J (--restrict to sts) <list>\n",
    "    # command += format_command_arg(\"-I\")                    # For human readable output\n",
    "    return command\n",
    "\n",
    "def format_command_arg(arg):\n",
    "    return \" \" + arg\n",
    "\n",
    "def run_metamap(startIndex,batch_size):\n",
    "    endIndex = startIndex + batch_size if startIndex + batch_size < DATA_SIZE else DATA_SIZE\n",
    "    input_list = [(record if record else \"None\") for record in findings_list[startIndex:endIndex]]\n",
    "    input = repr(\"\\n\\n\".join(input_list))\n",
    "    input_command = f\"echo -e {input}\"\n",
    "    input_process = subprocess.Popen(shlex.split(input_command), stdout=subprocess.PIPE)\n",
    "    \n",
    "    meta_command = get_metamap_command()\n",
    "    metamap_process = subprocess.Popen(shlex.split(meta_command), stdout=subprocess.PIPE, stdin=input_process.stdout)\n",
    "   \n",
    "    output_bytes, error_bytes = metamap_process.communicate()\n",
    "    if error:\n",
    "        logger.error(error_bytes.decode())\n",
    "    return output_bytes.decode(), [startIndex,endIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Object for JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveTokenIndices(section:Section, startPos, length) -> list:\n",
    "    indicesInSection = []\n",
    "    doInsert = False\n",
    "    posPointer = startPos\n",
    "    for i, currPos in enumerate(section.tokenPos):\n",
    "        nextPos = section.tokenPos[i+1] if i + 1 < len(section.tokenPos) else section.tokenPos[i] + 99\n",
    "        if not doInsert and posPointer >= currPos and posPointer < nextPos:\n",
    "            doInsert = True\n",
    "            posPointer = startPos + length - 1\n",
    "        elif doInsert and posPointer < currPos: \n",
    "            break # break the loop in advance, othewise will stop when finish the loop.\n",
    "        if doInsert:\n",
    "            indicesInSection.append(i)\n",
    "    return indicesInSection\n",
    "\n",
    "class Concept(object):\n",
    "    def __init__(self, sourceTokens:list, startPosList:list, lengthList:list, umlsCUI:str, preferedName:str, hitTerm:str, categories:list, isHead:int, isNegated:int):\n",
    "        self.sourceTokens = sourceTokens\n",
    "        self.startPosList = startPosList\n",
    "        self.lengthList = lengthList\n",
    "        self.indicesInSection = []\n",
    "        self.umlsCUI = umlsCUI\n",
    "        self.preferedName = preferedName\n",
    "        self.hitTerm = hitTerm\n",
    "        self.categories = categories\n",
    "        self.isHead =  1 if isHead == \"yes\" else 0\n",
    "        self.isNegated =  1 if isNegated == \"1\" else 0\n",
    "    def update(self, section:Section):\n",
    "        for startPos, length in zip(self.startPosList, self.lengthList):\n",
    "            indicesInSection = resolveTokenIndices(section, startPos, length)\n",
    "            self.indicesInSection.extend(indicesInSection)\n",
    "        \n",
    "class ConceptGroup(object):\n",
    "    def __init__(self):\n",
    "        self.concepts = []\n",
    "    def addConcept(self, concept:Concept):\n",
    "        self.concepts.append(concept)\n",
    "    def update(self, section:Section):\n",
    "        for obj in self.concepts:\n",
    "            obj.update(section)\n",
    "        \n",
    "class SyntaxChunk(object):\n",
    "    def __init__(self, text:str, lexicalMatch:str, syntaxType:str, partOfSpeech:str, tokens:list):\n",
    "        self.text = text # The original form of the text (case sensitive)\n",
    "        self.lexicalMatch = lexicalMatch\n",
    "        self.syntaxType = syntaxType\n",
    "        self.partOfSpeech = partOfSpeech\n",
    "        self.tokens = tokens\n",
    "                \n",
    "class Phrase(object):\n",
    "    def __init__(self, text:str, startPos:int, length:int):\n",
    "        self.text = text\n",
    "        self.startPos = startPos\n",
    "        self.length = length\n",
    "        self.tokens = []\n",
    "        self.indicesInSection = []\n",
    "        self.syntaxChunks = []\n",
    "        self.mappings = []\n",
    "    def addSyntaxChunk(self, syntaxChunk:SyntaxChunk):\n",
    "        self.syntaxChunks.append(syntaxChunk)\n",
    "        self.tokens.extend(syntaxChunk.tokens)\n",
    "    def addConceptGroup(self, conceptGroup:ConceptGroup):\n",
    "        self.mappings.append(conceptGroup)\n",
    "    def update(self, section:Section):\n",
    "        indicesInSection = resolveTokenIndices(section, self.startPos, self.length)\n",
    "        self.indicesInSection.extend(indicesInSection)\n",
    "        for obj in self.mappings:\n",
    "            obj.update(section)\n",
    "        \n",
    "        \n",
    "class Sentence(object):\n",
    "    def __init__(self, text:str, startPos:int, length:int):\n",
    "        self.text = text\n",
    "        self.startPos = startPos\n",
    "        self.length = length\n",
    "        self.tokens = []\n",
    "        self.indicesInSection = []\n",
    "        self.phrases = []\n",
    "    def addPhrase(self, phrase:Phrase):\n",
    "        self.phrases.append(phrase)\n",
    "        self.tokens.extend(phrase.tokens)\n",
    "    def update(self, section:Section):\n",
    "        indicesInSection = resolveTokenIndices(section, self.startPos, self.length)\n",
    "        self.indicesInSection.extend(indicesInSection)\n",
    "        # Update the children objs\n",
    "        for obj in self.phrases:\n",
    "            obj.update(section)\n",
    "\n",
    "class Negation(object):\n",
    "    def __init__(self, text:str, triStartPosList:list, triLengthList:list, conceptsCUIs:list, tarStartPosList:list, tarLengthList:list):\n",
    "        self.trgger = {\n",
    "            'text': text,\n",
    "            'startPosList': triStartPosList,\n",
    "            'lengthList': triLengthList,\n",
    "            'indicesInSection': []\n",
    "        }\n",
    "        self.tarrget = {\n",
    "            'conceptsCUIs': conceptsCUIs,\n",
    "            'startPosList': tarStartPosList,\n",
    "            'lengthList': tarLengthList,\n",
    "            'indicesInSection': []\n",
    "        }\n",
    "    def update(self, section:Section):\n",
    "        for startPos, length in zip(self.trgger['startPosList'], self.trgger['lengthList']):\n",
    "            indicesInSection = resolveTokenIndices(section, startPos, length)\n",
    "            self.trgger['indicesInSection'].extend(indicesInSection)\n",
    "        for startPos, length in zip(self.tarrget['startPosList'], self.tarrget['lengthList']):\n",
    "            indicesInSection = resolveTokenIndices(section, startPos, length)\n",
    "            self.tarrget['indicesInSection'].extend(indicesInSection) \n",
    "\n",
    "class Section(object):\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "        self.text = \"\" # context\n",
    "        self.tokens = []\n",
    "        self.tokenPos = []\n",
    "        self.sentences = []\n",
    "        self.negations = []\n",
    "    def addSentence(self, sentence:Sentence):\n",
    "        self.sentences.append(sentence)\n",
    "        self.text += sentence.text\n",
    "        self.tokens.extend(sentence.tokens)\n",
    "    def addNegation(self, negation:Negation):\n",
    "        self.negations.append(negation)\n",
    "    def update(self):\n",
    "        offset = [0]\n",
    "        for i,substring in enumerate(self.tokens):\n",
    "            offset.append(self.text.lower().find(substring,offset[i],len(self.text)))\n",
    "        offset = offset[1:]\n",
    "        self.tokenPos = offset\n",
    "        # Update the children objs\n",
    "        for obj in self.sentences:\n",
    "            obj.update(self)\n",
    "        for obj in self.negations:\n",
    "            obj.update(self)\n",
    "        \n",
    "class Record(object):\n",
    "    def __init__(self, sid:str):\n",
    "        self.sid = sid\n",
    "        self.sections = []\n",
    "    def addSection(self, section:Section):\n",
    "        section.update()\n",
    "        self.sections.append(section)\n",
    "        \n",
    "class Records(object):\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "    def addRecord(self, record:Record):\n",
    "        self.records.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to resolve specific JSON subtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolveSyntaxUnit(syntaxUnit):\n",
    "    text = syntaxUnit['InputMatch']\n",
    "    syntaxType = syntaxUnit['SyntaxType']\n",
    "    tokens = syntaxUnit['Tokens']\n",
    "    # Add punc to token list\n",
    "    if not tokens:\n",
    "        logger.trace(f\"Empty token detected: SyntaxType:{syntaxType}, InputMatch:{text}\")\n",
    "        tokens = [text]\n",
    "    try:\n",
    "        lexicalMatch = syntaxUnit['LexMatch']\n",
    "        partOfSpeech = syntaxUnit['LexCat']\n",
    "    except KeyError:\n",
    "        lexicalMatch = \"\"\n",
    "        partOfSpeech = \"\"\n",
    "    if text.lower() != lexicalMatch and text.isalnum():\n",
    "        logger.trace(f\"text:[{text}], lexicalMatch:[{lexicalMatch}]\")\n",
    "    return SyntaxChunk(text, lexicalMatch, syntaxType, partOfSpeech, tokens)\n",
    "\n",
    "def resolveConcept(mappingCandidate):\n",
    "    sourceTokens = mappingCandidate['MatchedWords']\n",
    "    startPosList = [int(i['StartPos']) for i in mappingCandidate['ConceptPIs']]\n",
    "    lengthList = [int(i['Length']) for i in mappingCandidate['ConceptPIs']]\n",
    "    umlsCUI = mappingCandidate['CandidateCUI']\n",
    "    preferedName = mappingCandidate['CandidatePreferred']\n",
    "    hitTerm = mappingCandidate['CandidateMatched']\n",
    "    categories = mappingCandidate['SemTypes']\n",
    "    isHead = 1 if mappingCandidate['IsHead'] == \"yes\" else 0\n",
    "    isNegated = 1 if mappingCandidate['Negated'] == \"1\" else 0\n",
    "    return Concept(sourceTokens, startPosList, lengthList, umlsCUI, preferedName, hitTerm, categories, isHead, isNegated)\n",
    "\n",
    "def resolveNegation(negation):\n",
    "    trigger = negation['NegTrigger']\n",
    "    triggerStartPosList = [int(i['StartPos']) for i in negation['NegTriggerPIs']]\n",
    "    triggerLengthList = [int(i['Length']) for i in negation['NegTriggerPIs']]\n",
    "    conceptCUIs = [i['NegConcCUI'] for i in negation['NegConcepts']]\n",
    "    targetStartPosList = [int(i['StartPos']) for i in negation['NegConcPIs']]\n",
    "    targetLengthList = [int(i['Length']) for i in negation['NegConcPIs']]\n",
    "    return Negation(trigger, triggerStartPosList, triggerLengthList, conceptCUIs, targetStartPosList, targetLengthList)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to resolve JSON format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMetamapJSON(json_obj,id_subList) -> Records:\n",
    "    records = Records()\n",
    "    for _idx, _document in enumerate(json_obj['AllDocuments']):\n",
    "        # print(_document.keys())\n",
    "        # print(record['Document']['Negations'])\n",
    "        record = Record(id_subList[_idx])\n",
    "        section = Section(\"findings\")\n",
    "        for _utterance in _document['Document']['Utterances']:\n",
    "            # print(_utterance.keys())\n",
    "            sentence = Sentence(text=_utterance['UttText'], startPos=int(_utterance['UttStartPos']), length=int(_utterance['UttLength']))\n",
    "            for _phrase in _utterance['Phrases']:\n",
    "                # print(_phrase.keys())\n",
    "                phrase = Phrase(text=_phrase['PhraseText'], startPos=int(_phrase['PhraseStartPos']), length=int(_phrase['PhraseLength']))\n",
    "                for _syntaxUnit in _phrase['SyntaxUnits']:\n",
    "                    # print(_syntaxUnit.keys())\n",
    "                    syntaxChunk = resolveSyntaxUnit(_syntaxUnit)\n",
    "                    phrase.addSyntaxChunk(syntaxChunk)\n",
    "                for _mapping in _phrase['Mappings']:\n",
    "                    # print(_mapping.keys())\n",
    "                    conceptGroup = ConceptGroup()\n",
    "                    for _mappingCandidate in _mapping['MappingCandidates']:\n",
    "                        # print(_mappingCandidate.keys())\n",
    "                        concept = resolveConcept(_mappingCandidate)\n",
    "                        conceptGroup.addConcept(concept)\n",
    "                    phrase.addConceptGroup(conceptGroup)\n",
    "                sentence.addPhrase(phrase)\n",
    "            section.addSentence(sentence)\n",
    "        for _negation in _document['Document']['Negations']:\n",
    "            negation = resolveNegation(_negation)\n",
    "            section.addNegation(negation)\n",
    "        record.addSection(section)\n",
    "        records.addRecord(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "\n",
    "def classToJSON(obj) -> str:\n",
    "    return jsonpickle.encode(obj,unpicklable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute metamap only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Lock\n",
    "import json\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "# mp.cpu_count()\n",
    "WORKERS_NUM = 5\n",
    "DATA_START_POS = 0\n",
    "DATA_STOP_POS = 100\n",
    "# DATA_END_INDEX = DATA_SIZE\n",
    "\n",
    "executor = ProcessPoolExecutor(max_workers=WORKERS_NUM)\n",
    "all_task = [executor.submit(run_metamap, startIndex, BATCH_SIZE) for startIndex in range(DATA_START_POS, DATA_STOP_POS, BATCH_SIZE)]\n",
    "\n",
    "lock=Lock()\n",
    "with open(\"/home/yuxiangliao/PhD/output/metamap/metamap_output_100.json\",\"w\") as f:\n",
    "    for future in as_completed(all_task):\n",
    "        output, idx_inteval = future.result()\n",
    "        # Only the second line is the required JSON string.\n",
    "        id_subList = id_list[idx_inteval[0]:idx_inteval[1]]\n",
    "        json_output = list(output.split(\"\\n\"))[1]\n",
    "        # with open(\"/home/yuxiangliao/PhD/output/metamap_output_test.json\",\"a\") as f:\n",
    "        #     f.write(json_output)\n",
    "        json_obj = json.loads(json_output)\n",
    "        records_batch = parseMetamapJSON(json_obj, id_subList)\n",
    "        # print(classToJSON(records_batch))\n",
    "        lock.acquire()\n",
    "        f.write(classToJSON(records_batch))\n",
    "        f.write(\"\\n\")\n",
    "        f.flush\n",
    "        lock.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Lock\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "BATCH_SIZE =1\n",
    "# mp.cpu_count()\n",
    "WORKERS_NUM = 1\n",
    "DATA_START_POS = 376\n",
    "DATA_END_POS = 377\n",
    "# DATA_END_INDEX = DATA_SIZE\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\",disable=['ner'])\n",
    "nlp.add_pipe(\"metamap_processor\", last=True)\n",
    "\n",
    "executor = ProcessPoolExecutor(max_workers=WORKERS_NUM)\n",
    "all_task = [executor.submit(run_metamap, startIndex, BATCH_SIZE) for startIndex in range(DATA_START_POS, DATA_END_POS, BATCH_SIZE)]\n",
    "\n",
    "lock=Lock()\n",
    "with open(\"/home/yuxiangliao/PhD/output/metamap/metamap_output_test.json\",\"w\") as f:\n",
    "    for future in as_completed(all_task):\n",
    "        # Metamap\n",
    "        metamap_output, idx_inteval = future.result()\n",
    "        id_subList = id_list[idx_inteval[0]:idx_inteval[1]] \n",
    "        metamap_json_output = list(metamap_output.split(\"\\n\"))[1] # Only the second line is the required JSON string.\n",
    "        # with open(\"/home/yuxiangliao/PhD/output/metamap_output_test.json\",\"a\") as f:\n",
    "        #     f.write(json_output)\n",
    "        metamap_json_obj = json.loads(metamap_json_output)\n",
    "        parsed_obj_batch = parseMetamapJSON(metamap_json_obj, id_subList)\n",
    "        # print(classToJSON(batch_record))\n",
    "        # lock.acquire()\n",
    "        # f.write(classToJSON(batch_record))\n",
    "        # f.write(\"\\n\")\n",
    "        # f.flush\n",
    "        # lock.release()\n",
    "    \n",
    "        # SpaCy\n",
    "        text_tuples = [(record.sections[0].text,{\"record\":record}) for record in parsed_obj_batch.records]\n",
    "        for doc, context in nlp.pipe(text_tuples, as_tuples=True):\n",
    "            print(context['record'].sid)\n",
    "            nounChunks = [-1] * len(doc)\n",
    "            for id, chunk in enumerate(doc.noun_chunks):\n",
    "                nounChunks[chunk.start:chunk.end] = [id] * (chunk.end-chunk.start)\n",
    "            sentences = [-1] * len(doc)\n",
    "            for id, sent in enumerate(doc.sents):\n",
    "                sentences[sent.start:sent.end] = [id] * (sent.end-sent.start)\n",
    "            offset = [0]\n",
    "            for i,tok in enumerate(doc):\n",
    "                offset.append(text.find(tok.text,offset[i],len(text)))\n",
    "            offset = offset[1:]\n",
    "            data = {\n",
    "                'token': [tok for tok in doc],\n",
    "                'tokenOffset': offset,\n",
    "                'sentenceGroup': sentences,\n",
    "                'nounChunk': nounChunks,\n",
    "                'lemma': [tok.lemma_ for tok in doc],\n",
    "                'pos_core': [f\"[{tok.pos_}]{spacy.explain(tok.pos_)}\" for tok in doc],\n",
    "                'pos_feature': [f\"[{tok.tag_}]{spacy.explain(tok.tag_)}\" for tok in doc],\n",
    "                'dependency': [f\"[{tok.dep_}]{spacy.explain(tok.dep_)}\" for tok in doc],\n",
    "                'dependency_head': [tok.head.text for tok in doc],\n",
    "                'dependency_children': [[child for child in tok.children] for tok in doc],\n",
    "                'morphology': [tok.morph for tok in doc],\n",
    "                'is_alpha': [tok.is_alpha for tok in doc],\n",
    "                'is_stop': [tok.is_stop for tok in doc],\n",
    "                'is_pronoun': [True if tok.pos_ == 'PRON' else False for tok in doc],\n",
    "                'trailing_space': [True if tok.whitespace_ else False for tok in doc]\n",
    "            }\n",
    "            output = pd.DataFrame(data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
