{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from loguru import logger\n",
    "\n",
    "LOG_ROOT = os.path.abspath(\"./\")\n",
    "LOG_FILE = LOG_ROOT + \"/logs/sr-3.log\"\n",
    "\n",
    "# Remove all handlers and reset stderr\n",
    "logger.remove(handler_id=None)\n",
    "logger.add(\n",
    "    LOG_FILE,\n",
    "    level=\"TRACE\",\n",
    "    mode=\"w\",\n",
    "    backtrace=False,\n",
    "    diagnose=True,\n",
    "    colorize=False,\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}\",\n",
    ")\n",
    "logger.info(\"\\r\\n\" + \">\" * 29 + \"\\r\\n\" + \">>> New execution started >>>\" + \"\\r\\n\" + \">\" * 29)\n",
    "# To filter log level: TRACE=5, DEBUG=10, INFO=20, SUCCESS=25, WARNING=30, ERROR=40, CRITICAL=50\n",
    "logger.add(sys.stdout, level=\"INFO\", filter=lambda record: record[\"level\"].no < 40, colorize=True)\n",
    "logger.add(sys.stderr, level=\"ERROR\", backtrace=False, diagnose=True, colorize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              pid        sid  \\\n",
      "0       p10000032  s50414267   \n",
      "1       p10000032  s53189527   \n",
      "2       p10000032  s53911762   \n",
      "3       p10000032  s56699142   \n",
      "4       p10000764  s57375967   \n",
      "...           ...        ...   \n",
      "227830  p19999442  s58708861   \n",
      "227831  p19999733  s57132437   \n",
      "227832  p19999987  s55368167   \n",
      "227833  p19999987  s58621812   \n",
      "227834  p19999987  s58971208   \n",
      "\n",
      "                                                 findings  \\\n",
      "0       There is no focal consolidation, pleural effus...   \n",
      "1       The cardiac, mediastinal and hilar contours ar...   \n",
      "2       Single frontal view of the chest provided. \\n ...   \n",
      "3       The lungs are clear of focal consolidation, pl...   \n",
      "4       PA and lateral views of the chest provided.   ...   \n",
      "...                                                   ...   \n",
      "227830  ET tube ends 4.7 cm above the carina.  NG tube...   \n",
      "227831  The lungs are clear, and the cardiomediastinal...   \n",
      "227832  There has been interval extubation and improve...   \n",
      "227833  Portable supine AP view of the chest provided ...   \n",
      "227834  The ET tube terminates approximately 2.9 cm fr...   \n",
      "\n",
      "                                               impression  \\\n",
      "0                       No acute cardiopulmonary process.   \n",
      "1                   No acute cardiopulmonary abnormality.   \n",
      "2                         No acute intrathoracic process.   \n",
      "3                       No acute cardiopulmonary process.   \n",
      "4       Focal consolidation at the left lung base, pos...   \n",
      "...                                                   ...   \n",
      "227830  1.  Lines and tubes are in adequate position. ...   \n",
      "227831                   No acute cardiothoracic process.   \n",
      "227832                                                      \n",
      "227833  Appropriately positioned ET and NG tubes.  Bib...   \n",
      "227834  Slight interval worsening of right lower lung ...   \n",
      "\n",
      "       provisional_findings_impression findings_and_impression  \n",
      "0                                                               \n",
      "1                                                               \n",
      "2                                                               \n",
      "3                                                               \n",
      "4                                                               \n",
      "...                                ...                     ...  \n",
      "227830                                                          \n",
      "227831                                                          \n",
      "227832                                                          \n",
      "227833                                                          \n",
      "227834                                                          \n",
      "\n",
      "[227835 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "REPORT_PATH = \"/home/yuxiangliao/PhD/data/mimic_cxr_reports_core.json\"\n",
    "df = pd.read_json(REPORT_PATH,orient=\"records\",lines=True)\n",
    "print(df)\n",
    "\n",
    "id_list = df.loc[:,'sid'].to_list()\n",
    "findings_list = df.loc[:,'findings'].to_list()\n",
    "impression_list = df.loc[:,'impression'].to_list()\n",
    "pfi_list = df.loc[:,'provisional_findings_impression'].to_list()\n",
    "fai_list = df.loc[:,'findings_and_impression'].to_list()\n",
    "\n",
    "DATA_SIZE = len(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $CORENLP_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "CUSTOM_PROPS = {\n",
    "    'annotators':'tokenize, ssplit, pos, lemma, ner, depparse, coref',\n",
    "    \"coref.algorithm\": \"statistical\"\n",
    "}\n",
    "with CoreNLPClient(memory='8G', threads=16, endpoint='http://localhost:8801', be_quiet=False, \n",
    "                   properties=CUSTOM_PROPS) as client:\n",
    "    start = time.time()\n",
    "    for text in findings_list[0:100]:\n",
    "        document = client.annotate(text)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biomedical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en', package='mimic')\n",
    "stanza.download('en', package='radiology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza processor: https://stanfordnlp.github.io/stanza/pipeline.html#processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.pipeline.core import DownloadMethod\n",
    "processor_dict = {\n",
    "    'tokenize': 'mimic', \n",
    "    'pos': 'mimic', \n",
    "    'lemma': 'mimic',\n",
    "    'depparse': 'mimic',\n",
    "    # 'sentiment':'sstplus', # Sentiment scores of 0, 1, or 2 (negative, neutral, positive).\n",
    "    'constituency': 'wsj', # wsj, wsj_bert, wsj_roberta\n",
    "    'ner': 'radiology',\n",
    "}\n",
    "nlp = stanza.Pipeline('en', processors=processor_dict, package=None, \n",
    "                      download_method=DownloadMethod.REUSE_RESOURCES,\n",
    "                      verbose=False) # logging_level='WARN'\n",
    "doc = nlp(findings_list[8690])\n",
    "# print out dependency tree\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Multiple Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28s -> 100docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "processor_dict = {\n",
    "    'tokenize': 'mimic', \n",
    "    'pos': 'mimic', \n",
    "    'lemma': 'mimic',\n",
    "    'depparse': 'mimic',\n",
    "    # 'sentiment':'sstplus', # Sentiment scores of 0, 1, or 2 (negative, neutral, positive).\n",
    "    'constituency': 'wsj', # wsj, wsj_bert, wsj_roberta\n",
    "    'ner': 'radiology',\n",
    "}\n",
    "\n",
    "def set_sid(self, value):\n",
    "    self._sid = value\n",
    "def get_sid(self):\n",
    "    return self._sid\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors=processor_dict, package=None, \n",
    "                      download_method=DownloadMethod.REUSE_RESOURCES,\n",
    "                      verbose=False) # logging_level='WARN'\n",
    "documents = findings_list[0:10] # Documents that we are going to process\n",
    "# stanza.Document.add_property(\"sid\",getter=get_sid, setter=set_sid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s50414267', 's53189527', 's53911762', 's56699142', 's57375967', 's50771383', 's54205396', 's50578979', 's51178377', 's55697293']\n",
      "['s50414267', 's53189527', 's53911762', 's56699142', 's57375967', 's50771383', 's54205396', 's50578979', 's51178377', 's55697293']\n"
     ]
    }
   ],
   "source": [
    "in_docs = []\n",
    "for id,d in enumerate(documents):\n",
    "    stanzaDoc = stanza.Document([], text=d)\n",
    "    stanzaDoc._sid = id_list[id]\n",
    "    in_docs.append(stanzaDoc)\n",
    "# in_docs = [stanza.Document([], text=d) for d in documents] # Wrap each document with a stanza.Document object\n",
    "out_docs = nlp(in_docs) # Call the neural pipeline on this list of documents\n",
    "# print(out_docs[0]) # The output is also a list of stanza.Document objects, each output corresponding to an input Document object\n",
    "print([i._sid for i in out_docs])\n",
    "print(id_list[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:corenlp]",
   "language": "python",
   "name": "conda-env-corenlp-py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
